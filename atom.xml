<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Louie&#39;s Blog</title>
  
  <subtitle>O ever youthful, O ever powerful.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://louielong.github.io/"/>
  <updated>2020-07-24T09:50:48.433Z</updated>
  <id>https://louielong.github.io/</id>
  
  <author>
    <name>Louie Long</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>K8S搭配Kata container</title>
    <link href="https://louielong.github.io/k8s-with-katacontainer.html"/>
    <id>https://louielong.github.io/k8s-with-katacontainer.html</id>
    <published>2020-07-20T08:47:56.000Z</published>
    <updated>2020-07-24T09:50:48.433Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>工作中有聊到容器安全相关的话题，专门研究了一下容器安全见上一篇博文<a href="container-security.html">Docker容器安全性分析</a>，文章详尽的分析了容器相关的安全如图所示：</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgcontainer-security.png" alt="容器安全"></p><p>针对其中的容器逃逸问题，之前关注过Openstack的一个项目<a href="https://katacontainers.io/" target="_blank" rel="noopener">Kata container</a>项目：</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgkata_banner.png" alt="kata container"></p><blockquote><p>Kata container是一个开源社区，致力于使用轻量级虚拟机构建安全的容器运行时，这些虚拟机感觉和执行起来都像容器，但是使用硬件虚拟化技术作为第二层防御，提供更强的工作负载隔离。</p></blockquote><p><strong>上一次部署k8s还是去年使用的是v1.17版本，这次打算重新安装一次k8s v1.18版本并搭配kata container试验一下。</strong></p><h2 id="二、Docker"><a href="#二、Docker" class="headerlink" title="二、Docker"></a>二、Docker</h2><p>这里简要介绍一下docker的组件，为后续的kata containe做一个铺垫。</p><p>docker在 1.11 之 后，被拆分成了多个组件以适应 OCI 标准。拆分之后，其包括 docker daemon，  containerd，containerd-shim 以及 runC。组件 containerd 负责集群节点上容器 的生命周期管理，并向上为  docker daemon 提供 gRPC 接口。</p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgdocker_call_stack.png" alt="docker 架构" style="zoom:33%;"><ol><li>dockerd 是docker-containerd 的父进程， docker-containerd 是n个docker-containerd-shim 的父进程。</li><li>Containerd 是一个 gRPC 的服务器。它会在接到 docker daemon 的远程请 求之后，新建一个线程去处理这次请求。依靠 runC 去创建容器进程。而在容器启动之后， runC 进程会退出。</li><li>runC 命令，是 libcontainer 的一个简单的封装。这个工具可以 用来管理单个容器，比如容器创建，或者容器删除。</li></ol><p>container-shim，shim的翻译是垫片，就是修自行车的时候，用来夹在螺丝和螺母之间的小铁片。关于shim本身，网上介绍的文章很少，但是作者在 <a href="https://groups.google.com/forum/#!topic/docker-dev/zaZFlvIx1_k" target="_blank" rel="noopener">Google Groups 里有解释到shim的作用</a>[2]：</p><ul><li>允许runc在创建&amp;运行容器之后退出</li><li>用shim作为容器的父进程，而不是直接用containerd作为容器的父进程，是为了防止这种情况：当containerd挂掉的时候，shim还在，因此可以保证容器打开的文件描述符不会被关掉</li><li>依靠shim来收集&amp;报告容器的退出状态，这样就不需要containerd来wait子进程</li></ul><p>关于docker组件的详细介绍这里不再赘述了，想深入了解可以查看[2]。主要关注的是<code>runc</code>，后续的kata containe也主要是替换<code>runc</code>为Hyper的<code>runv</code>。</p><h2 id="三、kata-container"><a href="#三、kata-container" class="headerlink" title="三、kata container"></a>三、kata container</h2><p>Kata container使用的是<code>Rust</code>语言编写，特性如下：</p><ul><li><p>安全：运行在专用的内核中，提供网络、I/O和内存隔离，可以利用虚拟化VT扩展中的硬件强制隔离。</p></li><li><p>兼容：支持行业标准，包括OCI容器格式、Kubernetes CRI接口以及传统虚拟化技术。</p></li><li><p>性能：提供与标准Linux容器一致的性能;增强了隔离性，没有标准虚拟机的性能负担。</p></li><li><p>简单：消除了在完整的虚拟机内部嵌套容器的要求； 标准接口使插入和入门变得容易。</p></li></ul><p>kata container基于轻量级虚拟机的容器，不同容器跑在一个个不同的虚拟机（kernel）上，比起传统容器提供了更好的隔离性和安全性，同时继承了容器快速启动和快速部署等优点。这也是本次想使用kata 的原因，能够提供比原生docker更好的隔离性，<strong>Kata 容器每个容器都使用专有内核，避免容器逃逸后影响宿主机的内核</strong>。</p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgthumbnail.jpg" alt="kata container与VM的区别" style="zoom:50%;"><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgkatacontainers_architecture_diagram.jpg" style="zoom: 33%;"><p>废话不多说，开始体验一下kata</p><h3 id="3-1-安装kata"><a href="#3-1-安装kata" class="headerlink" title="3.1  安装kata"></a>3.1  安装kata</h3><p>这里使用ubuntu 18.04安装kata最新的版本<code>Kata Container 1.12.0</code>参考kata官方安装手册[3]</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ ARCH=$(arch)</span><br><span class="line">admin@k8smaster:~$ BRANCH="$&#123;BRANCH:-master&#125;"</span><br><span class="line">admin@k8smaster:~$ sudo sh -c "echo 'deb http://download.opensuse.org/repositories/home:/katacontainers:/releases:/$&#123;ARCH&#125;:/$&#123;BRANCH&#125;/xUbuntu_$(lsb_release -rs)/ /' &gt; /etc/apt/sources.list.d/kata-containers.list"</span><br><span class="line">xdnsadmin@k8smaster:~$ curl -sL  http://download.opensuse.org/repositories/home:/katacontainers:/releases:/$&#123;ARCH&#125;:/$&#123;BRANCH&#125;/xUbuntu_$(lsb_release -rs)/Release.key | sudo apt-key add -</span><br><span class="line">OK</span><br><span class="line">xdnsadmin@k8smaster:~$ sudo -E apt-get update &amp;&amp; sudo -E apt-get -y install kata-runtime kata-proxy kata-shim</span><br></pre></td></tr></table></figure><p>安装完成后测试一下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kata-runtime --version</span><br><span class="line">kata-runtime  : 1.12.0-alpha0</span><br><span class="line">   commit   : &lt;&lt;unknown&gt;&gt;</span><br><span class="line">   OCI specs: 1.0.1-dev</span><br></pre></td></tr></table></figure><h3 id="3-2-配置docker的runtime"><a href="#3-2-配置docker的runtime" class="headerlink" title="3.2 配置docker的runtime"></a>3.2 配置docker的runtime</h3><p>为了保持安装的docker能够使用，这里默认仍然使用<code>runc</code>，但是增加<code>kata-runtime</code>做为可选的docker runtime。相关配置可参考官网<a href="https://github.com/kata-containers/kata-containers/tree/2.0-dev/docs/install" target="_blank" rel="noopener">kata install</a>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ sudo mkdir -p /etc/systemd/system/docker.service.d/</span><br><span class="line">admin@k8smaster:~$  cat &lt;&lt;EOF | sudo tee /etc/systemd/system/docker.service.d/kata-containers.conf</span><br><span class="line">[Service]</span><br><span class="line">ExecStart=</span><br><span class="line">ExecStart=/usr/bin/dockerd -D --default-runtime runc --add-runtime kata-runtime=/usr/bin/kata-runtime</span><br><span class="line">EOF</span><br><span class="line">admin@k8smaster:~$ sudo systemctl daemon-reload</span><br><span class="line">admin@k8smaster:~$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure><p>下面创建两个容器，一个使用 kata runtime，另一个使用默认的 runc。创建完容器之后，查看一下 kata container 的一些信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ docker run -d --name kata-test -p 8080:80 --runtime kata-runtime httpd:alpine</span><br><span class="line">ddbd72466c4ce862891538a8252a83450435b25f488cce98c9f8f9a8e51711e7</span><br><span class="line">admin@k8smaster:~$ curl http://localhost:8080</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;</span><br><span class="line">admin@k8smaster:~$ docker run -d --name runc-test -p 8082:80  httpd:alpine</span><br><span class="line">c7aff63e2d239ea0d27641398f0d3f108999ba3a125c6cef9f1330ccb75c93d4</span><br><span class="line">admin@k8smaster:~$ docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND              CREATED              STATUS              PORTS                  NAMES</span><br><span class="line">c7aff63e2d23        httpd:alpine        "httpd-foreground"   3 seconds ago        Up 2 seconds        0.0.0.0:8082-&gt;80/tcp   runc-test</span><br><span class="line">ddbd72466c4c        httpd:alpine        "httpd-foreground"   About a minute ago   Up About a minute   0.0.0.0:8080-&gt;80/tcp   kata-test</span><br><span class="line"></span><br><span class="line">sadmin@k8smaster:~$ ps -ef | grep docker | grep runtime | grep -v dockerd</span><br><span class="line">root     31346  1035  0 08:30 ?        00:00:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/ddbd72466c4ce862891538a8252a83450435b25f488cce98c9f8f9a8e51711e7 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-kata-runtime</span><br><span class="line">root     32169  1035  0 08:31 ?        00:00:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/c7aff63e2d239ea0d27641398f0d3f108999ba3a125c6cef9f1330ccb75c93d4 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line"></span><br><span class="line">root@k8smaster:/home/xdnsadmin# kata-runtime list    # 需要root权限</span><br><span class="line">ID                                                                 PID         STATUS      BUNDLE                     CREATED                          OWNER</span><br><span class="line">a5468f62e87ea88889d8773247d8b7863f6c8de13fc0a2c6b65ed987fa04db77   23146       running     /run/containers/storage/overlay-containers/a5468f62e87ea88889d8773247d8b7863f6c8de13fc0a2c6b65ed987fa04db77/userdata   2020-07-23T08:14:18.253804592Z   #0</span><br></pre></td></tr></table></figure><p>查看一下两个容器的内核版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ docker exec -it kata-test sh</span><br><span class="line">/usr/local/apache2 # uname -a</span><br><span class="line">Linux ddbd72466c4c 5.4.32-49.container #1 SMP Fri Jul 3 05:36:39 UTC 2020 x86_64 Linux</span><br><span class="line">/usr/local/apache2 # cat /proc/version</span><br><span class="line">Linux version 5.4.32-49.container (abuild@lamb04) (gcc version 7.3.0 (Ubuntu 7.3.0-16ubuntu3)) #1 SMP Fri Jul 3 05:36:39 UTC 2020</span><br><span class="line"></span><br><span class="line">admin@k8smaster:~$ docker exec -it runc-test sh</span><br><span class="line">/usr/local/apache2 # uname -a    # runc容器内核</span><br><span class="line">Linux c7aff63e2d23 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 Linux</span><br><span class="line"></span><br><span class="line">admin@k8smaster:~$ uname -a  #宿主机内核</span><br><span class="line">Linux ubuntu 4.15.0-112-generic #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure><p>查看一下kata容器的虚拟机，可以看到kata专门给容器启动了一个虚拟机沙盒</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$  ps -ef | grep qemu | grep -v 'grep'</span><br><span class="line">root     31390 31346  0 08:30 ?        00:00:03 /usr/bin/qemu-vanilla-system-x86_64 -name sandbox-ddbd72466c4ce862891538a8252a83450435b25f488cce98c9f8f9a8e51711e7 -uuid 0cbfc946-2979-4f44-9d82-62dcdb673c0a -machine pc,accel=kvm,kernel_irqchip,nvdimm -cpu host,pmu=off -qmp unix:/run/vc/vm/ddbd72466c4ce862891538a8252a83450435b25f488cce98c9f8f9a8e51711e7/qmp.sock,server,nowait -m 2048M,slots=10,maxmem=8995M -device pci-bridge,bus=pci.0,id=pci-bridge-0,chassis_nr=1,shpc=on,addr=2,romfile= -device virtio-serial-pci,disable-modern=true,id=serial0,romfile= -device virtconsole,chardev=charconsole0,id=console0 -chardev socket,id=charconsole0,path=/run/vc/vm/ddbd72466c4ce862891538a8252a83450435b25f488cce98c9f8f9a8e51711e7/console.sock,server,nowait -device nvdimm,id=nv0,memdev=mem0 -object memory-backend-file,id=mem0,mem-path=/usr/share/kata-containers/kata-containers-image_clearlinux_1.12.0-alpha0_agent_e01f289887.img,size=134217728 -device virtio-scsi-pci,id=scsi0,disable-modern=true,romfile= -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0,romfile= -device virtserialport,chardev=charch0,id=channel0,name=agent.channel.0 -chardev socket,id=charch0,path=/run/vc/vm/ddbd72466c4ce862891538a8252a83450435b25f488cce98c9f8f9a8e51711e7/kata.sock,server,nowait -device virtio-9p-pci,disable-modern=true,fsdev=extra-9p-kataShared,mount_tag=kataShared,romfile= -fsdev local,id=extra-9p-kataShared,path=/run/kata-containers/shared/sandboxes/ddbd72466c4ce862891538a8252a83450435b25f488cce98c9f8f9a8e51711e7/shared,security_model=none -netdev tap,id=network-0,vhost=on,vhostfds=3,fds=4 -device driver=virtio-net-pci,netdev=network-0,mac=02:42:ac:11:00:04,disable-modern=true,mq=on,vectors=4,romfile= -rtc base=utc,driftfix=slew,clock=host -global kvm-pit.lost_tick_policy=discard -vga none -no-user-config -nodefaults -nographic -daemonize -object memory-backend-ram,id=dimm1,size=2048M -numa node,memdev=dimm1 -kernel /usr/share/kata-containers/vmlinuz-5.4.32.80-49.container -append tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k console=hvc0 console=hvc1 cryptomgr.notests net.ifnames=0 pci=lastbus=0 iommu=off root=/dev/pmem0p1 rootflags=dax,data=ordered,errors=remount-ro ro rootfstype=ext4 quiet systemd.show_status=false panic=1 nr_cpus=4 agent.use_vsock=false systemd.unit=kata-containers.target systemd.mask=systemd-networkd.service systemd.mask=systemd-networkd.socket scsi_mod.scan=none -pidfile /run/vc/vm/ddbd72466c4ce862891538a8252a83450435b25f488cce98c9f8f9a8e51711e7/pid -smp 1,cores=1,threads=1,sockets=4,maxcpus=4</span><br></pre></td></tr></table></figure><h3 id="3-3-配置k8s与kata的集成"><a href="#3-3-配置k8s与kata的集成" class="headerlink" title="3.3 配置k8s与kata的集成"></a>3.3 配置k8s与kata的集成</h3><p>kata 不直接与 k8s 通信，因为对于 k8s 来说，它只跟实现了 CRI 接口的容器管理进程打交道，比如  docker-engine，rkt, containerd(使用 cri plugin) 或 CRI-O，而 kata 跟 runc  是同一个级别的进程。所以如果要与 k8s 集成，则需要安装 CRI-O 或 CRI-containerd 来支持 CRI 接口，本文使用  CRI-O。CRI-O 的 O 的意思是 OCI-compatible，即 CRI-O 是实现 CRI 接口来跑 OCI 容器的[4]。</p><p>由于ubuntu 18.04 的<a href="http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_18.04/" target="_blank" rel="noopener">suse源</a>中含有相关软件的包，这里展示如何直接用apt快速安装体验，相关软件的版本如表3.1所示</p><p>表3.1  相关软件版本</p><table><thead><tr><th>软件名</th><th>版本</th></tr></thead><tbody><tr><td>kubelet、kubeadm、kubectl</td><td>1.18.6</td></tr><tr><td>cri-o</td><td>1.17</td></tr><tr><td>kata</td><td>1.12-alpha0</td></tr></tbody></table><h4 id="3-3-1-安装-cri-o"><a href="#3-3-1-安装-cri-o" class="headerlink" title="3.3.1 安装 cri-o"></a>3.3.1 安装 cri-o</h4><p>CRI-O - OCI-based implementation of Kubernetes Container Runtime Interface<br>这是<a href="https://github.com/cri-o/cri-o" target="_blank" rel="noopener">cri-o</a>的github标题，符合OCI基准实现的Kubernetes容器运行时接口，从这个标题很容易看出，这是一个专门服务k8s的容器实现。</p><ul><li>CRI Container Runtime  Interface这是k8s提出的一个概念，在容器界除了最出名的docker和本文介绍的cri-o，还有很多不同的容器实现，例如contrainerd, frakti，rkt等，这些容器实现各有特色，只要支持CRI就可以被k8s支持</li><li>OCI Open Container  Initiative这是开放容器标准，也叫容器runtime，简单来说只要符合这个标准运行的就是容器，例如runC，Kata，gVisor这些runtime创建出来的都是OCI标准容器容器标准，也叫容器runtime，简单来说只要符合这个标准运行的，就是容器，例如runC，Kata，gVisor这些runtime创建出来的都是OCI标准容器</li></ul><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgcrio-logo.svg" alt="cri-o logo"></p><p>1）安装cri-o</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ CRIO_VERSION=1.17   # 当前ubuntu只有1.17</span><br><span class="line">admin@k8smaster:~$ . /etc/os-release</span><br><span class="line">admin@k8smaster:~$ sudo sh -c "echo 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/x$&#123;NAME&#125;_$&#123;VERSION_ID&#125;/ /' &gt;/etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list"</span><br><span class="line">admin@k8smaster:~$ wget -nv https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/x$&#123;NAME&#125;_$&#123;VERSION_ID&#125;/Release.key -O- | sudo apt-key add -</span><br><span class="line">admin@k8smaster:~$ sudo apt-get update -qq</span><br><span class="line">admin@k8smaster:~$ sudo apt-get install cri-o-$&#123;CRIO_VERSION&#125;</span><br></pre></td></tr></table></figure><p>2）修改crio配置文件<code>/etc/crio/crio.conf</code>，增加kata runtime</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ sudo vim /etc/crio/crio.conf</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置kata-runtime为容器运行时</span></span><br><span class="line">[crio.runtime.runtimes.kata-runtime]</span><br><span class="line">runtime_path = "/usr/bin/kata-runtime"</span><br><span class="line">runtime_type = "oci"</span><br><span class="line">runtime_root = ""</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 由于k8s.gcr.io 无法访问的原因修改镜像源</span></span><br><span class="line">pause_image = "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置crio镜像下载地址</span></span><br><span class="line">registries = ["docker.io"]</span><br></pre></td></tr></table></figure><p>3）修改crio systemd配置，设置自启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ sudo vim /usr/lib/systemd/system/crio.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">....</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">admin@k8smaster:~$ sudo systemctl daemon-reload</span><br><span class="line">admin@k8smaster:~$ sudo systemctl restart crio</span><br><span class="line">admin@k8smaster:~$ sudo service crio status</span><br><span class="line">[sudo] password for louie:</span><br><span class="line">● crio.service - Container Runtime Interface for OCI (CRI-O)</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/crio.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Thu 2020-07-23 02:13:39 UTC; 6h ago</span><br><span class="line">     Docs: https://github.com/cri-o/cri-o</span><br><span class="line"> Main PID: 1876 (crio)</span><br><span class="line">    Tasks: 21</span><br><span class="line">   CGroup: /system.slice/crio.service</span><br><span class="line">           └─1876 /usr/bin/crio</span><br></pre></td></tr></table></figure><p>4）测试crio</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ cat /etc/crictl.yaml</span><br><span class="line">runtime-endpoint: unix:///var/run/crio/crio.sock</span><br><span class="line">image-endpoint: unix:///var/run/crio/crio.sock</span><br><span class="line">timeout: 10</span><br><span class="line"></span><br><span class="line">admin@k8smaster:~$ sudo crictl version</span><br><span class="line">Version:  0.1.0</span><br><span class="line">RuntimeName:  cri-o</span><br><span class="line">RuntimeVersion:  1.17.4</span><br><span class="line">RuntimeApiVersion:  v1alpha1</span><br></pre></td></tr></table></figure><h4 id="3-3-2-安装k8s"><a href="#3-3-2-安装k8s" class="headerlink" title="3.3.2 安装k8s"></a>3.3.2 安装k8s</h4><p>1）添加k8s源并安装k8s</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使得 apt 支持 ssl 传输</span></span><br><span class="line">admin@k8smaster:~$ sudo apt-get update &amp;&amp; apt-get install -y apt-transport-https</span><br><span class="line"><span class="comment"># 下载 gpg 密钥</span></span><br><span class="line">admin@k8smaster:~$ curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -</span><br><span class="line"><span class="comment"># 添加 k8s 镜像源</span></span><br><span class="line">admin@k8smaster:~$ sudo cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># 更新源列表</span></span><br><span class="line">admin@k8smaster:~$ sudo apt-get update</span><br><span class="line"><span class="comment"># 下载 kubectl，kubeadm以及 kubelet</span></span><br><span class="line">admin@k8smaster:~$ sudo apt-get install -y kubelet=1.18.6-00 kubeadm=1.18.6-00 kubectl=1.18.6-00</span><br></pre></td></tr></table></figure><p>2）配置k8s使用crio</p><p>根据<a href="https://github.com/cri-o/cri-o/blob/master/tutorials/kubeadm.md" target="_blank" rel="noopener">官方手册</a>，通过cri-o运行k8s需要修改相应文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ sudo cat /etc/default/kubelet</span><br><span class="line">KUBELET_EXTRA_ARGS=--feature-gates="AllAlpha=false,RunAsGroup=true" --container-runtime=remote --cgroup-driver=systemd --container-runtime-endpoint='unix:///var/run/crio/crio.sock' --runtime-request-timeout=5m</span><br></pre></td></tr></table></figure><p>修改kubeadm配置，修改cgroup驱动为systemd</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ sudo vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">....</span><br><span class="line">Environment="KUBELET_EXTRA_ARGS=--container-runtime=remote --cgroup-driver=systemd --runtime-request-timeout=15m --container-runtime-endpoint=unix:///var/run/crio/crio.sock"</span><br></pre></td></tr></table></figure><p>重启kubelet</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ sudo systemctl daemon-reload; systemctl restart kubelet</span><br></pre></td></tr></table></figure><p>3）初始化k8s集群</p><p>这里通过制定k8s容器仓库为阿里云镜像仓库避免被墙而无法下载镜像，由于阿里云的镜像与官方不同步，因此这里镜像的版本指定为<code>v1.18.0</code>，稍微与kubeadm的镜像版本<code>v1.18.6</code>不一致。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ sudo kubeadm init --image-repository=registry.aliyuncs.com/google_containers --pod-network-cidr=192.168.0.0/16 --kubernetes-version=v1.18.0 --apiserver-advertise-address=10.37.129.3 --cri-socket=/var/run/crio/crio.sock --v=5</span><br><span class="line">....</span><br><span class="line"></span><br><span class="line">admin@k8smaster:~$ mkdir -p $HOME/.kube; cp -i /etc/kubernetes/admin.conf $HOME/.kube/config; chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure><ul><li><p><code>--pod-network-cidr=192.168.0.0/16</code>，若于主机所在网络冲突可修改</p></li><li><p><code>--apiserver-advertise-address=10.37.129.3</code> ，可不指定API地址，本次安装为了避免网络影响，给机器新增了一块网口</p></li></ul><hr><p><strong><em>【NOTE】</em></strong></p><p>1）想查看k8s对应镜像版本可以通过如下命令查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kubeadm config images list</span><br><span class="line">k8s.gcr.io/kube-apiserver:v1.18.6</span><br><span class="line">k8s.gcr.io/kube-controller-manager:v1.18.6</span><br><span class="line">k8s.gcr.io/kube-scheduler:v1.18.6</span><br><span class="line">k8s.gcr.io/kube-proxy:v1.18.6</span><br><span class="line">k8s.gcr.io/pause:3.2</span><br><span class="line">k8s.gcr.io/etcd:3.4.3-0</span><br><span class="line">k8s.gcr.io/coredns:1.6.7</span><br></pre></td></tr></table></figure><p>对于离线情况下，亦可手动下载镜像再导入，这里推荐一个博客<a href="https://www.cnblogs.com/kcxg/p/11457209.html" target="_blank" rel="noopener">Docker/Kubernetes镜像</a>，详细介绍了如何获取<code>gcr.io</code>镜像</p><p>2）手动安装相关软件最新版本可参考<a href="https://lingxiankong.github.io/2018-07-20-katacontainer-docker-k8s.html" target="_blank" rel="noopener">Katacontainers 与 Docker 和 Kubernetes 的集成</a>，文中相关软件版本较老，这里列出相关软件的最新realease版本</p><ul><li><p><a href="https://github.com/opencontainers/runc/releases" target="_blank" rel="noopener">Runc release</a></p></li><li><p><a href="https://github.com/kubernetes-sigs/cri-tools/releases" target="_blank" rel="noopener">Crictl release</a></p></li><li><p><a href="https://github.com/containers/conmon" target="_blank" rel="noopener">conmon</a></p></li><li><p><a href="https://github.com/containernetworking/plugins/releases" target="_blank" rel="noopener">CNI plugins</a></p></li></ul><hr><h4 id="3-3-3-配置k8s集群网络"><a href="#3-3-3-配置k8s集群网络" class="headerlink" title="3.3.3 配置k8s集群网络"></a>3.3.3 配置k8s集群网络</h4><p>这里选用calico网络，参考<a href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart" target="_blank" rel="noopener">calico官网</a></p><p>1）安装Tigera Calico操作符和自定义资源定义</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml</span><br></pre></td></tr></table></figure><p>2）通过创建必要的自定义资源来安装Calico</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml</span><br></pre></td></tr></table></figure><p>这里需要注意，若k8s集群初始化时<code>CIDR</code>指定的不是<code>192.168.0.0/16</code>需要修改<code>custom-resources.yaml</code>文件中的<code>CIDR</code>地址</p><p>3）确认所有pod都在运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ watch kubectl get pods -n calico-system</span><br></pre></td></tr></table></figure><p>等待所有pod的状态为<code>running</code></p><p>4）本次安装只使用了一个节点，需要配置master节点“无污点”</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line">node/&lt;your-hostname&gt; untainted</span><br></pre></td></tr></table></figure><p>5）查看所有master节点状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kubectl get nodes -o wide</span><br><span class="line">NAME     STATUS   ROLES    AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME</span><br><span class="line">ubuntu   Ready    master   6h36m   v1.18.6   10.211.55.5   &lt;none&gt;        Ubuntu 18.04.4 LTS   4.15.0-112-generic   cri-o://1.17.4</span><br></pre></td></tr></table></figure><p>6）查看所有pod状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kubectl get pods -A -o wide</span><br><span class="line">NAMESPACE         NAME                                       READY   STATUS    RESTARTS   AGE     IP                NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">calico-system     calico-kube-controllers-5687f44fd5-49d4d   1/1     Running   0          6h34m   192.168.243.194   ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-system     calico-node-grtnk                          1/1     Running   0          6h34m   10.211.55.5       ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">calico-system     calico-typha-7cd5478c69-vldhp              1/1     Running   0          6h34m   10.211.55.5       ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system       coredns-7ff77c879f-7b7lc                   1/1     Running   0          6h36m   192.168.243.192   ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system       coredns-7ff77c879f-m8glx                   1/1     Running   0          6h36m   192.168.243.195   ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system       etcd-ubuntu                                1/1     Running   0          6h36m   10.211.55.5       ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system       kube-apiserver-ubuntu                      1/1     Running   0          6h36m   10.211.55.5       ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system       kube-controller-manager-ubuntu             1/1     Running   0          6h36m   10.211.55.5       ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system       kube-proxy-mzpk5                           1/1     Running   0          6h36m   10.211.55.5       ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system       kube-scheduler-ubuntu                      1/1     Running   0          6h36m   10.211.55.5       ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">tigera-operator   tigera-operator-6659cdcd96-7nzsq           1/1     Running   0          6h35m   10.211.55.5       ubuntu   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h4 id="3-3-4-测试k8s使用kata-runtime"><a href="#3-3-4-测试k8s使用kata-runtime" class="headerlink" title="3.3.4 测试k8s使用kata runtime"></a>3.3.4 测试k8s使用kata runtime</h4><p>k8s从1.14版本开始设置了<a href="https://kubernetes.io/zh/docs/concepts/containers/runtime-class/" target="_blank" rel="noopener">runt imeclass</a> ，同时crio从1.14版本开始增加RuntimeHandler以取代<a href="https://lingxiankong.github.io/2018-07-20-katacontainer-docker-k8s.html" target="_blank" rel="noopener">trusted/untrusted runtime</a>，因此k8s使用kata不能再使用如下配置了。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">annotations:</span></span><br><span class="line">        <span class="string">io.kubernetes.cri-o.TrustedSandbox:</span> <span class="string">"false"</span></span><br></pre></td></tr></table></figure><p>1）首先创建kata runtime class</p><p>编写kata runtime class配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># k8s-runtimecleass.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">node.k8s.io/v1beta1</span>  <span class="comment"># RuntimeClass is defined in the node.k8s.io API group</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RuntimeClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kata</span></span><br><span class="line"><span class="attr">handler:</span> <span class="string">kata-runtime</span>  <span class="comment"># handler 名称需要与/etc/crio/crio.conf配置一致</span></span><br></pre></td></tr></table></figure><p>创建kata runtime class</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kubectl apply -f k8s-runtimecleass.yaml</span><br><span class="line">admin@k8smaster:~$ kubectl get runtimeclass</span><br><span class="line">NAME   HANDLER        AGE</span><br><span class="line">kata   kata-runtime   1m</span><br></pre></td></tr></table></figure><p>2）创建kata runtime pod</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ cat kata-test.yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: test-kata</span><br><span class="line">  labels:</span><br><span class="line">    app: kata</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: kata</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: kata</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: test-kata</span><br><span class="line">        image: lingxiankong/alpine-test</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">      runtimeClassName: kata  # 注明使用kata runtime</span><br><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-kata</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: kata</span><br><span class="line">  ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 8080</span><br><span class="line"></span><br><span class="line">admin@k8smaster:~$ kubectl apply -f kata-test.yaml</span><br></pre></td></tr></table></figure><p>等待kata pod 启动，查看pod状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">admin@k8smaster:~$ kubectl get pod -o wide</span><br><span class="line">NAME                         READY   STATUS    RESTARTS   AGE   IP                NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">test-kata-64b4c55f9c-bh7xl   1/1     Running   0          25h   192.168.243.199   ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-kata-64b4c55f9c-cjvg7   1/1     Running   0          25h   192.168.243.198   ubuntu   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">admin@k8smaster:~$ kubectl get pod test-kata</span><br><span class="line">admin@k8smaster:~$ kubectl get deployment test-kata</span><br><span class="line">admin@k8smaster:~$ kubectl get svc test-kata</span><br></pre></td></tr></table></figure><p>【参考链接】</p><p>1）<a href="https://jiajunhuang.com/articles/2018_12_24-docker_components_part2.md.html" target="_blank" rel="noopener">Docker组件介绍（二）：shim, docker-init和docker-proxy</a></p><p>2）<a href="https://jiajunhuang.com/articles/2018_12_22-docker_components.md.html" target="_blank" rel="noopener">Docker组件介绍（一）：runc和containerd</a></p><p>3）<a href="https://github.com/kata-containers/documentation/blob/master/install/ubuntu-installation-guide.md" target="_blank" rel="noopener">Install Kata Containers on Ubuntu</a></p><p>4）<a href="https://lingxiankong.github.io/2018-07-20-katacontainer-docker-k8s.html" target="_blank" rel="noopener">Katacontainers 与 Docker 和 Kubernetes 的集成</a></p>]]></content>
    
    <summary type="html">
    
      K8S 搭配 Kata container
    
    </summary>
    
      <category term="K8S" scheme="https://louielong.github.io/categories/K8S/"/>
    
    
      <category term="k8s" scheme="https://louielong.github.io/source/tags/k8s/"/>
    
      <category term="docker" scheme="https://louielong.github.io/source/tags/docker/"/>
    
      <category term="kata container" scheme="https://louielong.github.io/source/tags/kata-container/"/>
    
  </entry>
  
  <entry>
    <title>【转载】扩展 Kubernetes 之 CRI</title>
    <link href="https://louielong.github.io/k8s-cri.html"/>
    <id>https://louielong.github.io/k8s-cri.html</id>
    <published>2020-07-16T02:49:52.000Z</published>
    <updated>2020-07-16T05:24:05.582Z</updated>
    
    <content type="html"><![CDATA[<p>【转载】本文转载自<a href="https://cloud.tencent.com/developer/article/1579900" target="_blank" rel="noopener">扩展 Kubernetes 之 CRI</a></p><p>最近在研究k8s搭配kata container，对于其中的CRI很是困惑，借由这篇文章学习一下CRI及其相关概念。</p><h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><h3 id="1-1-CRI-是什么"><a href="#1-1-CRI-是什么" class="headerlink" title="1.1 CRI 是什么"></a>1.1 CRI 是什么</h3><p>容器运行时插件（Container Runtime Interface，简称 CRI）是 Kubernetes v1.5 引入的容器运行时接口，它将 Kubelet 与容器运行时解耦，将原来完全面向 Pod 级别的内部接口拆分成面向 Sandbox 和 Container 的 gRPC 接口，并将镜像管理和容器管理分离到不同的服务。</p><p>CRI 主要定义了两个 grpc interface.</p><ul><li><code>RuntimeService</code>：容器(container) 和 (Pod)Sandbox 运行时管理</li><li><code>ImageService</code>：拉取、查看、和移除镜像</li></ul><p>OCI (开放容器标准): 定义了 ImageSpec（镜像格式, 比如文件夹结构，压缩方式）和 RuntimeSpec（如何运行，比如支持 create, start, stop, delete）。代表实现有：runC，Kata（以及它的前身 runV 和 Clear Containers），gVisor。</p><p>CRI 区别于 OCI，CRI的定义比较简单直接，只是定义了一套协议（grpc 接口）。代表实现有 kubernetest 内置的 dockershim, CRI-containerd（或者 containerd with CRI plugin）, cri-o</p><h3 id="1-2-CRI-位于什么位置"><a href="#1-2-CRI-位于什么位置" class="headerlink" title="1.2 CRI 位于什么位置"></a>1.2 CRI 位于什么位置</h3><p>在 kubernetes 中:</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgxlyb0th1c7.png" alt="k8belet与CRI"></p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgrchsiecsii.png" alt="CRI组成"></p><p>在和OCI，调度层的角度看:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">OrchestrationAPI --&gt; ContainerAPI-criRuntime</span><br><span class="line">ContainerAPI-criRuntime --&gt; KernelAPI-ociRuntime</span><br></pre></td></tr></table></figure><h2 id="二、CRI-CRI-Runtime"><a href="#二、CRI-CRI-Runtime" class="headerlink" title="二、CRI/CRI Runtime"></a>二、CRI/CRI Runtime</h2><h3 id="2-1-CRI-Runtime-的执行流程"><a href="#2-1-CRI-Runtime-的执行流程" class="headerlink" title="2.1 CRI Runtime 的执行流程"></a>2.1 CRI Runtime 的执行流程</h3><p>经典的 kubernetes runtime 执行流程</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img00r84irf8c5.png" alt="k8s runtime 流程"></p><ul><li>执行流程里面核心组件是 kubelet/KubeGenericRuntimeManager 他调用很多其他组件，比如 cm (ContainerManager/podContainerManager/cgroupManager/cpuManager/deviceManager, pod 级别的资源管理), RuntimeService (grpc 调用 CRI 的客户端) 共同完成 SyncPod 的操作。</li></ul><p>经典的 dockershim -&gt; containerd 的流程 (称为 docker cri)</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgcczv8nowa.png" alt="k8s调用docker流程"></p><ol><li>Kubelet 通过 CRI 接口（gRPC）调用 dockershim，请求创建一个容器。CRI 即容器运行时接口（Container Runtime Interface），这一步中，Kubelet 可以视作一个简单的 CRI Client，而 dockershim 就是接收请求的 Server。目前 dockershim 的代码其实是内嵌在 Kubelet 中的，所以接收调用的凑巧就是 Kubelet 进程</li><li>dockershim 收到请求后，转化成 Docker Daemon 能听懂的请求，发到 Docker Daemon 上请求创建一个容器。</li><li>Docker Daemon 早在 1.12 版本中就已经将针对容器的操作移到另一个守护进程——containerd 中了，因此 Docker Daemon 仍然不能帮我们创建容器，而是要请求 containerd 创建一个容器；</li><li>containerd 收到请求后，并不会自己直接去操作容器，而是创建一个叫做 containerd-shim 的进程，让 containerd-shim 去操作容器。这是因为容器进程需要一个父进程来做诸如收集状态，维持 stdin 等 fd 打开等工作。而假如这个父进程就是 containerd，那每次 containerd 挂掉或升级，整个宿主机上所有的容器都得退出了。而引入了 containerd-shim 就规避了这个问题（containerd 和 shim 并不是父子进程关系）；</li><li>我们知道创建容器需要做一些设置 namespaces 和 cgroups，挂载 root filesystem 等等操作，而这些事该怎么做已经有了公开的规范了，那就是 OCI（Open Container Initiative，开放容器标准）。它的一个参考实现叫做 runC。于是，containerd-shim 在这一步需要调用 runC 这个命令行工具，来启动容器；</li><li>runC 启动完容器后本身会直接退出，containerd-shim 则会成为容器进程的父进程，负责收集容器进程的状态，上报给 containerd，并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理，确保不会出现僵尸进程。</li></ol><p>直接对接 cri-containerd/cri-o 的运行时</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img7z2bq3l54g.png" alt="k8s对接containerd"></p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgv9xrgk5bht.png" alt="k8s对接cri-o"></p><p>使用 cri-containerd 的调用流程更为简洁, 省去了上面的调用流程的 1，2 两步</p><p>常见 CRI runtime 实现</p><table><thead><tr><th>CRI容器运行时</th><th>维护者</th><th>主要特性</th><th>容器引擎</th></tr></thead><tbody><tr><td>Dockershim</td><td>Kubernetes</td><td>内置实现、特性最新</td><td>docker</td></tr><tr><td>cri-o</td><td>Kubernetes</td><td>OCI标准</td><td>OCI(runc、kaata、gVisor)</td></tr><tr><td>cri-containerd</td><td>Containerd</td><td>基于containerd</td><td>OCI(runc、kaata、gVisor)</td></tr><tr><td>Frakti</td><td>Kubernetes</td><td>虚拟化容器</td><td>hyperd、docker</td></tr><tr><td>rktlet</td><td>Kubernetes</td><td>支持rkt</td><td>rkt</td></tr><tr><td>PouchContainer</td><td>Alibaba</td><td>富容器</td><td>OCI(runc、kata)</td></tr><tr><td>Virlet</td><td>Mirantis</td><td>虚拟机和QCOW2镜像</td><td>Libvirt(KVM)</td></tr></tbody></table><p>Cri-containerd</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgztykbbu2la.png" alt="img"></p><p>执行流程为:</p><ol><li>Kubelet 通过 CRI runtime service API 调用 cri plugin 创建 pod</li><li>cri 通过 CNI 创建 pod 的网络配置和 namespace</li><li>cri 使用 containerd 创建并启动 pause container (sandbox container) 并且把这个 container 置于 pod 的 cgroups/namespace</li><li>Kubelet 接着通过 CRI image service API 调用 cri plugin, 获取容器镜像</li><li>cri 通过 containerd 获取容器镜像</li><li>Kubelet 通过 CRI runtime service API 调用 cri, 在 pod 的空间使用拉取的镜像启动容器</li><li>cri 通过 containerd 创建/启动 应用容器, 并且把 container 置于 pod 的 cgroups/namespace. Pod 完成启动.</li></ol><h2 id="【参考文献】"><a href="#【参考文献】" class="headerlink" title="【参考文献】"></a>【参考文献】</h2><p>1）<a href="https://feisky.gitbooks.io/kubernetes/plugins/CRI.html" target="_blank" rel="noopener">CRI-feisky</a></p><p>2）<a href="https://blog.csdn.net/u011563903/article/details/90743853" target="_blank" rel="noopener">K8S Runtime CRI OCI contained dockershim 理解</a></p><p>3）<a href="https://github.com/kubernetes/cri-api/blob/master/pkg/apis/runtime/v1alpha2/api.proto" target="_blank" rel="noopener">CRI API 定义</a></p><p>4）<a href="https://github.com/containerd/cri" target="_blank" rel="noopener">cri-containerd</a></p><p>5）<a href="https://github.com/containerd/containerd" target="_blank" rel="noopener">containerd</a></p><p>6）<a href="https://zhuanlan.zhihu.com/p/47012368" target="_blank" rel="noopener">K8S 容器运行时-feisky</a></p><p>7）<a href="https://github.com/kubernetes-sigs/cri-tools" target="_blank" rel="noopener">critools</a></p>]]></content>
    
    <summary type="html">
    
      扩展 Kubernetes 之 CRI
    
    </summary>
    
      <category term="k8s" scheme="https://louielong.github.io/categories/k8s/"/>
    
    
      <category term="k8s" scheme="https://louielong.github.io/source/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>【转载】容器、容器云和k8s技术</title>
    <link href="https://louielong.github.io/container-and-container-cloud.html"/>
    <id>https://louielong.github.io/container-and-container-cloud.html</id>
    <published>2020-07-15T05:28:02.000Z</published>
    <updated>2020-07-15T07:12:39.441Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自:<a href="https://www.cnblogs.com/technologykai/articles/10601317.html" target="_blank" rel="noopener">容器、容器云与Kubernetes技术—–史上最全最易懂解释</a></p><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p><strong>我们为什么使用容器？</strong></p><p>我们为什么使用虚拟机（云主机）？ 为什么使用物理机？  这一系列的问题并没有一个统一的标准答案。因为以上几类技术栈都有自身最适用的场景，在最佳实践之下，它们分别都是不可替代的。  原本没有虚拟机，所有类型的业务应用都直接跑在物理主机上面，计算资源和存储资源都难于增减，要么就是一直不够用，要么就一直是把过剩的资源浪费掉，所以后来我们看到大家越来越多得使用虚拟机（或云主机），物理机的使用场景被极大地压缩到了像数据库系统这样的特殊类型应用上面。</p><p>原本也没有容器，我们把大部分的业务应用跑在虚拟机（或云主机）上面，把少部分特殊类型的应用仍然跑在物理主机上面。但现在所有的虚拟机技术方案，都无法回避两个主要的问题，一个问题是虚拟化Hypervisor管理软件本身的资源消耗与磁盘IO性能降低，另一个是虚拟机仍然还是一个独立的操作系统，对很多类型的业务应用来说都显得太重了，导致我们在处理虚拟机的扩缩容与配置管理工作时效率低下。所以，我们后来发现了容器的好处，所有业务应用可以直接运行在物理主机的操作系统之上，可以直接读写磁盘，应用之间通过计算、存储和网络资源的命名空间进行隔离，为每个应用形成一个逻辑上独立的“容器操作系统”。除此之外，容器技术还有以下优点：简化部署、多环境支持、快速启动、服务编排、易于迁移。</p><p>容器技术的一些缺点：仍然不能做到彻底的安全隔离，技术栈复杂度飚升，尤其是在应用了容器集群技术之后。所以如果只是小规模的使用，做实验或测试是可以的，上生产环境需要三思而后行。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img404432816.jpg" alt="云计算与容器架构图"></p><h2 id="二、容器相关介绍"><a href="#二、容器相关介绍" class="headerlink" title="二、容器相关介绍"></a>二、容器相关介绍</h2><h3 id="2-1-容器的运行原理与基本组件"><a href="#2-1-容器的运行原理与基本组件" class="headerlink" title="2.1 容器的运行原理与基本组件"></a>2.1 容器的运行原理与基本组件</h3><p>Docker容器主要基于以下三个关键技术实现的：</p><ul><li>Namespaces </li><li>Cgroups技术</li><li>Image镜像</li></ul><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img493529346.png" alt="容器镜像"></p><h3 id="2-2-容器引擎"><a href="#2-2-容器引擎" class="headerlink" title="2.2 容器引擎"></a>2.2 容器引擎</h3><p>容器引擎（Engine）或者容器运行时（Runtime）是容器系统的核心，也是很多人使用“容器”这个词语的指代对象。容器引擎能够创建和运行容器，而容器的定义一般是以文本方式保存的，比如 Dockerfile。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img570842057.png" alt="rkt和docker进程"></p><ul><li>Docker Engine ：目前最流行的容器引擎，也是业界的事实标准。</li><li>Rkt：CoreOS 团队推出的容器引擎，有着更加简单的架构，一直作为 Docker 的直接竞争对手存在，是 kubernetes 调度系统支持的容器引擎之一。</li><li>containerd：这个新的Daemon是对Docker内部组件的一个重构以便支持OCI规范，containerd  主要职责是镜像管理（镜像、元信息等）、容器执行（调用最终运行时组件执行），向上为 Docker Daemon 提供了 gRPC 接口，向下通过  containerd-shim 结合 runC，使得引擎可以独立升级。</li><li>docker-shim：shim 通过调用 containerd 启动 docker  容器，所以每启动一个容器都会起一个新的docker-shim进程。docker-shim是通过指定的三个参数：容器id，boundle目录和运行时（默认为runC）来调用runC的api创建一个容器。</li><li>runC ：是 Docker 按照开放容器格式标准（OCF, Open Container  Format）制定的一种具体实现，实现了容器启停、资源隔离等功能，所以是可以不用通过 docker  引擎直接使用runC运行一个容器的。也支持通过改变参数配置，选择使用其他的容器运行时实现。RunC可以说是各大CaaS厂商间合纵连横、相互妥协的结果，</li></ul><p><em>注：RunC在各个CaaS厂商的推动下在生产环境得到广泛的应用。Kubernetes目前基本只支持RunC容器，对于Docker超出其容器抽象层之外的功能，一概不支持。同样，Mesos也通过其Unified Containerizer只支持RunC容器，目前还支持Docker，但是未来的规划是只支持Unified  Containerizer。CF也通过Garden只支持RunC，不支持Docker超出RunC之前的功能。</em></p><p><strong>【笔者注】：目前还有runv为代表的容器runtime，k8s也是支持的。</strong></p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img2081789657.png" alt="docker进程分解"></p><p>为什么在容器的启动或运行过程中需要一个 docker-containerd-shim 进程呢？ </p><p>其目的有如下几点： </p><ul><li>它允许容器运行时(即 runC)在启动容器之后退出，简单说就是不必为每个容器一直运行一个容器运行时(runC) </li><li>即使在 containerd 和 dockerd 都挂掉的情况下，容器的标准 IO 和其它的文件描述符也都是可用的 </li><li>向 containerd 报告容器的退出状态</li></ul><p>rkt与containerd的区别是什么？ </p><p>一个主要的不同之处是，rkt作为一个无守护进程的工具（daemonless  tool），可以用来在生产环境中，集成和执行那些特别的有关键用途的容器。举个例子，CoreOS Container  Linux使用rkt来以一个容器镜像的方式执行Kubernetes的agent，即kublet。更多的例子包括在Kubernetes生态环境中，使用rkt来用一种容器化的方式挂载volume。这也意味着rkt能被集成进并和Linux的init系统一起使用，因为rkt自己并不是一个init系统。kubernets支持容器进行部署，其所支持的容器不只是仅仅局限于docker，CoreOS的rkt也是容器玩家之一，虽然跟docker比起来还是明显处于绝对下风，但有竞争总要好过没有。</p><h3 id="2-3-容器编排和管理系统"><a href="#2-3-容器编排和管理系统" class="headerlink" title="2.3 容器编排和管理系统"></a>2.3 容器编排和管理系统</h3><p>容器是很轻量化的技术，相对于物理机和虚机而言，这意味着在等量资源的基础上能创建出更多的容器实例出来。一旦面对着分布在多台主机上且拥有数百套容器的大规模应用程序时，传统的或单机的容器管理解决方案就会变得力不从心。另一方面，由于为微服务提供了越来越完善的原生支持，在一个容器集群中的容器粒度越来越小、数量越来越多。在这种情况下，容器或微服务都需要接受管理并有序接入外部环境，从而实现调度、负载均衡以及分配等任务。 简单而高效地管理快速增涨的容器实例，自然成了一个容器编排系统的主要任务。</p><p>容器集群管理工具能在一组服务器上管理多容器组合成的应用，每个应用集群在容器编排工具看来是一个部署或管理实体，容器集群管理工具全方位为应用集群实现自动化，包括应用实例部署、应用更新、健康检查、弹性伸缩、自动容错等等。 容器编排和管理系统的分层结构图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img562040862.png" alt="容器应用架构"></p><p>容器编排和管理系统界的主要选手:</p><ul><li>Kubernetes：Google 开源的容器管理系统，起源于内部历史悠久的 Borg  系统。因为其丰富的功能被多家公司使用，其发展路线注重规范的标准化和厂商“中立”，支持底层不同的容器运行时和引擎（比如 Rkt），逐渐解除对  Docker  的依赖。Kubernetes的核心是如何解决自动部署，扩展和管理容器化（containerized）应用程序。目前该项目在github上Star数量为43k。 </li><li>Docker Swarm： 在 Docker 1.2 版本后将 Swarm 集成在了 Docker 引擎中。用户能够轻松快速搭建出来  docker 容器集群，几乎完全兼容 docker API 的特性。目前该项目在github上Star数量为5.3k。 </li><li>Mesosphere Marathon：Apache Mesos  的调度框架目标是成为数据中心的操作系统，完全接管数据中心的管理工作。Mesos理念是数据中心操作系统（DCOS），为了解决IaaS层的网络、计算和存储问题，所以Mesos的核心是解决物理资源层的问题。Marathon是为Mesosphere DC/OS和Apache Mesos设计的容器编排平台。目前该项目在github上Star数量为3.7k。</li></ul><p><em>注：国内外有很多公司在从事基于上面三个基础技术平台的创新创业，为企业提供增值服务，其中做得不错的如Rancher，其产品可以同时兼容 kubernetes、mesos 和 swarm 集群系统，此外还有很多商用解决方案，如OpenShift。</em></p><p>中国市场的表现 在中国市场，2017 年 6 月 Kubernetes 中国社区 K8SMeetup  曾组织了国内首个针对中国容器开发者和企业用户的调研。近 100 个受访用户和企业中给我们带来了关于 Kubernetes 在中国落地状况的一手调查资料显示：</p><ul><li>在容器编排工具中，Kubernetes占据了70%市场份额，此外是Mesos约11%，Swarm不足7%； </li><li>在中国受访企业用户中，Kubernetes 平台上运行的应用类型非常广泛，几乎包括了除hadoop大数据技术栈以外的各种类型应用； </li><li>中国受访企业运行 Kubernetes  的底层环境分布显示，29%的客户使用裸机直接运行容器集群，而在包括OpenStack、VMWare、阿里云和腾讯云在内的泛云平台上运行容器集群服务的客户占到了60%；</li></ul><h3 id="2-4-关于CNCF基金会"><a href="#2-4-关于CNCF基金会" class="headerlink" title="2.4 关于CNCF基金会"></a>2.4 关于CNCF基金会</h3><p>主要的容器技术厂商（包括 Docker、CoreOS、Google、Mesosphere、RedHat 等）成立了 Cloud  Native Computing Foundation (CNCF) 。 CNCF对云原生的定义是： –  云原生技术帮助公司和机构在公有云、私有云和混合云等新型动态环境中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器、服务网格、微服务、不可变基础设施和声明式API。 –  这些技术能够构建容错性好、易于管理和便于观察的松耦合系统。结合可靠的自动化手段，云原生技术可以使开发者轻松地对系统进行频繁并可预测的重大变更。 – 云原生计算基金会（CNCF）致力于培育和维护一个厂商中立的开源生态系统，来推广云原生技术。我们通过将最前沿的模式普惠，让这些创新为大众所用。</p><ul><li>云原生以容器为核心技术，分为运行时（Runtime）和 Orchestration 两层，Runtime 负责容器的计算、存储、网络；Orchestration 负责容器集群的调度、服务发现和资源管理。</li></ul><p><em>注：上图只截取了原图的核心组件部分，完整图表详见<a href="https://landscape.cncf.io/images/landscape.png" target="_blank" rel="noopener">https://landscape.cncf.io/images/landscape.png</a></em></p><h2 id="三、Kubernetes的核心组件"><a href="#三、Kubernetes的核心组件" class="headerlink" title="三、Kubernetes的核心组件"></a>三、Kubernetes的核心组件</h2><h3 id="3-1-Kubernetes核心组件介绍"><a href="#3-1-Kubernetes核心组件介绍" class="headerlink" title="3.1 Kubernetes核心组件介绍"></a>3.1 Kubernetes核心组件介绍</h3><p>如下为k8s核心组件示意图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img1366511983.png" alt="k8s核心组件示意图"></p><ul><li>etcd是Kubernetes的存储状态的分布式数据库，采用raft协议作为一致性算法（raft协议原理可参见一个动画演示<a href="http://thesecretlivesofdata.com/raft/）。" target="_blank" rel="noopener">http://thesecretlivesofdata.com/raft/）。</a></li><li>API Server组件主要提供认证与授权、运行一组准入控制器以及管理API版本等功能，通过REST API向外提供服务，允许各类组件创建、读取、写入、更新和监视资源（Pod, Deployment, Service等）。</li><li>Scheduler组件，根据集群资源和状态选择合适的节点用于创建Pod。</li><li>Controller Manager组件，实现ReplicaSet的行为。</li><li>Kubelet组件，负责监视绑定到其所在节点的一组Pod，并且能实时返回这些Pod的运行状态。</li></ul><h3 id="3-2-Pod创建流程"><a href="#3-2-Pod创建流程" class="headerlink" title="3.2 Pod创建流程"></a>3.2 Pod创建流程</h3><p>创建Pod的整个流程时序图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img1637489187.png" alt="k8s创建pod流程图"></p><h3 id="3-3-容器网络"><a href="#3-3-容器网络" class="headerlink" title="3.3 容器网络"></a>3.3 容器网络</h3><p>容器的大规模使用，也对网络提供了更高的要求。网络的不灵活也是很多企业的短板，目前也有很多公司和项目在尝试解决这些问题，希望提出容器时代的网络方案。 Docker采用插件化的网络模式，默认提供bridge、host、none、overlay、macvlan和Network  plugins这几种网络模式，运行容器时可以通过<code>--network</code>参数设置具体使用那一种模式。</p><ul><li><p>bridge：这是Docker默认的网络驱动，此模式会为每一个容器分配Network Namespace和设置IP等，并将容器连接到一个虚拟网桥上。如果未指定网络驱动，这默认使用此驱动。 – host：此网络驱动直接使用宿主机的网络。</p></li><li><p>none：此驱动不构造网络环境。采用了none 网络驱动，那么就只能使用loopback网络设备，容器只能使用127.0.0.1的本机网络。</p></li><li><p>overlay：此网络驱动可以使多个Docker daemons连接在一起，并能够使用swarm服务之间进行通讯。也可以使用overlay网络进行swarm服务和容器之间、容器之间进行通讯，</p></li><li><p>macvlan：此网络允许为容器指定一个MAC地址，允许容器作为网络中的物理设备，这样Docker daemon就可以通过MAC地址进行访问的路由。对于希望直接连接网络网络的遗留应用，这种网络驱动有时可能是最好的选择。</p></li><li><p>Network plugins：可以安装和使用第三方的网络插件。可以在Docker Store或第三方供应商处获取这些插件。</p></li></ul><p>在默认情况，Docker使用bridge网络模式。</p><p><strong>容器网络模型（CNM）</strong></p><p>CNM在2015年由Docker引入，CNM有IP 地址管理（IPAM）和网络插件功能。IPAM插件可以创建IP地址池并分配，删除和释放容器IP。网络插件API用于创建/删除网络，并从网络中添加/删除容器。</p><p><strong>容器网络接口（CNI）</strong></p><p>CNI诞生于2015年4月，由CoreOS公司推出，CNI是容器中的网络系统插件，它使得类似Kubernetes之类的管理平台更容易的支持IPAM、SDN或者其它网络方案。CNI实现的基本思想为：Contianer runtime在创建容器时，先创建好network  namespace，这在实际操作过程中，首先创建出来的容器是Pause容器。之后调用CNI插件为这个netns配置网络，最后在启动容器内的进程。</p><p>CNI Plugin负责为容器配置网络，包括两个基本接口： </p><ul><li>配置网络：AddNetwork(net NetworkConfig, rt RuntimeConf) (types.Result, error) </li><li>清理网络：DelNetwork(net NetworkConfig, rt RuntimeConf) error</li></ul><p>每个CNI插件只需实现两种基本操作：创建网络的ADD操作，和删除网络的DEL操作（以及一个可选的VERSION查看版本操作）。所以CNI的实现确实非常简单，把复杂的逻辑交给具体的Network Plugin实现。</p><h3 id="3-4-Kubernetes-CNI-插件"><a href="#3-4-Kubernetes-CNI-插件" class="headerlink" title="3.4 Kubernetes CNI 插件"></a>3.4 Kubernetes CNI 插件</h3><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img01438768223.png" alt="k8s CNI插件"></p><ul><li>Flannel：CoreOS 开源的网络方案，为 kubernetes  设计，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。Flannel的底层通信协议的可选余地有很多，比如UDP、VXlan、AWS VPC等等，不同协议实现下的网络通信效率相差较多，默认为使用UDP协议，部署和管理简单。目前为止，还不支持k8s的Network  Policy。</li><li>Calico：一个纯三层的网络解决方案，使用 BGP 协议进行路由，可以集成到 openstack 和  docker。Calico节点组网可以直接利用数据中心的网络结构（无论是L2或者L3），不需要额外的NAT，隧道或者Overlay  Network，网络通信性能好。Calico基于iptables还提供了丰富而灵活的网络Policy，保证通过各个节点上的ACLs来提供Workload的多租户隔离、安全组以及其他可达性限制等功能。如果企业生产环境可以开启BGP协议，可以考虑calico bgp方案。不过在现实中的网络并不总是支持BGP路由的，因此Calico也设计了一种IPIP模式，使用Overlay的方式来传输数据。</li><li>Weave Net：weaveworks 给出的网络的方案，使用 vxlan 技术通过Overlay网络实现的， 支持网络的隔离和安全，安装和使用都比较简单。</li><li>Contiv: 思科开源，兼容CNI模型以及CNM模型，支持 VXLAN 和 VLAN  方案，配置较复杂。支持Tenant，支持租户隔离，支持多种网络模式（L2、L3、overlay、cisco sdn  solution）。Contiv带来的方便是用户可以根据容器实例IP直接进行访问。</li><li>Canal：基于 Flannel 和 Calico 提供 Kubernetes Pod 之间网络防火墙的项目。</li><li>Cilium：利用 Linux 原生技术提供的网络方案，支持 L7 和 L3、L4 层的访问策略。</li><li>Romana：Panic Networks 推出的网络开源方案，基于 L3 实现的网络连通，因此没有 Overlay 网络带来的性能损耗，但是只能通过 IP 网段规划来实现租户划分。</li></ul><p>从理论上说，这些CNI工具的网络速度应该可以分为3个速度等级。 </p><ul><li>最快的是Romana、Gateway模式的Flannel、BGP模式的Calico。</li><li>次一级的是IPIP模式的Calico、Swarm的Overlay网络、VxLan模式的Flannel、Fastpath模式的Weave。 </li><li>最慢的是UDP模式的Flannel、Sleeve模式的Weave。</li></ul><h4 id="3-4-1-Flannel"><a href="#3-4-1-Flannel" class="headerlink" title="3.4.1 Flannel"></a>3.4.1 Flannel</h4><ul><li>UDP封包使用了Flannel自定义的一种包头协议，数据是在Linux的用户态进行封包和解包的，因此当数据进入主机后，需要经历两次内核态到用户态的转换。网络通信效率低且存在不可靠的因素。</li><li>VxLAN封包采用的是内置在Linux内核里的标准协议，因此虽然它的封包结构比UDP模式复杂，但由于所有数据装、解包过程均在内核中完成，实际的传输速度要比UDP模式快许多。Vxlan方案在做大规模应用时复杂度会提升，故障定位分析复杂。</li><li>Flannel的Gateway模式与Calico速度相当，甚至理论上还要快一点。Flannel的Host-Gateway模式，在这种模式下，Flannel通过在各个节点上的Agent进程，将容器网络的路由信息刷到主机的路由表上，这样一来所有的主机就都有整个容器网络的路由数据了。Host-Gateway的方式没有引入像Overlay中的额外装包解包操作，完全是普通的网络路由机制，它的效率与虚拟机直接的通信相差无几。Host-Gateway的模式就只能用于二层直接可达的网络，由于广播风暴的问题，这种网络通常是比较小规模的。路由网络对现有网络设备影响比较大，路由器的路由表有空间限制，一般是两三万条，而容器的大部分应用场景是运行微服务，数量集很大。</li></ul><p>Flannel网络通信原理示意图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img173868194.png" alt="flannel通信示意图"></p><h3 id="3-5-容器存储"><a href="#3-5-容器存储" class="headerlink" title="3.5 容器存储"></a>3.5 容器存储</h3><p>因为容器存活时间很短的特点，容器的状态（存储的数据）必须独立于容器的生命周期，也因为此，容器的存储变得非常重要。 </p><ul><li><p>Ceph：分布式存储系统，同时支持块存储、文件存储和对象存储，发展时间很久，稳定性也得到了验证。之前在 OpenStack  社区被广泛使用，目前在容器社区也是很好的选项。 </p></li><li><p>GlusterFS：RedHat 旗下的产品，部署简单，扩展性强。</p><p>商业存储：DELL  EMC，NetApp等。 </p></li><li><p>CSI（Container Storage  Interface）：定义云应用调度平台和各种存储服务接口的项目，核心的目标就是存储 provider 只需要编写一个  driver，就能集成到任何的容器平台上。 </p></li><li><p>Rook：基于 Ceph 作为后台存储技术，深度集成到 Kubernetes  容器平台的容器项目，因为选择了 Ceph 和 Kubernetes  这两个都很热门的技术，并且提供自动部署、管理、扩展、升级、迁移、灾备和监控等功能</p></li></ul><h4 id="3-5-1-Kubernetes支持的存储类型"><a href="#3-5-1-Kubernetes支持的存储类型" class="headerlink" title="3.5.1 Kubernetes支持的存储类型"></a>3.5.1 Kubernetes支持的存储类型</h4><p>k8s支持以下存储类型</p><ul><li>awsElasticBlockStore</li><li>azureDisk</li><li>azureFile</li><li>cephfs</li><li>configMap</li><li>csi</li><li>downwardAPI</li><li>emptyDir</li><li>fc (fibre channel)</li><li>flocker</li><li>gcePersistentDisk</li><li>gitRepo (deprecated)</li><li>glusterfs</li><li>hostPath</li><li>iscsi</li><li>local</li><li>nfs</li><li>persistentVolumeClaim</li><li>projected</li><li>portworxVolume</li><li>quobyte</li><li>rbd</li><li>scaleIO</li><li>secret</li><li>storageos</li><li>vsphereVolume</li></ul><p>Kubernetes以in-tree plugin的形式来对接不同的存储系统，满足用户可以根据自己业务的需要使用这些插件给容器提供存储服务。同时兼容用户使用FlexVolume和CSI定制化插件。 <img src="https://raw.githubusercontent.com/louielong/blogPic/master/img1242388064.png" alt="img"></p><p>一般来说，Kubernetes中Pod通过如下三种方式来访问存储资源： </p><ul><li>直接访问 </li><li>静态provision </li><li>动态provision（使用StorageClass动态创建PV）</li></ul><h3 id="3-6-服务发现"><a href="#3-6-服务发现" class="headerlink" title="3.6 服务发现"></a>3.6 服务发现</h3><p>容器和微服务的结合创造了另外的热潮，也让服务发现成功了热门名词。可以轻松扩展微服务的同时，也要有工具来实现服务之间相互发现的需求。 DNS 服务器监视着创建新 Service 的 Kubernetes API，从而为每一个 Service 创建一组 DNS 记录。 如果整个集群的  DNS 一直被启用，那么所有的 Pod应该能够自动对 Service 进行名称解析。在技术实现上是通过kubernetes  api监视Service资源的变化，并根据Service的信息生成DNS记录写入到etcd中。dns为集群中的Pod提供DNS查询服务，而DNS记录则从etcd中读取。</p><ul><li>kube-dns：kube-dns是Kubernetes中的一个内置插件，目前作为一个独立的开源项目维护。Kubernetes DNS  pod 中包括 3 个容器：kube-dns，sidecar，dnsmasq .</li><li>CoreDNS：CoreDNS是一套灵活且可扩展的权威DNS服务器，作为CNCF中的托管的一个项目，自k8s 1.11  版本起被正式作为集群DNS附加选项，且在用户使用kubeadm时默认生效。提供更好的可靠性、灵活性和安全性，可以选择使用CoreDNS替换Kubernetes插件kube-dns。</li></ul><h3 id="3-7-状态数据存储"><a href="#3-7-状态数据存储" class="headerlink" title="3.7 状态数据存储"></a>3.7 状态数据存储</h3><p>目前主要有三种工具，大部分的容器管理系统也都是同时可以支持这三种工具。</p><ul><li>etcd：CoreOS 开源的分布式 key-value  存储，通过 HTTP/HTTPS 协议提供服务。etcd 只是一个 key-value  存储，默认不支持服务发现，需要三方工具来集成。kubernetes 默认使用 etcd 作为存储。 </li><li>ZooKeeper：Hadoop  的一个子项目，本来是作为 Hadoop 集群管理的数据存储，目前也被应用到容器领域，开发语言是 Java。 </li><li>Consul：HashiCorp 开发的分布式服务发现和配置管理工具。</li></ul><p>这些工具的主要作用就是保证这个集群的动态信息能统一保存，并保证一致性，这样每个节点和容器就能正确地获取到集群当前的信息。</p><h3 id="3-8-健康检查"><a href="#3-8-健康检查" class="headerlink" title="3.8 健康检查"></a>3.8 健康检查</h3><p>Kubernetes提供两种类型的健康检查，支持进行三种类型的探测：HTTP、Command和TCP。 </p><ul><li>Readiness探针旨在让Kubernetes知道您的应用何时准备好其流量服务。  Kubernetes确保Readiness探针检测通过，然后允许服务将流量发送到Pod。  如果Readiness探针开始失败，Kubernetes将停止向该容器发送流量，直到它通过。 </li><li>Liveness探针让Kubernetes知道你的应用程序是活着还是死了。 如果你的应用程序还活着，那么Kubernetes就不管它了。  如果你的应用程序已经死了，Kubernetes将删除Pod并启动一个新的替换它。</li></ul><h3 id="3-9-容器监控"><a href="#3-9-容器监控" class="headerlink" title="3.9 容器监控"></a>3.9 容器监控</h3><p>我们习惯于在两个层次监控：应用以及运行它们的主机。现在由于容器处在中间层，以及 Kubernetes 本身也需要监控，因此有 4 个不同的组件需要监控并且搜集度量信息。 </p><p>1）cAdvisor + InfluxDB +  Grafana：一个简单的跨多主机的监控系统Cadvisor：将数据，写入InfluxDBInfluxDB  ：时序数据库，提供数据的存储，存储在指定的目录下Grafana ：提供了WEB控制台，自定义查询指标，从InfluxDB查询数据，并展示。 </p><p>2）Heapster + InfluxDB +  Grafana：Heapster是一个收集者，将每个Node上的cAdvisor的数据进行汇总，然后导到InfluxDB，支持从Cluster、Node、Pod的各个层面提供详细的资源使用情况。Heapster：在Kubernetes集群中获取Metrics和事件数据，写入InfluxDB，Heapster收集的数据比cAdvisor多，而且存储在InfluxDB的也少。InfluxDB：时序数据库，提供数据的存储，存储在指定的目录下。Grafana：提供了WEB控制台，自定义查询指标，从InfluxDB查询数据，并展示。 </p><p>3）Prometheus+Grafana：Prometheus是个集DB、Graph、Statistics、Alert  于一体的监控工具。提供多维数据模型（时序列数据由metric名和一组key/value组成）和在多维度上灵活的查询语言(PromQl)，提供了很高的写入和查询性能。对内存占用量大，不依赖分布式存储，单主节点工作，所以不具有高可用性，支持pull/push两种时序数据采集方式。</p><p>考虑到Prometheus在扩展性和高可用方面的缺点，在超大规模应用时可以考察下thanos这样的面向解决Prometheus的长时间数据存储与服务高可用解决方案的开源项目：<a href="https://github.com/improbable-eng/thanos" target="_blank" rel="noopener">https://github.com/improbable-eng/thanos</a></p><p>容器集群的四个监控层次</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img358659132.png" alt="4个监控层次"></p><h3 id="3-11-镜像-registry"><a href="#3-11-镜像-registry" class="headerlink" title="3.11 镜像 registry"></a>3.11 镜像 registry</h3><p>镜像 registry 是存储镜像的地方，可以方便地在团队、公司或者世界各地分享容器镜像，也是运行容器最基本的基础设施。 </p><ul><li>Docker Registry：Docker 公司提供的开源镜像服务器，也是目前最流行的自建 registry 的方案</li><li>Harbor：企业级的镜像  registry，提供了权限控制和图形界面</li></ul><p>每种对应技术几乎都有自己的基础镜像，例如：</p><ul><li><a href="https://hub.docker.com/_/java/" target="_blank" rel="noopener">java</a></li><li><a href="https://hub.docker.com/_/python/" target="_blank" rel="noopener">python</a></li><li><a href="https://hub.docker.com/_/nginx/" target="_blank" rel="noopener">nginx</a> </li><li><a href="https://hub.docker.com/_/alpine/" target="_blank" rel="noopener">alpine</a> 一个常用的基础镜像Alpine Linux（体积小于5MB）</li></ul><h2 id="【参考资料】"><a href="#【参考资料】" class="headerlink" title="【参考资料】"></a>【参考资料】</h2><p>1）<a href="http://www.cesi.cn/201612/750.html" target="_blank" rel="noopener">中国开源云联盟容器工作组-容器技术及其应用白皮书v1.0</a><br>2）<a href="https://mp.weixin.qq.com/s/hrgXzt7YKVf6ZCFzJ-WTFA" target="_blank" rel="noopener">从风口浪尖到十字路口，写在 Kubernetes 两周年之际</a><br>3）<a href="http://dockone.io/article/2616Kubernetes" target="_blank" rel="noopener">白话Kubernetes网络</a><br>4）<a href="http://dockone.io/article/2602CNCF" target="_blank" rel="noopener">主机和容器的监控方案</a><br>5）<a href="http://dockone.io/article/3006Kubernetes" target="_blank" rel="noopener">云原生容器生态系统概要</a><br>6）<a href="http://dockone.io/article/3063Kubernetes" target="_blank" rel="noopener">存储系统介绍及机制实现</a><br>7）<a href="http://dockone.io/article/5108Docker" target="_blank" rel="noopener">内部组件工作原理介绍</a><br>8）<a href="https://www.infoq.cn/article/2017%2F02%2FDocker-Containerd-RunC" target="_blank" rel="noopener">Containerd、RunC…：你应该知道的所有</a><br>9）<a href="https://www.cnblogs.com/sparkdev/p/9129334.html" target="_blank" rel="noopener">从 docker 到 runC</a></p>]]></content>
    
    <summary type="html">
    
      容器、容器云和k8s相关技术介绍
    
    </summary>
    
      <category term="k8s" scheme="https://louielong.github.io/categories/k8s/"/>
    
    
      <category term="container" scheme="https://louielong.github.io/source/tags/container/"/>
    
  </entry>
  
  <entry>
    <title>【转载】Docker容器安全性分析</title>
    <link href="https://louielong.github.io/container-security.html"/>
    <id>https://louielong.github.io/container-security.html</id>
    <published>2020-07-07T08:12:03.000Z</published>
    <updated>2020-07-08T03:47:02.278Z</updated>
    
    <content type="html"><![CDATA[<p><strong>本文作者：狴犴安全团队，转载自FreeBuf.COM</strong></p><p><strong>Docker是目前最具代表性的容器技术之一，对云计算及虚拟化技术产生了颠覆性的影响。本文对Docker容器在应用中可能面临的安全问题和风险进行了研究，并将Docker容器应用环境中的安全机制与相关解决方案分为容器虚拟化安全、容器安全管理、容器网络安全三部分进行分析。</strong></p><h2 id="一、从虚拟化安全到容器安全"><a href="#一、从虚拟化安全到容器安全" class="headerlink" title="一、从虚拟化安全到容器安全"></a>一、从虚拟化安全到容器安全</h2><h3 id="1-1-传统虚拟化技术"><a href="#1-1-传统虚拟化技术" class="headerlink" title="1.1 传统虚拟化技术"></a>1.1 传统虚拟化技术</h3><p>虚拟化技术是实现硬件基础设施资源的充分利用、合理分配和有效调度的重要技术手段。例如，在基于OpenStack的典型IaaS服务中，云服务提供商可通过搭建设备集群建立资源池，并将服务器、存储和网络等底层资源进行弹性虚拟化提供给租户。</p><p>传统虚拟化技术以虚拟机为管理单元，各虚拟机拥有独立的操作系统内核，不共用宿主机的软件系统资源，因此具有良好的隔离性，适用于云计算环境中的多租户场景。</p><h3 id="1-2-容器技术"><a href="#1-2-容器技术" class="headerlink" title="1.2 容器技术"></a>1.2 容器技术</h3><p>容器技术可以看作一种轻量级的虚拟化方式，将应用与必要的执行环境打包成容器镜像，使得应用程序可以直接在宿主机（物理机或虚拟机）中相对独立地运行。容器技术在操作系统层进行虚拟化，可在宿主机内核上运行多个虚拟化环境。相比于传统的应用测试与部署，容器的部署无需预先考虑应用的运行环境兼容性问题；相比于传统虚拟机，容器无需独立的操作系统内核就可在宿主机中运行，实现了更高的运行效率与资源利用率。</p><p>Docker是目前最具代表性的容器平台之一，它模糊了传统的IaaS和PaaS的边界，具有持续部署与测试、跨云平台支持等优点。在基于Kubernetes等容器编排工具实现的容器云环境中，通过对跨主机集群资源的调度，容器云可提供资源共享与隔离、容器编排与部署、应用支撑等功能。</p><p>Docker容器技术以宿主机中的容器为管理单元，但各容器共用宿主机内核资源，分别通过Linux系统的<code>Namespaces</code>和<code>CGroups</code>机制实现资源的隔离与限制。</p><p> <strong>1）Namespaces</strong></p><p>为了保证容器进程之间的资源隔离，避免相互影响和干扰，Linux内核的Namespaces（命名空间）机制提供了<code>UTS、User、Mount、Network、PID、IPC</code>等命名空间实现了主机名、用户权限、文件系统、网络、进程号、进程间通信等六项资源隔离功能。通过调用clone()函数并传入相应的系统调用参数创建容器进程，可实现对应资源内容的隔离，具体情况如表1所示。</p><p>表1：Namespaces隔离机制</p><table><thead><tr><th>命名空间</th><th>系统调用参数</th><th>隔离内容</th><th>Linux内核版本</th></tr></thead><tbody><tr><td>UTS</td><td>CLONE_NEWUTS</td><td>主机名和域名</td><td>2.6.19</td></tr><tr><td>IPC</td><td>CLONE_NEWIPC</td><td>信号量、信息队列和共享内存</td><td>2.6.19</td></tr><tr><td>PID</td><td>CLONE_NEWPID</td><td>进程编号</td><td>2.6.24</td></tr><tr><td>Network</td><td>CLONE_NEWNET</td><td>网络设备、网络栈、端口等</td><td>2.6.29</td></tr><tr><td>Mount</td><td>CLONE_NEWNS</td><td>挂载点（文件系统）</td><td>2.4.19</td></tr><tr><td>User</td><td>CLONE_NEWUSER</td><td>用户和用户组</td><td>3.8</td></tr></tbody></table><p>对于某个进程而言，可通过查看/proc/[PID]/ns文件，获取该进程下的命名空间隔离情况，如图1所示。其中，每一项命名空间都拥有一个编号对其进行唯一标识，如果宿主机中两个进程指向的命名空间编号相同，则表示他们同在一个命名空间之下。</p><p><img src="https://image.3001.net/images/20191127/1574822385_5ddde1f1ef754.png" alt="cgroup"></p><p>图1：进程命名空间</p><p><strong>2）CGroups</strong></p><p>CGroups（Control  Groups，控制组）机制最早于2006年由Google提出，目前是Linux内核的一种机制，可以实现对任务组（进程组或线程组）使用的物理资源（CPU、内存、I/O等）进行限制和记录，通过多种度量标准为各个容器相对公平地分配资源，以防止资源滥用的情况。</p><p>在实际应用中，CGroups会为每个执行任务创建一个钩子，在任务执行的过程中涉及到资源分配使用时，就会触发钩子上的函数并对相应的资源进行检测，从而对资源进行限制和优先级分配。</p><p>CGroups提供了资源限制（Resource  Limitation）、优先级分配（Prioritization）、资源统计（Accounting）、任务控制（Control）四个功能，包含<code>blkio、cpu、cpuacct、cpuset、devices、freezer、memory、perf_event、net_cls、net_prio、ns、hugetlb</code>等子系统，每种子系统独立地控制一种资源，可分别实现块设备输入/输出限制、CPU使用控制、生成CPU资源使用情况报告、内存使用量限制等功能。几个主要子系统的具体功能如表2所示。</p><p>表2：CGroups子系统</p><table><thead><tr><th>子系统</th><th>功能</th></tr></thead><tbody><tr><td>blkio</td><td>为块设备（如磁盘、固态硬盘等物理驱动设备）设定输入/输出限制</td></tr><tr><td>cpu</td><td>通过调度程序控制任务对CPU的使用</td></tr><tr><td>cpuacct</td><td>生成任务对CPU资源使用情况的报告</td></tr><tr><td>cpuset</td><td>为任务分配独立的CPU和内存</td></tr><tr><td>devices</td><td>开启或关闭任务对设备的访问</td></tr><tr><td>freezer</td><td>挂起或恢复任务</td></tr><tr><td>memory</td><td>设定任务对内存的使用量限制，生成任务对内存资源使用情况的报告</td></tr></tbody></table><h3 id="1-3-安全性"><a href="#1-3-安全性" class="headerlink" title="1.3 安全性"></a>1.3 安全性</h3><p>传统虚拟化技术与Docker容器技术在运行时的安全性差异主要体现在隔离性方面，包括进程隔离、文件系统隔离、设备隔离、进程间通信隔离、网络隔离、资源限制等。</p><p>在Docker容器环境中，由于各容器共享操作系统内核，而容器仅为运行在宿主机上的若干进程，其安全性特别是隔离性与传统虚拟机相比在理论上与实际上都存在一定的差距。</p><h2 id="二、Docker容器安全风险分析"><a href="#二、Docker容器安全风险分析" class="headerlink" title="二、Docker容器安全风险分析"></a>二、Docker容器安全风险分析</h2><p>根据Docker容器的主要特点及其在安全应用中的实际问题，本文将Docker容器技术应用中可能存在的技术性安全风险分为镜像安全风险、容器虚拟化安全风险、网络安全风险等类型进行具体分析，如图2所示。</p><p><img src="https://image.3001.net/images/20191127/1574822397_5ddde1fd95671.png" alt="容器安全风险"></p><p>图2：容器安全风险分类</p><h3 id="2-1-镜像安全风险"><a href="#2-1-镜像安全风险" class="headerlink" title="2.1 镜像安全风险"></a>2.1 镜像安全风险</h3><p>Docker镜像是Docker容器的静态表示形式，镜像的安全决定了容器的运行时安全。</p><p>Docker容器官方镜像仓库Docker  Hub中的镜像可能由个人开发者上传，其数量丰富、版本多样，但质量参差不齐，甚至存在包含恶意漏洞的恶意镜像，因而可能存在较大的安全风险。具体而言，Docker镜像的安全风险分布在创建过程、获取来源、获取途径等方方面面。</p><p><strong>1）Dockerfile安全问题</strong></p><p>Docker镜像的生成主要包括两种方式，一种是对运行中的动态容器通过docker  commit命令进行打包，另一种是通过docker  build命令执行Dockerfile文件进行创建。为了确保最小安装原则，同时考虑容器的易维护性，一般推荐采用Dockerfile文件构建容器镜像，即在基础镜像上进行逐层应用添加操作。</p><p>Dockerfile是包含用于组合镜像命令的文本文件，一般由基础镜像信息（FROM）、维护者信息（MAINTAINER）、镜像操作指令（RUN、ADD、COPY等）、容器启动时执行指令（CMD等）四个部分组成，Docker可通过读取Dockerfile中的命令创建容器镜像。</p><p>Dockerfile文件内容在一定程度上决定了Docker镜像的安全性，其安全风险具体包括但不限于以下情况：</p><blockquote><p>如果Dockerfile存在漏洞或被插入恶意脚本，那么生成的容器也可能产生漏洞或被恶意利用。例如，攻击者可构造特殊的Dockerfile压缩文件，在编译时触发漏洞获取执行任意代码的权限。</p><p>如果在Dockerfile中没有指定USER，Docker将默认以root用户的身份运行该Dockerfile创建的容器，如果该容器遭到攻击，那么宿主机的root访问权限也可能会被获取。</p><p>如果在Dockerfile文件中存储了固定密码等敏感信息并对外进行发布，则可能导致数据泄露的风险。</p><p>如果在Dockerfile的编写中添加了不必要的应用，如SSH、Telnet等，则会产生攻击面扩大的风险。</p></blockquote><p><strong>2）镜像漏洞</strong></p><p>对于大多数一般的开发者而言，通常需要获取一系列基础镜像进行容器云的部署和进一步开发，因此，基础镜像的安全性在一定程度上决定了容器云环境的安全性。</p><p>镜像漏洞安全风险具体包括镜像中的软件含有CVE漏洞、攻击者上传含有恶意漏洞的镜像等情况。</p><p><strong>① CVE漏洞</strong></p><p>由于镜像通常由基础操作系统与各类应用软件构成，因此，含有CVE漏洞的应用软件同样也会向Docker镜像中引入CVE漏洞。</p><p>镜像的获取通常是通过官方镜像仓库Docker Hub或网易、阿里云等提供的第三方镜像仓库。然而，根据对Docker  Hub中镜像安全漏洞的相关研究，无论是社区镜像还是官方镜像，其平均漏洞数均接近200个，包括nginx、mysql、redis在内的常用镜像都含有高危漏洞。</p><p><strong>② 恶意漏洞</strong></p><p>恶意用户可能将含有后门、病毒等恶意漏洞的镜像上传至官方镜像库。2018年6月，安全厂商Fortinet和Kromtech在Docker  Hub上发现17个包含用于数字货币挖矿恶意程序的Docker镜像，而这些恶意镜像当时已有500万次的下载量。目前，由于Docker应用在世界范围内具有广泛性，全网针对Docker容器的攻击很多都被用于进行数字货币挖矿，为攻击者带来实际经济利益，损害Docker用户的正常使用。</p><p><strong>3）镜像仓库安全</strong></p><p>作为搭建私有镜像存储仓库的工具，Docker Registry的应用安全性也必须得到保证。镜像仓库的安全风险主要包括仓库本身的安全风险和镜像拉取过程中的传输安全风险。</p><blockquote><p>仓库自身安全：如果镜像仓库特别是私有镜像仓库被恶意攻击者所控制，那么其中所有镜像的安全性将无法得到保证。例如，如果私有镜像仓库由于配置不当而开启了2357端口，将会导致私有仓库暴露在公网中，攻击者可直接访问私有仓库并篡改镜像内容，造成仓库内镜像的安全隐患。</p><p>镜像拉取安全：如何保证容器镜像从镜像仓库到用户端的完整性也是镜像仓库面临的一个重要安全问题。由于用户以明文形式拉取镜像，如果用户在与镜像仓库交互的过程中遭遇了中间人攻击，导致拉取的镜像在传输过程中被篡改或被冒名发布恶意镜像，会造成镜像仓库和用户双方的安全风险。Docker已在其1.8版本后采用内容校验机制解决中间人攻击的问题。</p></blockquote><h3 id="2-2-容器虚拟化安全风险"><a href="#2-2-容器虚拟化安全风险" class="headerlink" title="2.2 容器虚拟化安全风险"></a>2.2 容器虚拟化安全风险</h3><p>与传统虚拟机相比，Docker容器不拥有独立的资源配置，且没有做到操作系统内核层面的隔离，因此可能存在资源隔离不彻底与资源限制不到位所导致的安全风险。</p><p><strong>1）容器隔离问题</strong></p><p>对于Docker容器而言，由于容器与宿主机共享操作系统内核，因此存在容器与宿主机之间、容器与容器之间隔离方面的安全风险，具体包括进程隔离、文件系统隔离、进程间通信隔离等。</p><p>虽然Docker通过Namespaces进行了文件系统资源的基本隔离，但仍有/sys、/proc/sys、/proc/bus、/dev、time、syslog等重要系统文件目录和命名空间信息未实现隔离，而是与宿主机共享相关资源。</p><p>针对容器隔离安全风险问题，主要存在以下两种隔离失效的情况：</p><blockquote><p>攻击者可能通过对宿主机内核进行攻击达到攻击其中某个容器的目的。</p><p>由于容器所在主机文件系统存在联合挂载的情况，恶意用户控制的容器也可能通过共同挂载的文件系统访问其他容器或宿主机，造成数据安全问题。</p></blockquote><p><strong>2）容器逃逸攻击</strong></p><p>容器逃逸攻击指的是容器利用系统漏洞，“逃逸”出了其自身所拥有的权限，实现了对宿主机和宿主机上其他容器的访问。由于容器与宿主机共享操作系统内核，为了避免容器获取宿主机的root权限，通常不允许采用特权模式运行Docker容器。</p><p>在容器逃逸案例中，最为著名的是shocker.c程序，其通过调用open_by_handle_at函数对宿主机文件系统进行暴力扫描，以获取宿主机的目标文件内容。由于Docker1.0之前版本对容器能力（Capability）使用黑名单策略进行管理，并没有限制CAP_DAC_READ_SEARCH能力，赋予了shocker.c程序调用open_by_handle_at函数的能力，导致容器逃逸的发生。因此，对容器能力的限制不当是可能造成容器逃逸等安全问题的风险成因之一。所幸的是，Docker在后续版本中对容器能力采用白名单管理，避免了默认创建的容器通过shocker.c案例实现容器逃逸的情况。</p><p>此外，在Black Hat USA  2019会议中，来自Capsule8的研究员也给出了若干Docker容器引擎漏洞与容器逃逸攻击方法，包括CVE-2019-5736、CVE-2018-18955、CVE-2016-5195等可能造成容器逃逸的漏洞。</p><blockquote><p>CVE-2019-5736是runC的一个安全漏洞，导致18.09.2版本前的Docker允许恶意容器覆盖宿主机上的runC二进制文件。runC是用于创建和运行Docker容器的CLI工具，该漏洞使攻击者能够以root身份在宿主机上执行任意命令。</p><p>CVE-2018-18955漏洞涉及到User命名空间中的嵌套用户命名空间，用户命名空间中针对uid（用户ID）和gid（用户组ID）的ID映射机制保证了进程拥有的权限不会逾越其父命名空间的范畴。该漏洞利用创建用户命名空间的子命名空间时损坏的ID映射实现提权。</p><p>CVE-2016-5195脏牛（Dirty  CoW）Linux内核提权漏洞可以使低权限用户在多版本Linux系统上实现本地提权，进而可能导致容器逃逸的发生。Linux内核函数get_user_page在处理Copy-on-Write时可能产生竞态条件，导致出现向进程地址空间内只读内存区域写数据的机会，攻击者可进一步修改su或者passwd程序以获取root权限。</p></blockquote><p><strong>3）拒绝服务攻击</strong></p><p>由于容器与宿主机共享CPU、内存、磁盘空间等硬件资源，且Docker本身对容器使用的资源并没有默认限制，如果单个容器耗尽宿主机的计算资源或存储资源（例如进程数量、存储空间等）可能导致宿主机或其他容器的拒绝服务。</p><p><strong>① 计算型DoS攻击</strong></p><p>Fork  Bomb是一类典型的针对计算资源的拒绝服务攻击手段，其可通过递归方式无限循环调用fork()系统函数快速创建大量进程。由于宿主机操作系统内核支持的进程总数有限，如果某个容器遭到了Fork Bomb攻击，那么就有可能存在由于短时间内在该容器内创建过多进程而耗尽宿主机进程资源的情况，宿主机及其他容器就无法再创建新的进程。</p><p><strong>② 存储型DoS攻击</strong></p><p>针对存储资源，虽然Docker通过Mount命名空间实现了文件系统的隔离，但CGroups并没有针对AUFS文件系统进行单个容器的存储资源限制，因此采用AUFS作为存储驱动具有一定的安全风险。如果宿主机上的某个容器向AUFS文件系统中不断地进行写文件操作，则可能会导致宿主机存储设备空间不足，无法再满足其自身及其他容器的数据存储需求。</p><h3 id="2-3-网络安全风险"><a href="#2-3-网络安全风险" class="headerlink" title="2.3 网络安全风险"></a>2.3 网络安全风险</h3><p>网络安全风险是互联网中所有信息系统所面临的重要风险，不论是物理设备还是虚拟机，都存在难以完全规避的网络安全风险问题。而在轻量级虚拟化的容器网络环境中，其网络安全风险较传统网络而言更为复杂严峻。</p><p><strong>1）容器网络攻击</strong></p><p>Docker提供桥接网络、MacVLAN、覆盖网络（Overlay）等多种组网模式，可分别实现同一宿主机内容器互联、跨宿主机容器互联、容器集群网络等功能。</p><p><strong>① 网桥模式</strong></p><p>Docker默认采用网桥模式，利用iptables进行NAT转换和端口映射。Docker将所有容器都通过虚拟网络接口对连接在一个名为docker0的虚拟网桥上，作为容器的默认网关，而该网桥与宿主机直接相连。</p><p>容器内部的数据包经过虚拟网络接口对到达docker0，实现同一子网内不同容器间的通信。在网桥模式下，同一宿主机内各容器间可以互相通信，而宿主机外部无法通过分配给容器的IP地址对容器进行外部访问。</p><p>由于缺乏容器间的网络安全管理机制，无法对同一宿主机内各容器之间的网络访问权限进行限制。具体而言，由于各容器之间通过宿主机内部网络的docker0网桥连接以实现路由和NAT转换，如果容器间没有防火墙等保护机制，则攻击者可通过某个容器对宿主机内的其他容器进行ARP欺骗、嗅探、广播风暴等攻击，导致信息泄露、影响网络正常运行等安全后果。</p><p>因此，如果在同一台宿主机上部署的多个容器没有进行合理的网络配置进行访问控制边界隔离，将可能产生容器间的网络安全风险。</p><p><strong>② MacVLAN</strong></p><p>MacVLAN是一种轻量级网络虚拟化技术，通过与主机的网络接口连接实现了与实体网络的隔离性。</p><p>MacVLAN允许为同一个物理网卡配置多个拥有独立MAC地址的网络接口并可分别配置IP地址，实现了网卡的虚拟化。MacVLAN模式无需创建网桥，即无需NAT转换和端口映射就可以直接通过网络接口连接到物理网络，不同MacVLAN网络间不能在二层网络上进行通信。</p><p>然而，处于同一虚拟网络下各容器间同样没有进行访问权限控制，因此MacVLAN模式依然存在与网桥模式类似的内部网络攻击的安全风险。</p><p><strong>③ Overlay网络</strong></p><p>Overlay网络架构主要用于构建分布式容器集群，通过VxLAN技术在不同主机之间的Underlay网络上建立虚拟网络，以搭建跨主机容器集群，实现不同物理主机中同一Overlay网络下的容器间通信。</p><p>与其他组网模式一样，Overlay网络也没有对同一网络内各容器间的连接进行访问控制。此外，由于VxLAN网络流量没有加密，需要在设定IPSec隧道参数时选择加密以保证容器网络传输内容安全。</p><p>因此，无论采用何种网络连接模式，都难以避免容器间互相攻击的安全风险。</p><p><strong>2）网络<em>DoS</em>攻击</strong></p><p>由于网络虚拟化的存在，容器网络面临着与传统网络不同的DoS攻击安全风险。Docker容器网络的DoS攻击分为内部威胁和外部威胁两种主要形式。</p><blockquote><p>内部威胁：针对Docker容器网络环境，DoS攻击可不通过物理网卡而在宿主机内部的容器之间进行，攻击者通过某个容器向其他容器发起DoS攻击可能降低其他容器的网络数据处理能力。因此，存在容器虚拟网络间的DoS攻击风险。</p><p>外部威胁：由于同一台宿主机上的所有容器共享宿主机的物理网卡资源，若外部攻击者使用包含大量受控主机的僵尸网络向某一个目标容器发送大量数据包进行DDoS攻击，将可能占满宿主机的网络带宽资源，造成宿主机和其他容器的拒绝服务。</p></blockquote><h2 id="三、Docker容器安全机制与解决方案"><a href="#三、Docker容器安全机制与解决方案" class="headerlink" title="三、Docker容器安全机制与解决方案"></a>三、Docker容器安全机制与解决方案</h2><h3 id="3-1-容器虚拟化安全"><a href="#3-1-容器虚拟化安全" class="headerlink" title="3.1 容器虚拟化安全"></a>3.1 容器虚拟化安全</h3><p>在传统虚拟化技术架构中，Hypervisor虚拟机监视器是虚拟机资源的管理与调度模块。而在容器架构中，由于不含有Hypervisor层，因此需要依靠操作系统内核层面的相关机制对容器进行安全的资源管理。</p><p><strong>1）容器资源隔离与限制</strong></p><p>在资源隔离方面，与采用虚拟化技术实现操作系统内核级隔离不同，Docker通过Linux内核的Namespace机制实现容器与宿主机之间、容器与容器之间资源的相对独立。通过为各运行容器创建自己的命名空间，保证了容器中进程的运行不会影响到其他容器或宿主机中的进程。</p><p>在资源限制方面，Docker通过CGroups实现宿主机中不同容器的资源限制与审计，包括对CPU、内存、I/O等物理资源进行均衡化配置，防止单个容器耗尽所有资源造成其他容器或宿主机的拒绝服务，保证所有容器的正常运行。</p><p>但是，CGroups未实现对磁盘存储资源的限制。若宿主机中的某个容器耗尽了宿主机的所有存储空间，那么宿主机中的其他容器无法再进行数据写入。Docker提供的–storage-opt=[]磁盘限额仅支持Device  Mapper文件系统，而Linux系统本身采用的磁盘限额机制是基于用户和文件系统的quota技术，难以针对Docker容器实现基于进程或目录的磁盘限额。因此，可考虑采用以下方法实现容器的磁盘存储限制：</p><blockquote><p>为每个容器创建单独用户，限制每个用户的磁盘使用量；</p><p>选择XFS等支持针对目录进行磁盘使用量限制的文件系统；</p><p>为每个容器创建单独的虚拟文件系统，具体步骤为创建固定大小的磁盘文件，并从该磁盘文件创建虚拟文件系统，然后将该虚拟文件系统挂载到指定的容器目录。</p></blockquote><p>此外，在默认情况下，容器可以使用主机上的所有内存。可以使用内存限制机制来防止一个容器消耗所有主机资源的拒绝服务攻击，具体可使用使用-m或-memory参数运行容器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：docker run [运行参数] -memory [内存大小] [容器镜像名或ID] [命令]）</span><br></pre></td></tr></table></figure><p><strong>2）容器能力限制</strong></p><p>Linux内核能力表示进程所拥有的系统调用权限，决定了程序的系统调用能力。</p><p>容器的默认能力包括<code>CHOWN、DAC_OVERRIDE、FSETID、SETGID、SETUID、SETFCAP、NET_RAW、MKNOD、SYS_REBOOT、SYS_CHROOT、KILL、NET_BIND_SERVICE、AUDIT_WRITE</code>等等，具体功能如表3所示。</p><p>表3：容器默认能力</p><table><thead><tr><th>容器默认能力</th><th>作用</th></tr></thead><tbody><tr><td>CHOWN</td><td>允许任意更改文件UID以及GID</td></tr><tr><td>DAC_OVERRIDE</td><td>允许忽略文件的读、写、执行访问权限检查</td></tr><tr><td>FSETID</td><td>允许文件修改后保留setuid/setgid标志位</td></tr><tr><td>SETGID</td><td>允许改变进程组ID</td></tr><tr><td>SETUID</td><td>允许改变进程用户ID</td></tr><tr><td>SETFCAP</td><td>允许向其他进程转移或删除能力</td></tr><tr><td>NET_RAW</td><td>允许创建RAW和PACKET套接字</td></tr><tr><td>MKNOD</td><td>允许使用mknod创建指定文件</td></tr><tr><td>SYS_REBOOT</td><td>允许使用reboot或者kexec_load</td></tr><tr><td>SYS_CHROOT</td><td>允许使用chroot</td></tr><tr><td>KILL</td><td>允许发送信号</td></tr><tr><td>NET_BIND_SERVICE</td><td>允许绑定常用端口号（端口号小于1024）</td></tr><tr><td>AUDIT_WRITE</td><td>允许审计日志写入</td></tr></tbody></table><p>如果对容器能力不加以适当限制，可能会存在以下安全隐患：</p><blockquote><p>内部因素：在运行Docker容器时，如果采用默认的内核功能配置可能会产生容器的隔离问题。</p><p>外部因素：不必要的内核功能可能导致攻击者通过容器实现对宿主机内核的攻击。</p></blockquote><p>因此，不当的容器能力配置可能会扩大攻击面，增加容器与宿主机面临的安全风险，在执行docker run命令运行Docker容器时可根据实际需求通过–cap-add或–cap-drop配置接口对容器的能力进行增删。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：docker run --cap-drop ALL --cap-add SYS_TIME ntpd /bin/sh）</span><br></pre></td></tr></table></figure><p><strong>3）强制访问控制</strong></p><p>强制访问控制（Mandatory Access Control,  MAC）是指每一个主体（包括用户和程序）和客体都拥有固定的安全标记，主体能否对客体进行相关操作，取决于主体和客体所拥有安全标记的关系。在Docker容器应用环境下，可通过强制访问控制机制限制容器的访问资源。Linux内核的强制访问控制机制包括SELinux、AppArmor等。</p><p><strong>① SELinux机制</strong></p><p>SELinux（Security-Enhanced  Linux）是Linux内核的强制访问控制实现，由美国国家安全局（NSA）发起，用以限制进程的资源访问，即进程仅能访问其任务所需的文件资源。因此，可通过SELinux对Docker容器的资源访问进行控制。</p><p>在启动Docker  daemon守护进程时，可通过将–selinux-enabled参数设为true，从而在Docker容器中使用SELinux。SELinux可以使经典的shocker.c程序失效，使其无法逃逸出Docker容器实现对宿主机资源的访问。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：docker daemon --selinux-enabled = true）</span><br></pre></td></tr></table></figure><p><strong>② AppArmor机制</strong></p><p>与SELinux类似，AppArmor（Application Armor，应用程序防护）也是Linux的一种强制访问控制机制，其作用是对可执行程序进行目录和文件读写、网络端口访问和读写等权限的控制。</p><p>在Docker  daemon启动后会在/etc/apparmor.d/docker自动创建AppArmor的默认配置文件docker-default，可通过在该默认配置文件中新增访问控制规则的方式对容器进行权限控制，同时可在启动容器时通过–security-opt指定其他配置文件。例如，在配置文件中加入一行deny /etc/hosts rwklx限制对/etc/hosts的获取，同样可使shocker.c容器逃逸攻击失效。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：docker run --rm -ti --cap-add=all --security-opt apparmor:docker-default shocker bash）</span><br></pre></td></tr></table></figure><p><strong>4）Seccomp机制</strong></p><p>Seccomp（Secure Computing Mode）是Linux内核提供的安全特性，可实现应用程序的沙盒机制构建，以白名单或黑名单的方式限制进程能够进行的系统调用范围。</p><p>在Docker中，可通过为每个容器编写json格式的seccomp profile实现对容器中进程系统调用的限制。在seccomp profile中，可定义以下行为对进程的系统调用做出响应：</p><blockquote><p>SCMP_ACT_KILL：当进程进行对应的系统调用时，内核发出SIGSYS信号终止该进程，该进程不会接受到这个信号；</p><p>SCMP_ACT_TRAP：当进程进行对应的系统调用时，该进程会接收到SIGSYS信号，并改变自身行为；</p><p>SCMP_ACT_ERRNO：当进程进行对应的系统调用时，系统调用失败，进程会接收到errno返回值；</p><p>SCMP_ACT_TRACE：当进程进行对应的系统调用时，进程会被跟踪；</p><p>SCMP_ACT_ALLOW：允许进程进行对应的系统调用行为。</p></blockquote><p>默认情况下，在Docker容器的启动过程中会使用默认的seccomp profile，可使用security-opt seccomp选项使用特定的seccomp profile。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：docker run --rm -it --security-opt seccomp:/path/to/seccomp/profile.json hello-world）</span><br></pre></td></tr></table></figure><h3 id="3-2-容器安全管理"><a href="#3-2-容器安全管理" class="headerlink" title="3.2 容器安全管理"></a>3.2 容器安全管理</h3><p><strong>1）镜像仓库安全</strong></p><p><strong>① 内容信任机制</strong></p><p>Docker的内容信任（Content Trust）机制可保护镜像在镜像仓库与用户之间传输过程中的完整性。目前，Docker的内容信任机制默认关闭，需要手动开启。内容信任机制启用后，镜像发布者可对镜像进行签名，而镜像使用者可以对镜像签名进行验证。</p><p>具体而言，镜像构建者在通过docker  build命令运行Dockerfile文件前，需要通过手动或脚本方式将DOCKER_CONTENT_TRUST环境变量置为1进行启用。在内容信任机制开启后，push、build、create、pull、run等命令均与内容信任机制绑定，只有通过内容信任验证的镜像才可成功运行这些操作。例如，Dockerfile中如果包含未签名的基础镜像，将无法成功通过docker  build进行镜像构建。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：export DOCKER_CONTENT_TRUST = 1）</span><br></pre></td></tr></table></figure><p><strong>② Notary项目</strong></p><p>Notary是一个从Docker中剥离的独立开源项目，提供数据收集的安全性。Notary用于发布内容的安全管理，可对发布的内容进行数字签名，并允许用户验证内容的完整性和来源。Notary的目标是保证服务器与客户端之间使用可信连接进行交互，用于解决互联网内容发布的安全性，并未局限于容器应用。</p><p>在Docker容器场景中，Notary可支持Docker内容信任机制。因此，可使用Notary构建镜像仓库服务器，实现对容器镜像的签名，对镜像源认证、镜像完整性等安全需求提供更好的支持。</p><p><strong>2）镜像安全扫描</strong></p><p>为了保证容器运行的安全性，在从公共镜像仓库获取镜像时需要对镜像进行安全检查，防止存在安全隐患甚至恶意漏洞的镜像运行，从源头端预防安全事故的发生。镜像漏洞扫描工具是一类常用的镜像安全检查辅助工具，可检测出容器镜像中含有的CVE漏洞。</p><p>针对Docker镜像的漏洞扫描，目前已经有许多相关工具与解决方案，包括Docker Security Scanning、Clair、Anchore、Trivy、Aqua等等。</p><p><strong>① Docker Security Scanning服务</strong></p><p>Docker Security Scanning是Docker官方推出的不开源镜像漏洞扫描服务，用于检测Docker Cloud服务中私有仓库和Docker Hub官方仓库中的镜像是否安全。</p><p>Docker Security Scanning包括扫描触发、扫描器、数据库、附加元件框架以及CVE漏洞数据库比对等服务。当仓库中有镜像发生更新时，会自动启动漏洞扫描；当CVE漏洞数据库发生更新时，也会实时更新镜像漏洞扫描结果。</p><p><strong>② Clair工具</strong></p><p>Clair是一款开源的Docker镜像漏洞扫描工具。与Docker Security Scanning类似，Clair通过对Docker镜像进行静态分析并与公共漏洞数据库关联，得到相应的漏洞分析结果。Clair主要包括以下模块：</p><blockquote><p>Fetcher（获取器）：从公共的CVE漏洞源收集漏洞数据；</p><p>Detector（检测器）：对镜像的每一个Layer进行扫描，提取镜像特征；</p><p>Notifier（通知器）：用于接收WebHook从公开CVE漏洞库中的最新漏洞信息并进行漏洞库更新；</p><p>Databases（数据库）：PostSQL数据库存储容器中的各个层和CVE漏洞；</p></blockquote><p><strong>③ Trivy工具</strong></p><p>Trivy是一个简单而全面的开源容器漏洞扫描程序。Trivy可检测操作系统软件包（Alpine、RHEL、CentOS等）和应用程序依赖项（Bundler、Composer、npm、yarn等）的漏洞。此外，Trivy具有较高的易用性，只需安装二进制文件并指定扫描容器的镜像名称即可执行扫描。Trivy提供了丰富的功能接口，相比于其他容器镜像漏洞扫描工具更适合自动化操作，可更好地满足持续集成的需求。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：trivy [镜像名]）</span><br></pre></td></tr></table></figure><p><strong>3）容器运行时监控</strong></p><p>为了在系统运维层面保证容器运行的安全性，实现安全风险的即时告警与应急响应，需要对Docker容器运行时的各项性能指标进行实时监控。</p><p>针对Docker容器监控的工具与解决方案包括docker stats、cAdvisor、Scout、DataDog、Sensu等等，其中最常见的是Docker原生的docker stats命令和Google的cAdvisor开源工具。</p><p><strong>① docker stats命令</strong></p><p>docker  stats是Docker自带的容器资源使用统计命令，可用于对宿主机上的Docker容器的资源使用情况进行手动监控，具体内容包括容器的基本信息、容器的CPU使用率、内存使用率、内存使用量与限制、块设备I/O使用量、网络I/O使用量、进程数等信息。用户可根据自身需求设置<code>--format</code>参数控制docker stats 命令输出的内容格式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：docker stats [容器名]）</span><br></pre></td></tr></table></figure><p><strong>② cAdvisor工具</strong></p><p>由于docker stats只是简单的容器资源查看命令，其可视化程度不高，同时不支持监控数据的存储。cAdvisor是由Google开源的容器监控工具，优化了docker stats在可视化展示与数据存储方面的缺陷。</p><p>cAdvisor在宿主机上以容器方式运行，通过挂载在本地卷，可对同一台宿主机上运行的所有容器进行实时监控和性能数据采集，具体包括CPU使用情况、内存使用情况、网络吞吐量、文件系统使用情况等信息，并提供本地基础查询界面和API接口，方便与其他第三方工具进行搭配使用。cAdvisor默认将数据缓存在内存中，同时也提供不同的持久化存储后端支持，可将监控数据保存Google BigQuery、InfluxDB或Redis等数据库中。</p><p>cAdvisor基于Go语言开发，利用CGroups获取容器的资源使用信息，目前已被集成在Kubernetes中的Kubelet组件里作为默认启动项。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：docker run -v /var/run:/var/run:rw -v/sys:/sys:ro -v/var/lib/docker:/var/lib/docker:ro -p8080:8080 -d --name cadvisor google/cadvisor）</span><br></pre></td></tr></table></figure><p><strong>4）容器安全审计</strong></p><p><strong>① Docker守护进程审计</strong></p><p>在安全审计方面，对于运行Docker容器的宿主机而言，除需对主机Linux文件系统等进行审计外，还需对Docker守护进程的活动进行审计。由于系统默认不会对Docker守护进程进行审计，需要通过主动添加审计规则或修改规则文件进行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：auditctl -w /usr/bin/docker -k docker或修改/etc/audit/audit.rules文件）</span><br></pre></td></tr></table></figure><p><strong>② Docker相关文件目录审计</strong></p><p>除Docker守护进程之外，还需对与Docker的运行相关的文件和目录进行审计，同样需要通过命令行添加审计规则或修改规则配置文件，具体文件和目录如表4所示。</p><p>表4：Docker相关文件和目录审计</p><table><thead><tr><th>需要审计的文件或目录</th><th>备注</th></tr></thead><tbody><tr><td>/var/lib/docker</td><td>包含有关容器的所有信息</td></tr><tr><td>/etc/docker</td><td>包含Docker守护进程和客户端TLS通信的密钥和证书</td></tr><tr><td>docker.service</td><td>Docker守护进程运行参数配置文件</td></tr><tr><td>docker.socket</td><td>守护进程运行socket</td></tr><tr><td>/etc/default/docker</td><td>支持Docker守护进程各种参数</td></tr><tr><td>/etc/default/daemon.json</td><td>支持Docker守护进程各种参数</td></tr><tr><td>/usr/bin/docker-containerd</td><td>Docker可用containerd生成容器</td></tr><tr><td>/usr/bin/docker-runc</td><td>Docker可用runC生成容器</td></tr></tbody></table><p>Docker公司与美国互联网安全中心（CIS）联合制定了Docker最佳安全实践CIS Docker  Benchmark，目前最新版本为1.2.0。为了帮助Docker用户对其部署的容器环境进行安全检查，Docker官方提供了Docker  Bench for  Security安全配置检查脚本工具docker-bench-security，其检查依据便是CIS制定的Docker最佳安全实践。</p><h3 id="3-3-容器网络安全"><a href="#3-3-容器网络安全" class="headerlink" title="3.3 容器网络安全"></a>3.3 容器网络安全</h3><p><strong>1）容器间流量限制</strong></p><p>由于Docker容器默认的网桥模式不会对网络流量进行控制和限制，为了防止潜在的网络DoS攻击风险，需要根据实际需求对网络流量进行相应的配置。</p><p><strong>① 完全禁止容器间通信</strong></p><p>在特定的应用场景中，如果宿主机中的所有容器无需在三层或四层进行网络通信交互，可通过将Docker daemon的–icc参数设为false以禁止容器与容器间的通信。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：dockerd --icc = false）</span><br></pre></td></tr></table></figure><p><strong>② 容器间流量控制</strong></p><p>在存在多租户的容器云环境中，可能存在单个容器占用大量宿主机物理网卡抢占其他容器带宽的情况。为了保证容器之间的正常通信，同时避免异常流量造成网络DoS攻击等后果，需要对容器之间的通信流量进行一定的限制。</p><p>由于Docker通过创建虚拟网卡对（eth0和veth<em>）将容器与虚拟网桥docker0连接，而容器之间的通信需要经由虚拟网卡对eth0和veth</em>通过网桥连接，因此，可采用Linux的流量控制模块traffic controller对容器网络进行流量限制。</p><p>traffic  controller的原理是建立数据包队列并制定发送规则，实现流量限制与调度的功能。为了在一定程度上减轻容器间的DoS攻击的危害，可将traffic controller的dev设置为宿主机中与各容器连接的veth*虚拟网卡，以此进行宿主机上容器间流量限制。</p><p><strong>2）网桥模式下的网络访问控制</strong></p><p>在默认的网桥连接模式中，连接在同一个网桥的两个容器可以进行直接相互访问。因此，为了实现网络访问控制，可按需配置网络访问控制机制和策略。</p><p><strong>① 为容器创建不同的桥接网络</strong></p><p>为了实现容器间的网络隔离，可将容器放在不同的桥接网络中。当在Docker中使用docker network  create命令创建新的桥接网络时，会在iptables中的DOCKER-ISOLATION新增DROP丢弃规则，阻断与其他网络之间的通信流量，实现容器网络之间隔离的目的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">（命令示例：docker network create --subnet 102.102.0.0/24 test）</span><br></pre></td></tr></table></figure><p><strong>② 基于白名单策略的网络访问控制</strong></p><p>为了保证容器间的网络安全，可默认禁止容器间的通信，然后按需设置网络访问控制规则。</p><p>具体而言，在同一虚拟网络内，不同Docker容器之间的网络访问可通过iptables进行控制。在将Docker  daemon的–icc参数设为false后，iptables的FORWARD链策略为默认全部丢弃。此时，可采用白名单策略实现网络访问控制，即根据实际需要在iptables中添加访问控制策略，以最小化策略减小攻击面。</p><p><strong>3）集群模式下的网络访问控制</strong></p><p>与通过OpenStack建立的虚拟化集群通过VLAN对不同租户进行子网隔离不同，基于Overlay网络的容器集群在同一主机内相同子网中的不同容器之间默认可以直接访问。</p><p>如需控制宿主机外部到内部容器应用的访问，可通过在宿主机iptables中的DOCKER-INGRESS链手动添加ACL访问控制规则以控制宿主机的eth0到容器的访问，或者在宿主机外部部署防火墙等方法实现。</p><p>然而，在大型的容器云环境中，由于存在频繁的微服务动态变化更新，通过手动的方式配置iptables或更新防火墙是不现实的。因此，可通过微分段（Micro-Segmentation）实现面向容器云环境中的容器防火墙。微分段是一种细粒度的网络分段隔离机制，与传统的以网络地址为基本单位的网络分段机制不同，微分段可以以单个容器、同网段容器、容器应用为粒度实现分段隔离，并通过容器防火墙对实现微分段间的网络访问控制。</p><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>与虚拟化技术相比，Docker容器技术具有敏捷化、轻量化等特点，在推进云原生应用方面具有不可替代性。与此同时，容器技术对于高效性的追求也牺牲了隔离性等安全要求，在安全性方面与虚拟化技术相比还存在较大差距，且所涉及的面较广，涉及到容器的镜像安全、内核安全、网络安全、虚拟化安全、运行时安全等各个层面。</p><p>在应用容器技术进行系统部署时，应充分评估安全风险，根据应用场景制定相应安全需求，并整合相关安全解决方案，形成容器安全应用最佳实践。</p>]]></content>
    
    <summary type="html">
    
      Docker容器安全性分析
    
    </summary>
    
      <category term="Docker" scheme="https://louielong.github.io/categories/Docker/"/>
    
    
      <category term="Docker" scheme="https://louielong.github.io/source/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker开启IPv6</title>
    <link href="https://louielong.github.io/docker-ipv6.html"/>
    <id>https://louielong.github.io/docker-ipv6.html</id>
    <published>2020-02-26T05:55:29.000Z</published>
    <updated>2020-02-26T07:32:23.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、序言"><a href="#一、序言" class="headerlink" title="一、序言"></a>一、序言</h2><p>因需要在创建的容器内使用IPv6网络进行测试，详细记录一下Docker如何开始IPv6，以及一些调试的<strong>奇淫技巧</strong>。</p><h2 id="二、-docker配置IPv6"><a href="#二、-docker配置IPv6" class="headerlink" title="二、 docker配置IPv6"></a>二、 docker配置IPv6</h2><p>方案一：直接使用宿主机的网络在容器启动时加入<code>--host</code>参数；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name=busybox --net=host busybox top</span><br></pre></td></tr></table></figure><p>显然方案一，太low，这里就不在介绍了。</p><p>方案二：将宿主机IPv6网络下划分一个子段，通过nd代理容器流量；</p><p>1）查看宿主机IPv6地址</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ubuntu:~$ ifconfig ens3</span><br><span class="line">ens3      Link encap:Ethernet  HWaddr 52:54:00:44:85:34</span><br><span class="line">          inet addr:10.253.1.125  Bcast:10.253.1.255  Mask:255.255.255.0</span><br><span class="line">          inet6 addr: fe80::5054:ff:fe44:8534/64 Scope:Link</span><br><span class="line">          inet6 addr: 2001:eb:8001:e01::125/64 Scope:Global</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>2）划分子段并配置docker</p><p>配置<code>/etc/docker/daemon.json</code>，重启容器进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ubuntu:~$ sudo cat /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">   "registry-mirrors": ["https://registry.docker-cn.com"],</span><br><span class="line">   "ipv6": true,</span><br><span class="line">   "fixed-cidr-v6": "2001:eb:8001:e01:2::/120"</span><br><span class="line">&#125;</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo service docker restart</span><br><span class="line">xdnsadmin@ubuntu:~$ ifconfig docker0</span><br><span class="line">docker0   Link encap:Ethernet  HWaddr 02:42:97:70:d9:1e</span><br><span class="line">          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:255.255.0.0</span><br><span class="line">          inet6 addr: 2001:eb:8001:e01:2::1/120 Scope:Global</span><br><span class="line">          inet6 addr: fe80::42:97ff:fe70:d91e/64 Scope:Link</span><br><span class="line">          inet6 addr: fe80::1/64 Scope:Link</span><br><span class="line">          UP BROADCAST MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:265 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:101 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:36482 (36.4 KB)  TX bytes:12592 (12.5 KB)</span><br></pre></td></tr></table></figure><p>3）配置转发</p><p>配置NDP（邻居发现协议）代理。关于这部分内容，请参见 <a href="https://docs.docker.com/v17.09/engine/userguide/networking/default_network/ipv6/#using-ndp-proxying" target="_blank" rel="noopener">Using NDP proxying</a>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ubuntu:~$ sudo ip -6 neigh add proxy 2001:eb:8001:e01:2::2 dev ens3</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo sysctl net.ipv6.conf.default.forwarding=1</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo sysctl net.ipv6.conf.all.forwarding=1</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo sysctl net.ipv6.conf.ens3.accept_ra=0</span><br><span class="line">net.ipv6.conf.ens3.accept_ra = 0</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo sysctl net.ipv6.conf.ens3.proxy_ndp=1</span><br><span class="line">net.ipv6.conf.ens3.proxy_ndp = 1</span><br></pre></td></tr></table></figure><p>4）测试IPv6</p><p>启动ubuntu 18.04镜像测试是否有IPv6网络，由于18.04镜像内无<code>ifconfig</code>和<code>ip</code>命令，可以安装相应工具或在宿主机上ping测试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ubuntu:~$ docker run -itd ubuntu:18.04</span><br><span class="line">78604aa6c229</span><br><span class="line">xdnsadmin@ubuntu:~$ docker inspect 78604aa6c229</span><br><span class="line">....</span><br><span class="line">            "Networks": &#123;</span><br><span class="line">                "bridge": &#123;</span><br><span class="line">                    "IPAMConfig": null,</span><br><span class="line">                    "Links": null,</span><br><span class="line">                    "Aliases": null,</span><br><span class="line">                    "NetworkID": "91f147cc84bbe58b8d817e98866573768b70c090463066fc086735860e3e586e",</span><br><span class="line">                    "EndpointID": "adb855e72644da1e5959e2718f06149308c83fce3c9471d251eaedf31c6a33dd",</span><br><span class="line">                    "Gateway": "172.17.0.1",</span><br><span class="line">                    "IPAddress": "172.17.0.2",</span><br><span class="line">                    "IPPrefixLen": 16,</span><br><span class="line">                    "IPv6Gateway": "2001:eb:8001:e01:2::1",</span><br><span class="line">                    "GlobalIPv6Address": "2001:eb:8001:e01:2::2",</span><br><span class="line">                    "GlobalIPv6PrefixLen": 120,</span><br><span class="line">                    "MacAddress": "02:42:ac:11:00:02",</span><br><span class="line">                    "DriverOpts": null</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">....</span><br><span class="line">xdnsadmin@ubuntu:~$ ping6 2001:eb:8001:e01:2::2 -c 2</span><br><span class="line">PING 2001:eb:8001:e01:2::2(2001:eb:8001:e01:2::2) 56 data bytes</span><br><span class="line">64 bytes from 2001:eb:8001:e01:2::2: icmp_seq=1 ttl=64 time=0.263 ms</span><br><span class="line">64 bytes from 2001:eb:8001:e01:2::2: icmp_seq=2 ttl=64 time=0.058 ms</span><br><span class="line"></span><br><span class="line">--- 2001:eb:8001:e01:2::2 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.058/0.160/0.263/0.103 ms</span><br></pre></td></tr></table></figure><p>如需在镜像内测试，需要安装相应工具</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ubuntu:~$ docker exec -it 78604aa6c229 bash</span><br><span class="line">root@78604aa6c229:/# sed -i "s/archive.ubuntu.com/mirrors.aliyun.com/g" /etc/apt/sources.list &amp;&amp; apt update &amp;&amp; apt install -y iproute2 iputils-ping</span><br><span class="line">.....</span><br><span class="line">root@78604aa6c229:/# ip a</span><br><span class="line">....</span><br><span class="line">34: eth0@if35: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 2001:eb:8001:e01:2::2/120 scope global nodad</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:acff:fe11:2/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">root@78604aa6c229:/# ping6 240c::6666 -c 2</span><br><span class="line">PING 240c::6666(240c::6666) 56 data bytes</span><br><span class="line">64 bytes from 240c::6666: icmp_seq=1 ttl=62 time=0.644 ms</span><br><span class="line">64 bytes from 240c::6666: icmp_seq=2 ttl=62 time=0.413 ms</span><br><span class="line"></span><br><span class="line">--- 240c::6666 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.413/0.528/0.644/0.117 ms</span><br></pre></td></tr></table></figure><p><strong>【奇淫技巧】</strong>也可以在宿主机上测试通过namespace的方式测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ubuntu:~$ docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS              PORTS                                              NAMES</span><br><span class="line">78604aa6c229        ubuntu:18.04            "/bin/bash"              3 hours ago         Up 16 minutes                                                          epic_blackwell</span><br><span class="line">xdnsadmin@ubuntu:~$ docker inspect -f '&#123;&#123;.State.Pid&#125;&#125;' epic_blackwell</span><br><span class="line">10740</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo mkdir /var/run/netns/</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo ln -fs /proc/10740/ns/net /var/run/netns/epic_blackwell</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo ip netns exec epic_blackwell ip -c a</span><br><span class="line">....</span><br><span class="line">34: eth0@if35: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 2001:eb:8001:e01:2::2/120 scope global nodad</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:acff:fe11:2/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">xdnsadmin@ubuntu:~$ sudo ip netns exec epic_blackwell ping6 240c::6666 -c 2</span><br><span class="line">PING 240c::6666(240c::6666) 56 data bytes</span><br><span class="line">64 bytes from 240c::6666: icmp_seq=1 ttl=62 time=0.548 ms</span><br><span class="line">64 bytes from 240c::6666: icmp_seq=2 ttl=62 time=0.414 ms</span><br><span class="line"></span><br><span class="line">--- 240c::6666 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 1000ms</span><br><span class="line">rtt min/avg/max/mdev = 0.414/0.481/0.548/0.067 ms</span><br></pre></td></tr></table></figure><p>上述操作可以保存为一个脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">id=docker inspect -f '&#123;&#123;.State.Pid&#125;&#125;' $1</span><br><span class="line">sudo mkdir /var/run/netns/</span><br><span class="line">sudo ln -fs /proc/$id/ns/net /var/run/netns/$1</span><br></pre></td></tr></table></figure><p>方案二的一个缺点是每创建一个容器或重启机器都需要添加<code>ip -6 neigh add proxy &lt;ipv6_addr&gt; dev ens3</code>，对于自动化构建较为麻烦。</p><p>【Tips】:检查宿主机DNS配置，在配置过程中出现宿主机仅配置了IPv6的DNS导致容器在开启IPv6服务后容器无法解析域名的情况。</p><p>方案三：创建支持IPv6的网桥</p><p>IPv6地址需要依据宿主机地址段来修改。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ubuntu:~$ docker network create -d bridge --ipv6 --subnet "2001:eb:8001:e01:3::/120" --gateway="2001:eb:8001:e01:3::1" --subnet=172.30.0.0/16 --gateway=172.30.0.1 IPv6Net</span><br><span class="line">xdnsadmin@ubuntu:~$ docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">018c6246979b        IPv6Net             bridge              local</span><br><span class="line">91f147cc84bb        bridge              bridge              local</span><br><span class="line">e3db5466ef0a        host                host                local</span><br><span class="line">f3c3efd928a6        none                null                local</span><br><span class="line">xdnsadmin@ubuntu:~$ docker run -itd --ip=172.30.0.2 --ip6="2001:eb:8001:e01:3::2" --network=IPv6Net ubuntu:18.04</span><br><span class="line">6d9933a583a6c5890f951ff8ab81382fc315c4ac862833397dc047562e667a51</span><br><span class="line">xdnsadmin@ubuntu:~$ docker exec -it 6d9933a583a bash</span><br><span class="line">root@6d9933a583a6:/# ed -i "s/archive.ubuntu.com/mirrors.aliyun.com/g" /etc/apt/sources.list &amp;&amp; apt update &amp;&amp; apt install -y iproute2 iputils-ping</span><br><span class="line">....</span><br><span class="line">root@6d9933a583a6:/# ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">41: eth0@if42: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:ac:1e:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 172.30.0.2/16 brd 172.30.255.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 2001:eb:8001:e01:3::2/120 scope global nodad</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:acff:fe1e:2/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">root@6d9933a583a6:/# ping6 240c::6666</span><br><span class="line">PING 240c::6666(240c::6666) 56 data bytes</span><br><span class="line">^C</span><br><span class="line">--- 240c::6666 ping statistics ---</span><br><span class="line">4 packets transmitted, 0 received, 100% packet loss, time 3023ms</span><br><span class="line"></span><br><span class="line">root@6d9933a583a6:/# ping baidu.com</span><br><span class="line">PING baidu.com (39.156.69.79) 56(84) bytes of data.</span><br><span class="line">64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=1 ttl=45 time=25.6 ms</span><br><span class="line">64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=2 ttl=45 time=28.5 ms</span><br><span class="line">64 bytes from 39.156.69.79 (39.156.69.79): icmp_seq=3 ttl=45 time=23.6 ms</span><br><span class="line">^C</span><br><span class="line">--- baidu.com ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 2446ms</span><br><span class="line">rtt min/avg/max/mdev = 23.696/25.947/28.536/1.998 ms</span><br></pre></td></tr></table></figure><p> 容器仍然无法访问v6网络，需要向方案二一样添加配置NDP（邻居发现协议）代理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ubuntu:~$ sudo ip -6 neigh add proxy 2001:eb:8001:e01:3::2 dev ens3</span><br><span class="line">[sudo] password for xdnsadmin:</span><br><span class="line">xdnsadmin@ubuntu:~$ docker exec -it 6d9933a583a bash</span><br><span class="line">root@6d9933a583a6:/# ping6 240c::6666</span><br><span class="line">PING 240c::6666(240c::6666) 56 data bytes</span><br><span class="line">64 bytes from 2001::6666: icmp_seq=1 ttl=62 time=320 ms</span><br><span class="line">64 bytes from 2001::6666: icmp_seq=2 ttl=62 time=0.370 ms</span><br><span class="line">64 bytes from 2001::6666: icmp_seq=3 ttl=62 time=0.447 ms</span><br><span class="line"></span><br><span class="line">--- 240c::6666 ping statistics ---</span><br><span class="line">3 packets transmitted, 3 received, 0% packet loss, time 1998ms</span><br><span class="line">rtt min/avg/max/mdev = 0.370/107.216/320.833/151.050 ms</span><br></pre></td></tr></table></figure><p>相比方案二方案三的好处是能够指定容器的IP，可以预先配置好NPD代理。同样都有的缺点是IPv6无法像IPv4一样仅暴漏端口，IPv6下地址全端口都开放。</p><p>【参考链接】</p><p>1）<a href="https://www.v2ex.com/t/553822#reply0" target="_blank" rel="noopener">docker + ipv6</a></p><p>2）<a href="https://blog.csdn.net/taiyangdao/article/details/83066009" target="_blank" rel="noopener">Docker容器支持IPv6的方法</a></p>]]></content>
    
    <summary type="html">
    
      Docker 开启IPv6配置
    
    </summary>
    
      <category term="Docker" scheme="https://louielong.github.io/categories/Docker/"/>
    
    
      <category term="Docker" scheme="https://louielong.github.io/source/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>NFV的一些思考</title>
    <link href="https://louielong.github.io/NFV-thinking.html"/>
    <id>https://louielong.github.io/NFV-thinking.html</id>
    <published>2019-11-04T03:38:19.000Z</published>
    <updated>2019-11-05T09:21:23.000Z</updated>
    
    <content type="html"><![CDATA[<p>先前看到<code>litght reading</code>的NFV已死，也尝试进行了翻译<a href="nfv-is-dead.html">NFV已死</a>，从文中不断看到SASE、云原生这样的词汇。2017年初由于工作原因从IOT转向了NFV相关工作，随后接触到了Openstack、OPNFV、ONAP，这段时间的工作使自己对于网络、云计算、NFV、docker容器、k8s有了很多的认识，也为自己在超级算力中心的整体规划设计以及建设中积累了宝贵的经验。</p><p>话说回来，在从事NFV研究的一年多时间里，NFV始终是雷声大雨点小，我所在的OPNFV社区主要关注NFV中的基础设计层即NFVI层的测试，我参与了OPNFV的Pharos项目以及OVP项目的首批内测。在多次测试活动中也了解了三大运营商以及国内主流厂商如华为、中兴、华三等厂商的NFV产品。但是NFV呼声虽高，却没有看到太多的市场，运营商期望通过通用x86+软件的方式来去除厂商黑盒的绑定，但是最后还是掉落到了硬件厂商的陷阱里，期望的硬件+虚拟化+云平台的三层解耦在现实中也只是泡影，最终还是被厂商的硬件(x86)+云平台(Openstack)+网元(VNF)所绑定。运营商都会有自研的NFVO编排器，期望通过自己的编排器与厂商A的VNF和厂商B的云平台层进行解耦调度。在我们的测试中基本的网元LCM生命管理都没什么问题，但是涉及到高级的特性如：自愈、热迁移等都会出现问题[1]。同时在ONAP的设想中有一个VNF测试项目，期望将厂商自研的VNF网元做成一个类似于应用市场的APP，使得运营商可以直接在应用市场里选择需要的网元，但也面临许多的挑战。 </p><p>NFV已死的文中提到了SASE(Secure Access Service Edge，安全访问服务边缘)将会取代NFV，我找了下关于SASE的描述，在<a href="https://www.sdxcentral.com/articles/sponsored/syndicated/blog/cato-and-the-secure-access-service-edge-where-your-digital-business-network-starts/2019/10/" target="_blank" rel="noopener">Cato and the Secure Access Service Edge: Where Your Digital Business Network Starts</a>中提到了SASE的四个特点。</p><blockquote><p>它提供了一个单一网络，可在任何地方连接并保护任何企业资源（物理，云和移动）。 在这种情况下，SASE Cloud具有四个主要特征：它是身份驱动的，“云原生”的，支持所有边缘的并且在全球范围内分布：</p><p>​     身份驱动。用户和资源身份，而不仅仅是IP地址，决定了网络体验和访问权限级别。服务质量，路由选择，应用风险驱动的安全控制-所有这些都由与每个网络连接关联的身份驱动。通过让公司为用户开发一套网络和安全策略，而不论设备或位置如何，该方法都可以减少运营开销。<br>​    云原生架构。 SASE体系结构利用了关键的云功能（包括弹性，适应性，自修复和自维护），提供了一个可分摊客户成本以实现最大效率的平台，可轻松适应新兴业务需求，并可在任何地方使用。<br>​    支持所有边缘。 SASE为所有公司资源（数据中心，分支机构，云资源和移动用户）创建一个网络。例如，SD-WAN设备支持物理边缘，而移动客户端和无客户端浏览器访问则可以连接用户。<br>​    全球分布。为确保全面的网络和安全功能随处可用，并向所有边缘提供最佳体验，SASE云必须在全球范围内分布。因此，Gartner指出，他们必须扩展自己的足迹，以向企业边缘提供低延迟的服务。</p></blockquote><p>云原生也是这两年来提到的非常多的概念，结合云计算以及火热的docker容器、k8s编排、微服务等，服务提供商在开发应用时要更多的结合云和容器的一些特性来提供服务。边缘计算也是近来热门的话题，从18年ETSI将MEC的定义为Multi-Access Edge Computing 多接入边缘计算而不是先前大家都认为的Mobile Edge Compute移动边缘计算。这样看起来SASE确实很适应潮流的发展。</p><p>但是这并不是说NFV已经死亡，借助虚拟化技术的发展，网络功能虚拟化仍将继续发展，在云中东西向流量防护以及安全方面NFV还是有发展余地的，此外5G 的各个网元也是NFV的主场。NFV现在的冷淡也只是没有像最开始吹的那么神，什么网络功能都能虚拟化，由于性能和稳定的的不足，一些核心网络还是没有办法全上NFV，但是在边缘接入上NFV仍然能够有的放矢。我认为更多的只是NFV技术的慢慢趋于成熟，概念上不再新颖，炒的人少了而已。</p><p>Ps：话又说回来，确实NFV的市场前景越来越不乐观，因此我也么有更多的专注在NFV方面了 - -。</p><p>【1】<a href="https://www.sdnctc.com/download/resource_download/id/6" target="_blank" rel="noopener">2018 SDN+NFV+IPv6 Fest白皮书</a></p>]]></content>
    
    <summary type="html">
    
      最近以来的一些NFV的思考
    
    </summary>
    
      <category term="NFV" scheme="https://louielong.github.io/categories/NFV/"/>
    
    
      <category term="NFV" scheme="https://louielong.github.io/source/tags/NFV/"/>
    
      <category term="SASE" scheme="https://louielong.github.io/source/tags/SASE/"/>
    
  </entry>
  
  <entry>
    <title>NFV已死-云杀死的</title>
    <link href="https://louielong.github.io/nfv-is-dead.html"/>
    <id>https://louielong.github.io/nfv-is-dead.html</id>
    <published>2019-11-04T03:23:48.000Z</published>
    <updated>2019-11-04T03:36:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文翻译自<a href="https://www.lightreading.com/cloud/gartner-nfv-is-dead-andndash-the-cloud-killed-it-/d/d-id/754790?utm_source=linkedin&utm_medium=social_organic&utm_content=2f9905b7-b5d4-4bf4-bf4d-b7e846075d39&utm_campaign=GFMC_global_Big_Ideas_Blog_20180101" target="_blank" rel="noopener">Gartner: NFV Is Dead– the Cloud Killed It</a>，可能存在翻译不准确的地方。</p><p>Enterprises are demanding a new generation of cloud-based wide-area networking services that’s swallowing up SD-WAN, killing network functions virtualization (NFV) and challenging existing telco business and technology models, according to Gartner analysts. </p><p>根据Gartner的分析师，企业需要新一代的基于云的广域网服务，这些服务将吞噬SD-WAN，杀死网络功能虚拟化（NFV）并挑战现有的电信业务和技术模型。</p><p>Gartner has given the new network delivery business model a name, and it’s an ugly one: SASE, pronounced “sassy,” which stands for the “Secure Access Service Edge.” And if Gartner is right, the effect on service providers’ business is going to be ugly too. </p><p>Gartner给新的网络交付业务模型起了一个名字，一个非常丑陋的名字：SASE，发音为“ sassy”，代表“安全访问服务边缘”(Secure Access Service Edge)。 如果Gartner是正确的，那么对服务提供商业务的影响也将是丑陋的。</p><p>The SASE transformation has been building for years. Five years ago, almost all enterprise applications and data lived in the data center, Gartner analyst Joe Skorupa tells Light Reading. Branch office networking connected to the data center, as did remote workers. Whatever cloud access was necessary then went to the data center first, then out to the public Internet. </p><p>SASE转型已经进行了多年。 Gartner分析师Joe Skorupa告诉Light Reading，五年前，几乎所有企业应用程序和数据都存在于数据中心。 分支机构网络连接到数据中心，远程工作者也是如此。 无论需要什么云访问权限，然后都首先访问数据中心，然后再访问公共Internet。</p><p>“Now, applications are pretty much everywhere,” Skorupa says. Some are in the data center, some are outside of it. Mission-critical applications live in the cloud, including Workday, Microsoft Office 365, and custom applications written for Microsoft Azure and Amazon Web Services. “The data center is no longer the center of the universe,” he says. </p><p>“现在，到处都有应用程序，” Skorupa说。 有些在数据中心内，有些在数据中心外。 关键任务应用程序存在于云中，包括Workday，Microsoft Office 365和为Microsoft Azure和Amazon Web Services编写的自定义应用程序。 他说：“数据中心不再是宇宙的中心。”</p><p>Skorupa adds, “We have gone from having a ‘data center’ to having ‘centers of data,’ and they are all over the place.”</p><p>Skorupa补充说：“我们已经从拥有“数据中心”变为拥有“中心的数据”，并且它们遍布各处。”</p><p>Likewise, consumers of data aren’t just branch offices. Endpoints are mobile. “They’re a sales executive sitting in a car with a cup of coffee and an iPad,” Skorupa says. “They’re not funneling through the data center. It’s a hub and spoke. But the hub is the individual, which could be a person, could be an IoT device, and could be software.”</p><p>同样，数据的使用者不仅仅是分支机构。 端点是移动的。 Skorupa说：“他们是一名销售主管，坐在汽车上喝着咖啡和iPad。” “他们并没有通过数据中心进行传输。它是一个枢纽。但是枢纽是个体，可以是人，可以是IoT设备，也可以是软件。”</p><p>The new network architecture requires different technologies to suit different needs, Skorupa says. For example, a home worker doesn’t need SD-WAN because they’re not balancing multiple links, but that worker does need quality-of-service guarantees to make video calls. On the other hand, a branch office requires SD-WAN for security and path selection. </p><p>Skorupa说，新的网络架构需要不同的技术来满足不同的需求。 例如，家庭工作者不需要SD-WAN，因为他们没有多个链接需要平衡，但是该工作者确实需要服务质量保证才能进行视频通话。 另一方面，分支机构需要SD-WAN进行安全性和路径选择。</p><p>The changing nature of business requires changing security policies and technology as well, Skorupa says. “If it’s a contractor using an untrusted laptop logging in from Southeast Asia at two o’clock Sunday morning directly into Salesforce, trying to get at the entire client database, you want to apply a lot of security policy against that,” Skorupa says. </p><p>Skorupa说，业务性质的变化也需要安全策略和技术的变化。 Skorupa说：“如果承包商使用的是不受信任的笔记本电脑，则需要在周日凌晨两点从东南亚直接登录到Salesforce，以获取整个客户数据库，那么您将对此应用大量的安全策略。”</p><p>Additionally, enterprise locations need intrusion detection and prevention services (IDS/IPS), data loss prevention (DLP), anti-spam, anti-malware, whitelisting, blacklisting and so on. “The overhead of trying to keep that stuff patched is a nightmare. You’re always out of date. You’re not going to put seven boxes stacked up – and duct them to the back of my iPad when I’m traveling,” Skorupa says. Cloud delivery is the only model that makes sense. </p><p>此外，企业场所需要入侵检测和预防服务（IDS / IPS），数据丢失预防（DLP），反垃圾邮件，反恶意软件，白名单，黑名单等。 “试图修补这些东西的开销是一场噩梦。您总是错过最佳防护时机。您不会愿意在旅行时将他们七个盒子叠在一起并放在iPad的背面， ” Skorupa说。 云交付是唯一有意义的模型。</p><p>He adds, “The only way to apply policy anywhere and everywhere, scaling up and scaling down as needed, delivering a set of functions you need on demand, is to deliver it primarily cloud-based.”</p><p>他补充说：“在任何地方和任何地方应用策略，按需扩展和按比例缩小，提供按需提供的功能集的唯一方法是基于云来提供它。”</p><p><strong>R.I.P. CPE</strong><br> That means on-premises equipment needs to go from being the standard way of delivering enterprise services to a specialized case, says the Gartner man. </p><p>Gartner负责人说，这意味着本地设备需要从提供企业服务的标准方式转变为特殊情况。</p><p>“The model says on-prem only when you must, cloud-delivered whenever you can,” Skorupa says.</p><p>Skorupa说：“该模型仅在必要时才在企业内部显示，并在可能的情况下通过云交付。”</p><p>This “represents an existential threat to NFV” because NFV depends on selling expensive boxes that happen to be x86-based. The cost benefits promised initially for NFV failed to materialize because vendors simply refused to lower their prices by a lot, Skorupa says.</p><p>这“代表对NFV的生存威胁”，因为NFV依赖出售恰好基于x86的昂贵盒子。 Skorupa说，最初承诺为NFV提供的成本收益未能实现，因为供应商只是拒绝大幅降低价格。 </p><p>NFV proved “incredibly complicated,” and while the telco industry struggled to make it work, “application consumption patterns changed and the branch was no longer the center of the universe, and a solution that was non-scalable and hard to maintain and expensive and complex winds up being obsoleted by something that is elastic and easy to maintain and it’s cloud delivered,” Skorupa says. </p><p>事实证明NFV“异常复杂”，而电信业正努力使之运转，“应用程序消费模式发生了变化，分支机构不再是整个领域的中心， 一个不可扩展、难以维护且价格昂贵的解决方案，将被弹性、易于维护的东西淘汰了，并且使用了云交付，” Skorupa说。</p><p>There are cases where NFV makes sense. “But by and large the days of NFV have already come and gone. It’s basically stillborn,” Skorupa says. </p><p>在某些情况下，NFV是有意义的。 “但是总的来说，NFV的时代已经过去了。它基本上已经死了，” Skorupa说。</p><p>In a July note, Gartner recommends several steps for technology and service providers to succeed in the new market. They need to transform offerings to a cloud-native architecture, transform business models to “cloud-native-as-a service,” deliver “a clear vision” to the market, fill out their “portfolio organically, with the fewest acquisitions possible to minimize integration challenges and inconsistencies across services,” and invest in distributed real estate, such as PoPs and colocation facilities, to place service as close to the access point as required. </p><p>在7月的一份报告中，Gartner建议技术和服务提供商在新市场上取得成功的几个步骤。 他们需要将产品转变为云原生架构，将业务模型转变为“云原生即服务”，向市场提供“清晰的愿景”，通过最少的收购来有机地组合产品组合，以最大程度地减少集成挑战和服务之间的不一致，并投资于分布式资产（例如PoP和托管设施），以根据需要将服务放置在靠近接入点的位置。</p><p>Gartner names several vendors as already network-security focused, including Cato Networks, Fortinet, Forcepoint, Juniper and Versa Networks. Other SD-WAN vendors without cloud-delivered security are partnering with Zscaler, Palo Alto Networks and others. </p><p>Gartner列出了多家已经关注网络安全的供应商，包括Cato Networks，Fortinet，Forcepoint，Juniper和Versa Networks。 其他没有云交付安全性的SD-WAN供应商正在与Zscaler，Palo Alto Networks等合作。</p><p>Of course, the industry being what it is, these vendors are going into paroxysms of joy by merely being mentioned by Gartner. Versa and Cato Networks put out press releases and statements on their websites, and zScaler devoted some discussion to the subject on an earnings call.</p><p>当然，无论是什么行业，这些供应商都只是被Gartner提及而陷入欢乐的阵营。 Versa和Cato Networks在其网站上发布了新闻稿和声明，而zScaler在财报电话会议上对该主题进行了一些讨论。</p><p><strong>Telcos behind the eight-ball</strong></p><p><strong>电信公司陷入困境</strong><br> Cato Networks, for one, sees the shift to SASE as a competitive advantage. “Telcos are behind the eight-ball,” Yishah Yovel, Cato CMO and chief strategist, tells Light Reading. Telco networks are based on appliances, and they’re two years behind catching up on the cloud networking model. </p><p>例如，Cato Networks将向SASE的转变视为一种竞争优势。 Cato首席营销官兼首席策略师Yishah Yovel对Light Reading表示：“电信公司陷入困境。” 电信网络是基于设备的，比追赶云网络模型落后了两年。</p><p>Telcos are disadvantaged because they don’t own the code. “If I’m a Palo Alto or Zscaler, I have my own code. I already have some percentage of the SASE platform. Telcos don’t operate this way. They integrate other people’s code. That’s very dangerous for them, unless they become more of a software player.”</p><p>电信公司处于不利地位，因为它们不拥有代码。 “如果我是Palo Alto或Zscaler，我有我自己的代码。我已经拥有一定比例的SASE平台。电信公司无法以这种方式运行。它们会集成其他人的代码。这对他们来说非常危险，除非他们变得 更多的软件播放器。”</p><p>Looked at one way, Gartner’s SASE pitch is nothing new. Indeed, when a Cato Networks spokesman brought it to my attention a few weeks ago, I initially scoffed.</p><p>从一种角度看，Gartner的SASE呼声渐涨并不是什么新鲜事物。 确实，几周前，当Cato Networks的一位发言人提请我注意时，我开始嘲笑它。 </p><p>Normally I would have been more polite, but I was in a bad mood on account of being still jet-lagged and sleep deprived from a trip to Dallas, to Light Reading’s Network Virtualization &amp; Software Defined Networking conference, which was all about the trends Gartner had apparently just discovered. And it wasn’t the first year we’ve done that conference; far from it. So my first reaction to the tip was, “<a href="https://knowyourmeme.com/memes/captain-obvious" target="_blank" rel="noopener">Thank you, Captain Obvious!</a>“</p><p>通常，我本来会比较有礼貌，但是由于刚到达拉斯的时差，无法参加Light Reading的网络虚拟化和软件定义网络会议，所以我心情很不好，这也与Gartner刚刚发现的趋势有关。 这不是我们召开会议的第一年。所以我对小费的第一反应是：“谢谢，上尉船长！”</p><p>The software-defined networking (SDN) movement, launched at the beginning of the decade, was all about moving network intelligence into software for increased agility; the reason we don’t hear much about that anymore is because the philosophy has become mainstream.</p><p>十年之初发起的软件定义网络（SDN）运动就是将网络智能转移到软件中以提高敏捷性。 我们之所以不再听到太多的原因是因为该哲学已成为主流。</p><p>More recently, AT&amp;T, Orange and startup Rakuten are aggressively moving their networks to cloud architectures. Just last week, <a href="https://www.lightreading.com/nfv/nfv-elements/colt-brings-virtual-networks-to-enterprise-premises/d/d-id/754763" target="_blank" rel="noopener">Colt launched</a> a new line of universal CPE (uCPE) equipment, providing SD-WAN, firewall and other services to enterprises, based on NFV.</p><p>最近，AT＆T，Orange和创业公司Rakuten正在积极地将其网络迁移到云架构。 就在上周，Colt推出了新的通用CPE（uCPE）设备系列，为基于NFV的企业提供SD-WAN，防火墙和其他服务。</p><p>Still, NFV has attracted skeptics almost since its founding in 2012, and at about the same time Gartner issued its SASE note, we <a href="https://www.lightreading.com/nfv/nfv-strategies/smokey-and-the-nfv-bandit/a/d-id/753939" target="_blank" rel="noopener">reported</a> that critics were saying the technology is too rigid and monolithic for the cloud era, (though Prayson Pate, CTO, Edge Cloud, ADVA Optical Networking <a href="https://www.lightreading.com/nfv/nfv-strategies/smokey-and-the-nfv-bandit/a/d-id/753939" target="_blank" rel="noopener">took issue with our report</a>). </p><p>尽管如此，NFV几乎自2012年成立以来就一直引起怀疑论者的关注，并且大约在同一时间，Gartner发布了SASE说明，据我们报道，批评者称该技术对于云时代而言过于僵化和整体化（尽管 Prayson Pate (ADVA光网络公司Edge Cloud 的CTO)对我们的报告对提出了质疑）。</p><p>However, my initial dismissal was misplaced. Gartner does a good job of weaving together and articulating several long-term trends shaping the service provider business and networks. Gartner deserves credit for stepping back and summarizing a decade of trends in a few pages. </p><p>但是，我最初的解雇是错误的。 Gartner很好地组织并阐明了影响服务提供商业务和网络的几种长期趋势。 Gartner值得一提的是在几页中后退并总结了十年的趋势。</p><p>Also, Gartner is influential, particularly among enterprises who are service provider customers. Gartner’s SASE coinage means ideas about wide-area network virtualization and cloudification have gone mainstream. Telcos are going to start hearing demand for SASE, and need to be prepared to meet it. </p><p>此外，Gartner很有影响力，特别是在服务提供商客户企业中。 Gartner的SASE概念意味着有关广域网虚拟化和云化的想法已成为主流。 电信公司将开始听到对SASE的需求，并且需要做好准备以满足它。</p><p>For more about how AT&amp;T, Orange, Rakuten and other service providers already cloudifying and virtualizing their networks, see these articles:</p><ul><li><a href="https://www.lightreading.com/nfv/nfv-elements/colt-brings-virtual-networks-to-enterprise-premises/d/d-id/754763" target="_blank" rel="noopener">Colt Brings Virtual Networks to      Enterprise Premises</a></li><li><a href="https://www.lightreading.com/automation/atandts-wheelus-from-mechanization-to-automation/d/d-id/754688" target="_blank" rel="noopener">AT&amp;T’s Wheelus: From Mechanization      to Automation</a></li><li><a href="https://www.lightreading.com/mobile/4g-lte/alaskas-rakuten-preps-for-4g-launch/d/d-id/754538" target="_blank" rel="noopener">Alaska’s Rakuten Preps for 4G Launch</a></li><li><a href="https://www.lightreading.com/optical-ip/why-atandts-latest-open-source-contribution-matters/d/d-id/754484" target="_blank" rel="noopener">Why AT&amp;T’s Latest Open Source      Contribution Matters</a></li><li><a href="https://www.lightreading.com/nfv/nfv-specs-open-source/common-nfvi-telco-taskforce-(cntt)-boasts-reference-milestone/d/d-id/754308" target="_blank" rel="noopener">Common NFVI Telco Taskforce (CNTT)      Boasts Reference Milestone</a></li><li><a href="https://www.lightreading.com/carrier-sdn/sdn-technology/atandt-on-track-for-100--core-network-virtualization-next-year/d/d-id/754104" target="_blank" rel="noopener">AT&amp;T on Track for 100% Core      Network Virtualization Next Year</a></li><li><a href="https://www.lightreading.com/mobile/5g/rakuten-delays-launch-of-cloud-native-network-/d/d-id/753940" target="_blank" rel="noopener">Rakuten Delays Launch of Cloud-Native      Network </a></li><li><a href="https://www.lightreading.com/cloud/applications/its-groundhog-day!-explaining-atandts-microsoft-and-ibm-deals/a/d-id/753001" target="_blank" rel="noopener">It’s Groundhog Day! Explaining      AT&amp;T’s Microsoft &amp; IBM Deals</a></li><li><a href="https://www.lightreading.com/cloud/iot-and-edge/atandt-and-microsoft-ink-extensive-deal-for-cloud-5g-ai-and-edge/d/d-id/752852" target="_blank" rel="noopener">AT&amp;T &amp; Microsoft Ink      ‘Extensive’ Deal for Cloud, 5G, AI &amp; Edge</a></li><li><a href="https://www.lightreading.com/nfv/nfv-strategies/is-x-by-orange-showing-us-the-ott-future-for-telcos/d/d-id/746400" target="_blank" rel="noopener">Is X by Orange Showing Us the OTT      Future for Telcos?</a></li></ul>]]></content>
    
    <summary type="html">
    
      NFV已死
    
    </summary>
    
      <category term="NFV" scheme="https://louielong.github.io/categories/NFV/"/>
    
    
      <category term="NFV" scheme="https://louielong.github.io/source/tags/NFV/"/>
    
  </entry>
  
  <entry>
    <title>the_zen_of_python</title>
    <link href="https://louielong.github.io/the-zen-of-python.html"/>
    <id>https://louielong.github.io/the-zen-of-python.html</id>
    <published>2019-07-24T01:51:15.000Z</published>
    <updated>2019-07-24T02:54:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>看到“GitHubDaily”公众号推送一个关于<a href="https://github.com/jackfrued/Python-100-Days" target="_blank" rel="noopener">Python-100-Days</a>的一个项目，觉得里面的东西蛮有意思，之前由于工作需要也玩过一段时间的python，但是没有很系统的学习，所以打算花上一段时间根据这个项目进行较为系统的学习。</p><p>在教程的第一节就是关于python之禅的介绍，搜索了一下网上的翻译，觉得很有意思这里也转载记录一下。</p><h2 id="二、python禅道"><a href="#二、python禅道" class="headerlink" title="二、python禅道"></a>二、python禅道</h2><p>看到很对提及此事的文章中都提到了python禅道出自PEP20，这里也顺带介绍一下PEP[1]</p><h3 id="2-1-什么是PEP"><a href="#2-1-什么是PEP" class="headerlink" title="2.1 什么是PEP"></a>2.1 什么是PEP</h3><p>PEP 是 Python 增强提案(Python Enhancement Proposal)的缩写。社区通过PEP来给 Python 语言建言献策，每个版本你所看到的新特性和一些变化都是通过PEP提案经过社区决策层讨论、投票决议，最终才有我们看到的功能。</p><h3 id="2-2-PEP8"><a href="#2-2-PEP8" class="headerlink" title="2.2 PEP8"></a>2.2 PEP8</h3><p>如果你还不知道PEP8是什么，可能还算不上一位合格的Python程序员，PEP8是每个Python程序员必读的提案，Python虽然以语法简洁著称，但并不意味着你就一定能写出简洁优雅的代码，PEP8风格指南定义了编写  Python  代码的规范和应该遵守的编码原则，大家都应该按照此规范约束代码，多读几遍此规范，做到了然于心。网上有各种版本的中译版，可选择性参考阅读。</p><p>Tim Peter 回答过什么是好的Python代码？</p><blockquote><p>有个通用的约定应该是可维护的、清晰可懂的、满足一致性的，同时也应该是好的编程习惯的基础。它不会违背你的意愿来强制要求你遵循那些规则。这就是Python!” </p></blockquote><p>地址：[pep-0008][<a href="https://www.python.org/dev/peps/pep-0008/]" target="_blank" rel="noopener">https://www.python.org/dev/peps/pep-0008/]</a></p><h3 id="2-3-PEP20-the-zen-of-python"><a href="#2-3-PEP20-the-zen-of-python" class="headerlink" title="2.3 PEP20 - the zen of python"></a>2.3 PEP20 - the zen of python</h3><p>打开终端输入<code>python</code>，<code>import this</code>即可看到Tim Peters的<strong>Then Zen of Python</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">python3</span><br><span class="line">Python 3.5.2 (default, Nov 12 2018, 13:43:14)</span><br><span class="line">[GCC 5.4.0 20160609] on linux</span><br><span class="line">Type "help", "copyright", "credits" or "license" for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import this</span></span><br><span class="line">The Zen of Python, by Tim Peters</span><br><span class="line"></span><br><span class="line">Beautiful is better than ugly.</span><br><span class="line">Explicit is better than implicit.</span><br><span class="line">Simple is better than complex.</span><br><span class="line">Complex is better than complicated.</span><br><span class="line">Flat is better than nested.</span><br><span class="line">Sparse is better than dense.</span><br><span class="line">Readability counts.</span><br><span class="line">Special cases aren't special enough to break the rules.</span><br><span class="line">Although practicality beats purity.</span><br><span class="line">Errors should never pass silently.</span><br><span class="line">Unless explicitly silenced.</span><br><span class="line">In the face of ambiguity, refuse the temptation to guess.</span><br><span class="line">There should be one-- and preferably only one --obvious way to do it.</span><br><span class="line">Although that way may not be obvious at first unless you're Dutch.</span><br><span class="line">Now is better than never.</span><br><span class="line">Although never is often better than *right* now.</span><br><span class="line">If the implementation is hard to explain, it's a bad idea.</span><br><span class="line">If the implementation is easy to explain, it may be a good idea.</span><br><span class="line">Namespaces are one honking great idea -- let's do more of those!</span><br></pre></td></tr></table></figure><p>在网上找到关于这段话的翻译，转载自豆瓣<a href="https://www.douban.com/group/topic/3740034/" target="_blank" rel="noopener">The Zen of Python </a>,</p><blockquote><p>Python之禅<br>赖勇浩翻译</p><p>Beautiful is better than ugly.<br>优美胜于丑陋（Python 以编写优美的代码为目标）<br>Explicit is better than implicit.<br>明了胜于晦涩（优美的代码应当是明了的，命名规范，风格相似）<br>Simple is better than complex.<br>简洁胜于复杂（优美的代码应当是简洁的，不要有复杂的内部实现）<br>Complex is better than complicated.<br>复杂胜于凌乱（如果复杂不可避免，那代码间也不能有难懂的关系，要保持接口简洁）<br>Flat is better than nested.<br>扁平胜于嵌套（优美的代码应当是扁平的，不能有太多的嵌套）<br>Sparse is better than dense.<br>间隔胜于紧凑（优美的代码有适当的间隔，不要奢望一行代码解决问题）<br>Readability counts.<br>可读性很重要（优美的代码是可读的）<br>Special cases aren’t special enough to break the rules. Although practicality beats purity.<br>即便假借特例的实用性之名，也不可违背这些规则（这些规则至高无上）<br>Errors should never pass silently. Unless explicitly silenced.<br>不要包容所有错误，除非你确定需要这样做（精准地捕获异常，不写 except:pass 风格的代码）<br>In the face of ambiguity, refuse the temptation to guess.<br>当存在多种可能，不要尝试去猜测<br>There should be one– and preferably only one –obvious way to do it.<br>而是尽量找一种，最好是唯一一种明显的解决方案（如果不确定，就用穷举法）<br>Although that way may not be obvious at first unless you’re Dutch.<br>虽然这并不容易，因为你不是 Python 之父（这里的 Dutch 是指 Guido ）<br>Now is better than never. Although never is often better than <em>right</em> now.<br>做也许好过不做，但不假思索就动手还不如不做（动手之前要细思量）<br>If the implementation is hard to explain, it’s a bad idea. If the implementation is easy to explain, it may be a good idea.<br>如果你无法向人描述你的方案，那肯定不是一个好方案；反之亦然（方案测评标准）<br>Namespaces are one honking great idea – let’s do more of those!<br>命名空间是一种绝妙的理念，我们应当多加利用（倡导与号召）</p></blockquote><p>也有简洁版的如下：</p><blockquote><p>The Zen of Python,<br>蛇宗三字经<br>作者：Tim Peters<br>翻译：元创</p><p>Beautiful is better than ugly.<br>美胜丑<br>Explicit is better than implicit.<br>明胜暗<br>Simple is better than complex.<br>简胜复<br>Complex is better than complicated.<br>复胜杂<br>Flat is better than nested.<br>浅胜深<br>Sparse is better than dense.<br>疏胜密<br>Readability counts.<br>辞达意<br>Special cases aren’t special enough to break the rules.<br>不逾矩<br>Although practicality beats purity.<br>弃至清<br>Errors should never pass silently.<br>无阴差<br>Unless explicitly silenced.<br>有阳错<br>In the face of ambiguity, refuse the temptation to guess.<br>拒疑数<br>There should be one– and preferably only one –obvious way to do it.<br>求完一<br>Although that way may not be obvious at first unless you’re Dutch.<br>虽不至，向往之<br>Now is better than never.<br>敏于行<br>Although never is often better than <em>right</em> now.<br>戒莽撞<br>If the implementation is hard to explain, it’s a bad idea.<br>差难言<br>If the implementation is easy to explain, it may be a good idea.<br>好易说<br>Namespaces are one honking great idea – let’s do more of those!<br>每师出，多有名</p></blockquote><p>在查找的过程中也看到了比较有意思的是上面那段话在python源码<a href="https://github.com/python/cpython/blob/master/Lib/this.py" target="_blank" rel="noopener">this.py</a>中并不是明文存储的，而是使用了凯撒加密法，将每个字符前移13位然后再跟26取余，源码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"""Gur Mra bs Clguba, ol Gvz Crgref</span></span><br><span class="line"><span class="string">Ornhgvshy vf orggre guna htyl.</span></span><br><span class="line"><span class="string">Rkcyvpvg vf orggre guna vzcyvpvg.</span></span><br><span class="line"><span class="string">Fvzcyr vf orggre guna pbzcyrk.</span></span><br><span class="line"><span class="string">Pbzcyrk vf orggre guna pbzcyvpngrq.</span></span><br><span class="line"><span class="string">Syng vf orggre guna arfgrq.</span></span><br><span class="line"><span class="string">Fcnefr vf orggre guna qrafr.</span></span><br><span class="line"><span class="string">Ernqnovyvgl pbhagf.</span></span><br><span class="line"><span class="string">Fcrpvny pnfrf nera'g fcrpvny rabhtu gb oernx gur ehyrf.</span></span><br><span class="line"><span class="string">Nygubhtu cenpgvpnyvgl orngf chevgl.</span></span><br><span class="line"><span class="string">Reebef fubhyq arire cnff fvyragyl.</span></span><br><span class="line"><span class="string">Hayrff rkcyvpvgyl fvyraprq.</span></span><br><span class="line"><span class="string">Va gur snpr bs nzovthvgl, ershfr gur grzcgngvba gb thrff.</span></span><br><span class="line"><span class="string">Gurer fubhyq or bar-- naq cersrenoyl bayl bar --boivbhf jnl gb qb vg.</span></span><br><span class="line"><span class="string">Nygubhtu gung jnl znl abg or boivbhf ng svefg hayrff lbh'er Qhgpu.</span></span><br><span class="line"><span class="string">Abj vf orggre guna arire.</span></span><br><span class="line"><span class="string">Nygubhtu arire vf bsgra orggre guna *evtug* abj.</span></span><br><span class="line"><span class="string">Vs gur vzcyrzragngvba vf uneq gb rkcynva, vg'f n onq vqrn.</span></span><br><span class="line"><span class="string">Vs gur vzcyrzragngvba vf rnfl gb rkcynva, vg znl or n tbbq vqrn.</span></span><br><span class="line"><span class="string">Anzrfcnprf ner bar ubaxvat terng vqrn -- yrg'f qb zber bs gubfr!"""</span></span><br><span class="line"></span><br><span class="line">d = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> (<span class="number">65</span>, <span class="number">97</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">26</span>):</span><br><span class="line">        d[chr(i+c)] = chr((i+<span class="number">13</span>) % <span class="number">26</span> + c)</span><br><span class="line"></span><br><span class="line">print(<span class="string">""</span>.join([d.get(c, c) <span class="keyword">for</span> c <span class="keyword">in</span> s]))</span><br></pre></td></tr></table></figure><p>大致的对应如下：</p><blockquote><p>ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz<br>↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓<br>NOPQRSTUVWXYZABCDEFGHIJKLMnopqrstuvwxyzabcdefghijklm</p></blockquote><p>这里也简单的作为一个学习开始的记录，勉励自己能够坚持学习下去，用python做一些有意思的小项目玩一玩。</p><p>加油↖(^ω^)↗</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img1355839526-2935297530.jpg" alt="加油"></p><p>【参考资料】</p><p>1）<a href="https://foofish.net/python-pep.html" target="_blank" rel="noopener">Python中10个必读的PEP提案</a></p>]]></content>
    
    <summary type="html">
    
      python禅道
    
    </summary>
    
      <category term="python" scheme="https://louielong.github.io/categories/python/"/>
    
    
      <category term="python" scheme="https://louielong.github.io/source/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Openstack虚拟机Resize</title>
    <link href="https://louielong.github.io/openstack-server-resize.html"/>
    <id>https://louielong.github.io/openstack-server-resize.html</id>
    <published>2019-07-05T03:17:48.000Z</published>
    <updated>2019-07-05T06:02:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>最近研究区块链时，突然发现没法连接区块链服务器，一检查发现是区块链程序进程退出了，重启之后仍然退出，再次检查发现是磁盘空间不够了，因为部署在Openstack环境中，因此可以直接使用Openstack的Resize扩展一下磁盘空间，但是点击Resize后并没有改变flavor大小，dashboard上并没有报错，控制节点也看不到错误，进入到计算节点查看才发现有如下报错:</p><blockquote><p>2019-07-05 11:12:12.599 21343 ERROR oslo_messaging.rpc.server   File “/usr/lib/python2.7/contextlib.py”, line 35, in <strong>exit</strong><br>2019-07-05 11:12:12.599 21343 ERROR oslo_messaging.rpc.server     self.gen.throw(type, value, traceback)<br>2019-07-05 11:12:12.599 21343 ERROR oslo_messaging.rpc.server   File “/usr/lib/python2.7/dist-packages/nova/compute/manager.py”, line 7958, in _error_out_instance_on_exception<br>2019-07-05 11:12:12.599 21343 ERROR oslo_messaging.rpc.server     raise error.inner_exception<br>2019-07-05 11:12:12.599 21343 ERROR oslo_messaging.rpc.server ResizeError: Resize error: not able to execute ssh command: Unexpected error while running command.<br>2019-07-05 11:12:12.599 21343 ERROR oslo_messaging.rpc.server Command: ssh -o BatchMode=yes 10.0.0.62 mkdir -p /var/lib/nova/instances/6119b573-1b02-4c72-87c9-14e1fd645449<br>2019-07-05 11:12:12.599 21343 ERROR oslo_messaging.rpc.server Exit code: 255</p></blockquote><p>错误显示是计算节点<em>cmp002</em>无法登录<em>cpm001</em>，因此无法在另外一个节点重建虚机。</p><h2 id="二、处理办法"><a href="#二、处理办法" class="headerlink" title="二、处理办法"></a>二、处理办法</h2><p>通过允许计算节点间nova用户免密登录解决。</p><h3 id="2-1-计算节点间nova用户免密登录"><a href="#2-1-计算节点间nova用户免密登录" class="headerlink" title="2.1 计算节点间nova用户免密登录"></a>2.1 计算节点间nova用户免密登录</h3><p>对nova用户进行配置[1]。</p><h3 id="2-1-1-取消禁止登录权限"><a href="#2-1-1-取消禁止登录权限" class="headerlink" title="2.1.1 取消禁止登录权限"></a>2.1.1 取消禁止登录权限</h3><p>使用<strong><em>root</em></strong>用户在<strong>所有计算节点</strong>上执行以下操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> usermod -s /bin/bash nova</span></span><br></pre></td></tr></table></figure><p>检查修改，确保nova用户具备shell执行权限：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> cat /etc/passwd | grep nova</span></span><br><span class="line">nova:x:64060:64060::/var/lib/nova:/bin/bash</span><br></pre></td></tr></table></figure><h3 id="2-1-2-配置SSH"><a href="#2-1-2-配置SSH" class="headerlink" title="2.1.2 配置SSH"></a>2.1.2 配置SSH</h3><p>在<strong>任一个计算节点</strong>上，用nova用户登录，创建密钥</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> su - nova</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ssh-keygen -t rsa</span></span><br></pre></td></tr></table></figure><p>在该节点上，配置nova用户的SSH配置，不执行主机密钥验证，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat &lt;&lt; EOF &gt; ~/.ssh/config</span></span><br><span class="line">Host *</span><br><span class="line">    StrictHostKeyChecking no</span><br><span class="line">    UserKnownHostsFile=/dev/null</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>拷贝id_rsa.pub为authorized_keys，并修改权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat ~/.ssh/id_rsa.pub &gt; .ssh/authorized_keys</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> chmod 600 .ssh/authorized_keys</span></span><br></pre></td></tr></table></figure><h3 id="2-1-3-将公钥拷贝至其他所有计算节点"><a href="#2-1-3-将公钥拷贝至其他所有计算节点" class="headerlink" title="2.1.3 将公钥拷贝至其他所有计算节点"></a>2.1.3 将公钥拷贝至其他所有计算节点</h3><p>1）直接拷贝公钥</p><p>将计算节点的nova根目录下<code>/var/lib/nova/.ssh/id_rsa.pub</code>内容拷贝其他所有计算节点的<code>/var/lib/nova/.ssh/authorized_keys</code>即可。</p><p>2）使用命令上传</p><p><em>若计算节点的nova用户设置的有密码也可直接用命令上传</em>，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova@cmp002:~$ ssh-copy-id -i ~/.ssh/id_rsa.pub nova@cmp001</span><br></pre></td></tr></table></figure><p>在任意节点验证，到其他所有计算节点可否SSH登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> su - nova</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ssh nova@cmp001</span></span><br></pre></td></tr></table></figure><p>重试resize，即可完成，发现resize完成后，虚拟机迁移到目标计算节点上。</p><h3 id="【备注】"><a href="#【备注】" class="headerlink" title="【备注】"></a><strong>【备注】</strong></h3><p>以上的操作需要配置计算点间无密码访问，但是那么多计算节点相互访问配置较为麻烦，有提到通过修改nova配置文件，允许在同一台服务器上进行resize和migrate操作配置项，如下[2]：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/nova/nova.conf</span></span><br><span class="line"></span><br><span class="line">allow_resize_to_same_host=true</span><br><span class="line">allow_migrate_to_same_host=true</span><br></pre></td></tr></table></figure><p>然后重启nova-api和nova-compute服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl restart nova-compute</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> systemctl restart nova-api</span></span><br></pre></td></tr></table></figure><p>但在计算节点配置后，重启nova-compute服务，重试resize没有效果。<strong>后来才发现，这仅适用于计算节点只有一台的测试环境，多计算节点的生产环境并不适用</strong>，仍需要ssh到其他计算节点。</p><h2 id="【参考文献】"><a href="#【参考文献】" class="headerlink" title="【参考文献】"></a>【参考文献】</h2><p>1）<a href="https://blog.csdn.net/Tech_Salon/article/details/75124135" target="_blank" rel="noopener">nova对instance做resize操作失败</a></p><p>2） <a href="http://zjzone.cc/index.php/2017/05/06/openstack-xu-ni-ji-resize-shi-bai-wen-ti-jie-jue-fang-fa/" target="_blank" rel="noopener">Openstack虚拟机Resize失败问题解决方法</a></p>]]></content>
    
    <summary type="html">
    
      Openstack 虚拟机 Resize
    
    </summary>
    
      <category term="Openstack" scheme="https://louielong.github.io/categories/Openstack/"/>
    
    
      <category term="Openstack" scheme="https://louielong.github.io/source/tags/Openstack/"/>
    
  </entry>
  
  <entry>
    <title>OPNFV Hunter Fuel安装以及OVP测试使用</title>
    <link href="https://louielong.github.io/OPNFV_Hunter_install_OVP_Test.html"/>
    <id>https://louielong.github.io/OPNFV_Hunter_install_OVP_Test.html</id>
    <published>2019-05-27T05:49:55.000Z</published>
    <updated>2019-06-03T07:08:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>OPNFV与5月发布了第八个版本Hunter，随即升级了实验室的环境，同时也测试一下即将发布的OVP 第三个版本。官方社区的安装工具也减少到只剩下<strong>MCP (Fuel)</strong>和<strong>TripleO</strong>两个了，Fuel的安装方式没有太多的改变，只是引入了<strong>Docker</strong>将原来的<em>fuel</em>和<em>MAAS</em>两个安装虚拟机换成了Docker容器。实际在部署过程中发现与先前的<a href="opnfv_Euphrates_install-02.html">OPNFV Euphrates部署 02</a>和<a href="opnfv_Euphrates_install-02.html">OPNFV Euphrates部署 03</a>没有区别，在配置好<strong>网络</strong>、<strong>PDF</strong>以及<strong>IDF</strong>后可以很顺利的安装。同时看到Fuel部署的文档写的更加详细了，参见：</p><ul><li><a href="https://docs.opnfv.org/en/stable-fraser/submodules/fuel/docs/release/installation/installation.instruction.html" target="_blank" rel="noopener">OPNFV Hunter Fuel安装指导</a></li><li><a href="https://opnfv-fuel.readthedocs.io/en/stable-hunter/release/userguide/userguide.html" target="_blank" rel="noopener">OPNFV Fuel 用户手册</a></li></ul><p>OVP也将发布第三个版本OVP-2019.08增强测试内容，因此这里一并整理一下使用与问题排查办法。</p><h2 id="二、Hunter-部署"><a href="#二、Hunter-部署" class="headerlink" title="二、Hunter 部署"></a>二、Hunter 部署</h2><p>Fuel的Hunter部署和Euphrates以及Gambia没有太大的差别，完全可以参考先前的<a href="opnfv_Euphrates_install-02.html">OPNFV Euphrates部署 02</a>和<a href="opnfv_Euphrates_install-02.html">OPNFV Euphrates部署 03</a>，这里主要介绍一下加快部署的办法。</p><h2 id="2-1-MAAS-local-mirros"><a href="#2-1-MAAS-local-mirros" class="headerlink" title="2.1 MAAS local mirros"></a>2.1 MAAS local mirros</h2><p>部署过程中遇到的最大的问题就是MAAS每次都会去重新获取最新的ubuntu镜像，常常因为镜像下载缓慢导致部署超时，这里可以参看<a href="./MAAS+ubuntu-local-repo.html">MAAS本地源设置</a>，配置本地MAAS源以加快部署</p><p>修改<code>mcp/reclass/classes/cluster/all-mcp-arch-common/infra/maas.yml.j2</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">boot_sources:</span></span><br><span class="line"><span class="attr">  resources_mirror:</span></span><br><span class="line">    <span class="comment">#url: http://images.maas.io/ephemeral-v3/daily</span></span><br><span class="line"><span class="attr">    url:</span> <span class="attr">http://&lt;IMG</span> <span class="string">mirror</span> <span class="string">IP&gt;/maas/images/ephemeral-v3/daily</span></span><br><span class="line"><span class="attr">    keyring_file:</span> <span class="string">/usr/share/keyrings/ubuntu-cloudimage-keyring.gpg</span></span><br><span class="line"><span class="attr">boot_sources_selections:</span></span><br><span class="line"><span class="attr">  xenial:</span></span><br><span class="line">    <span class="comment">#url: "http://images.maas.io/ephemeral-v3/daily"</span></span><br><span class="line"><span class="attr">    url:</span> <span class="string">"http://&lt;IMG mirror IP&gt;/maas/images/ephemeral-v3/daily"</span></span><br></pre></td></tr></table></figure><h2 id="2-2-dashboard不可用"><a href="#2-2-dashboard不可用" class="headerlink" title="2.2  dashboard不可用"></a>2.2  dashboard不可用</h2><p>本次安装在解决完MAAS本地源后安装十分顺利，但是安装完后无法使用dashboard，给社区提了一个jira <a href="https://jira.opnfv.org/browse/FUEL-408" target="_blank" rel="noopener">Fuel-408</a>看后续修复吧，尝试在本地解决了一些问题，使得访问<strong>prx01/prx02</strong>的的8078端口可以看到正常的页面，但是使用密码无法登录。</p><h2 id="【Note】"><a href="#【Note】" class="headerlink" title="【Note】"></a>【Note】</h2><p>这里记录一下在无dashboard的情况下配置并测试openstack环境</p><h3 id="1-上传镜像"><a href="#1-上传镜像" class="headerlink" title="1)上传镜像"></a>1)上传镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack image create cirros --file cirros-0.3.5.qcow2 --container-format bare --public</span><br></pre></td></tr></table></figure><h3 id="2-创建external网络"><a href="#2-创建external网络" class="headerlink" title="2)创建external网络"></a>2)创建external网络</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack subnet create --network floating_net --allocation-pool start=192.168.20.100,end=192.168.20.200  --dns-nameserver 10.2.2.20 --gateway 192.168.20.1 --subnet-range 192.168.20.0/1 floating_subnet</span><br></pre></td></tr></table></figure><h3 id="3-创建内部虚拟网络"><a href="#3-创建内部虚拟网络" class="headerlink" title="3)创建内部虚拟网络"></a>3)创建内部虚拟网络</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openstack network create test</span><br><span class="line">openstack subnet create worker_subnet --network worker_network --subnet-range 10.20.30.0/24</span><br></pre></td></tr></table></figure><h3 id="4-创建路由"><a href="#4-创建路由" class="headerlink" title="4)创建路由"></a>4)创建路由</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">openstack floating ip create floating_net</span><br><span class="line">openstack router create router</span><br><span class="line">openstack router set router --external-gateway floating_net</span><br><span class="line">openstack router add subnet router test_subnet</span><br></pre></td></tr></table></figure><h3 id="5-安全组配置"><a href="#5-安全组配置" class="headerlink" title="5)安全组配置"></a>5)安全组配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openstack security group rule create &lt;default group id&gt; --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0</span><br><span class="line">openstack security group rule create --protocol icmp &lt;default group id&gt;</span><br></pre></td></tr></table></figure><h3 id="6-创建flavor"><a href="#6-创建flavor" class="headerlink" title="6)创建flavor"></a>6)创建flavor</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openstack flavor create m1.tiny --ram 64 --disk 0 --vcpus 1 --public</span><br><span class="line">nova flavor-key &lt;ID&gt; set hw:mem_page_size=large  # 由于使用的dpdk需要配置大页内存</span><br></pre></td></tr></table></figure><h3 id="7-启动虚拟机"><a href="#7-启动虚拟机" class="headerlink" title="7)启动虚拟机"></a>7)启动虚拟机</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova boot --flavor m1.tiny --image cirros --nic net-id=&lt;floating_net id&gt; test</span><br></pre></td></tr></table></figure><h3 id="8-分配浮动IP"><a href="#8-分配浮动IP" class="headerlink" title="8)分配浮动IP"></a>8)分配浮动IP</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openstack floating ip create floating_net</span><br><span class="line">openstack floating ip set --port &lt;port&gt; &lt;flaoting ip id&gt;</span><br></pre></td></tr></table></figure><h2 id="三、OVP测试使用"><a href="#三、OVP测试使用" class="headerlink" title="三、OVP测试使用"></a>三、OVP测试使用</h2><p>OVP测试使用参考：<a href="https://opnfv-dovetail.readthedocs.io/en/latest/testing/user/userguide/testing_guide.html" target="_blank" rel="noopener">OVP用户手册</a></p><p>选择一台机器，安装配置好docker，并确保机器可以访问OPNFV环境的keystone API</p><h3 id="3-1-测试环境配置"><a href="#3-1-测试环境配置" class="headerlink" title="3.1 测试环境配置"></a>3.1 测试环境配置</h3><h4 id="1）设置dovetial-home目录"><a href="#1）设置dovetial-home目录" class="headerlink" title="1）设置dovetial home目录"></a>1）设置dovetial home目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir -p $&#123;HOME&#125;/dovetail</span><br><span class="line"><span class="meta">$</span> export DOVETAIL_HOME=$&#123;HOME&#125;/dovetail</span><br></pre></td></tr></table></figure><h4 id="2-配置文件设置"><a href="#2-配置文件设置" class="headerlink" title="2)配置文件设置"></a>2)配置文件设置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> mkdir -p $&#123;DOVETAIL_HOME&#125;/pre_config</span><br><span class="line"><span class="meta">$</span> mkdir -p $&#123;DOVETAIL_HOME&#125;/images</span><br></pre></td></tr></table></figure><p>填写配置文件内容</p><p>该文件的内容可以从控制节点<strong>ctl0x</strong>的<code>/root/keystonercv3</code>获取。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cat $&#123;DOVETAIL_HOME&#125;/pre_config/env_config.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Project-level authentication scope (name or ID), recommend admin project.</span><br><span class="line">export OS_PROJECT_NAME=admin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> For identity v2, it uses OS_TENANT_NAME rather than OS_PROJECT_NAME.</span><br><span class="line">export OS_TENANT_NAME=admin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Authentication username, belongs to the project above, recommend admin user.</span><br><span class="line">export OS_USERNAME=admin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Authentication password. Use your own password</span><br><span class="line">export OS_PASSWORD=opnfv_secret</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Authentication URL, one of the endpoints of keystone service. If this is v3 version,</span><br><span class="line"><span class="meta">#</span> there need some extra variables as follows.</span><br><span class="line">export OS_AUTH_URL=http://xxxxxxxxxxxx:35357/v3</span><br><span class="line"></span><br><span class="line">export OS_IDENTITY_API_VERSION=3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Domain name or ID containing the user above.</span><br><span class="line"><span class="meta">#</span> Command to check the domain: openstack user show &lt;OS_USERNAME&gt;</span><br><span class="line">export OS_USER_DOMAIN_NAME=Default</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Domain name or ID containing the project above.</span><br><span class="line"><span class="meta">#</span> Command to check the domain: openstack project show &lt;OS_PROJECT_NAME&gt;</span><br><span class="line">export OS_PROJECT_DOMAIN_NAME=Default</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Special environment parameters for https.</span><br><span class="line"><span class="meta">#</span> If using https + cacert, the path of cacert file should be provided.</span><br><span class="line"><span class="meta">#</span> The cacert file should be put at $DOVETAIL_HOME/pre_config.</span><br><span class="line">export OS_CACERT=/path_to/pre_config/os_cacert</span><br><span class="line">export OS_INSECURE=false</span><br><span class="line"></span><br><span class="line">export OS_REGION_NAME=RegionOne</span><br><span class="line">export OS_INTERFACE=internal</span><br><span class="line">export OS_ENDPOINT_TYPE="internal"</span><br><span class="line">export INSTALLER_TYPE=fuel</span><br><span class="line">export EXTERNAL_NETWORK="floating_net"</span><br><span class="line"><span class="meta">#</span>export VOLUME_DEVICE_NAME=vdc</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> Set an existing role used to create project and user for vping test cases.</span><br><span class="line"><span class="meta">#</span> Otherwise, it will create a role 'Member' to do that.</span><br><span class="line">export NEW_USER_ROLE=xxx</span><br></pre></td></tr></table></figure><p>配置<code>Tempest</code>所需的配置文件，编辑文件<code>$DOVETAIL_HOME/pre_config/tempest_conf.yaml</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">compute:</span><br><span class="line"><span class="meta">  #</span> The minimum number of compute nodes expected.</span><br><span class="line"><span class="meta">  #</span> This should be no less than 2 and no larger than the compute nodes the SUT actually has.</span><br><span class="line">  min_compute_nodes: 2</span><br><span class="line"></span><br><span class="line"><span class="meta">  #</span> Expected device name when a volume is attached to an instance.</span><br><span class="line">  volume_device_name: vdb</span><br></pre></td></tr></table></figure><p>节点描述信息，dovetail在测试时会获取节点信息，同时在HA测试部分会重启节点也需要用到节点信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">nodes:</span><br><span class="line">-</span><br><span class="line">    # This can not be changed and must be node0.</span><br><span class="line">    name: node0</span><br><span class="line">    role: Jumpserver</span><br><span class="line">    ip: xx.xx.xx.xx</span><br><span class="line">    user: root</span><br><span class="line">    password: root</span><br><span class="line"></span><br><span class="line">-</span><br><span class="line">    # This can not be changed and must be node1.</span><br><span class="line">    name: node1</span><br><span class="line">    # This must be controller.</span><br><span class="line">    role: Controller</span><br><span class="line">    # This is the instance IP of a controller node, which is the haproxy primary node</span><br><span class="line">    ip: xx.xx.xx.xx</span><br><span class="line">    # User name of the user of this node. This user **must** have sudo privileges.</span><br><span class="line">    user: root</span><br><span class="line">    key_filename: /path_to/pre_config/id_rsa</span><br><span class="line"></span><br><span class="line">process_info:</span><br><span class="line">-</span><br><span class="line">    # The default attack process of yardstick.ha.rabbitmq is 'rabbitmq-server'.</span><br><span class="line">    # Here can be reset to 'rabbitmq'.</span><br><span class="line">    testcase_name: yardstick.ha.rabbitmq</span><br><span class="line">    attack_process: rabbitmq</span><br><span class="line"></span><br><span class="line">-</span><br><span class="line">    # The default attack host for all HA test cases is 'node1'.</span><br><span class="line">    # Here can be reset to any other node given in the section 'nodes'.</span><br><span class="line">    testcase_name: yardstick.ha.glance_api</span><br><span class="line">    attack_host: node2</span><br></pre></td></tr></table></figure><p><em>process_info</em>表明在测试相应HA用例时yardstick重启哪些<strong>进程</strong>或<strong>节点</strong></p><p>这里给出相应测试用例攻击的进程名</p><table><thead><tr><th>Test Case Name</th><th>Attack Process Name</th></tr></thead><tbody><tr><td>yardstick.ha.cinder_api</td><td>cinder-api</td></tr><tr><td>yardstick.ha.database</td><td>mysql</td></tr><tr><td>yardstick.ha.glance_api</td><td>glance-api</td></tr><tr><td>yardstick.ha.haproxy</td><td>haproxy</td></tr><tr><td>yardstick.ha.keystone</td><td>keystone</td></tr><tr><td>yardstick.ha.neutron_l3_agent</td><td>neutron-l3-agent</td></tr><tr><td>yardstick.ha.neutron_server</td><td>neutron-server</td></tr><tr><td>yardstick.ha.nova_api</td><td>nova-api</td></tr><tr><td>yardstick.ha.rabbitmq</td><td>rabbitmq-server</td></tr></tbody></table><h3 id="3-2-镜像下载"><a href="#3-2-镜像下载" class="headerlink" title="3.2 镜像下载"></a>3.2 镜像下载</h3><p>这里仅以OVP-2.2为例</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> wget -nc http://download.cirros-cloud.net/0.4.0/cirros-0.4.0-x86_64-disk.img -P $&#123;DOVETAIL_HOME&#125;/images</span><br><span class="line"><span class="meta">$</span> wget -nc https://cloud-images.ubuntu.com/releases/14.04/release/ubuntu-14.04-server-cloudimg-amd64-disk1.img -P $&#123;DOVETAIL_HOME&#125;/images</span><br><span class="line"><span class="meta">$</span> wget -nc https://cloud-images.ubuntu.com/releases/16.04/release/ubuntu-16.04-server-cloudimg-amd64-disk1.img -P $&#123;DOVETAIL_HOME&#125;/images</span><br><span class="line"><span class="meta">$</span> wget -nc http://repository.cloudifysource.org/cloudify/4.0.1/sp-release/cloudify-manager-premium-4.0.1.qcow2 -P $&#123;DOVETAIL_HOME&#125;/images  </span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> sudo docker pull opnfv/dovetail:ovp-2.2.0</span><br><span class="line">ovp-2.2.0: Pulling from opnfv/dovetail</span><br><span class="line">....</span><br><span class="line">Digest: sha256:7449601108ebc5c40f76a5cd9065ca5e18053be643a0eeac778f537719336c29</span><br><span class="line">Status: Downloaded newer image for opnfv/dovetail:ovp-2.2.0</span><br></pre></td></tr></table></figure><h3 id="3-3-启动dovetail容器"><a href="#3-3-启动dovetail容器" class="headerlink" title="3.3 启动dovetail容器"></a>3.3 启动dovetail容器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo docker run --privileged=true -it \</span><br><span class="line">          -e DOVETAIL_HOME=$DOVETAIL_HOME \</span><br><span class="line">          -v $DOVETAIL_HOME:$DOVETAIL_HOME \</span><br><span class="line">          -v /var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">          --name dovetail \</span><br><span class="line">          opnfv/dovetail:&lt;tag&gt; /bin/bash</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>-e</code>指定DOVETAIL_HOME变量</li><li><code>-v</code> 映射DOVETAIL_HOME目录</li></ul><h3 id="3-3-启动测试"><a href="#3-3-启动测试" class="headerlink" title="3.3 启动测试"></a>3.3 启动测试</h3><p>进入容器查看测试用例</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> docker exec -it dovetail bash</span><br><span class="line">root@f02275952f91:~# dovetail list</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><p>执行测试用例</p><p>简单测试</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> dovetail run --offline --debug --testcase  functest.vping.userdata --deploy-scenario os-nosdn-ovs-ha --report</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><strong>offline</strong>指明为离线测试</li><li><strong>debug</strong>打开debug选项</li><li><strong>testcase</strong>指明单项测试用例名</li><li><strong>deploy-scenario</strong>指明所使用的部署策略，主要影响在于ovs策略中使用了DPDK则会在flavor中附加大页内存配置</li><li><strong>report</strong>指明生成测试报告</li></ul><p>完整测试</p><p>OVP认证测试内容分为<code>mandatory</code>和<code>optional</code>，可以单独执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> dovetail run --mandatory --report</span><br></pre></td></tr></table></figure><h3 id="3-4-测试结果"><a href="#3-4-测试结果" class="headerlink" title="3.4 测试结果"></a>3.4 测试结果</h3><p>执行完测试后在<code>$DOVETAIL_HOME/results</code>可以获得完整的测试记录，执行不通过的测试用例也可在相应的目录获得日志文件</p><ul><li>Log file: dovetail.log<ul><li>Review the dovetail.log to see if all important information has been captured - in default mode without DEBUG.</li><li>Review the results.json to see all results data including criteria for PASS or FAIL.</li></ul></li><li>Tempest and security test cases<ul><li>Can see the log details in <code>tempest_logs/functest.tempest.XXX.html</code> and <code>security_logs/functest.security.XXX.html</code> respectively, which has the passed, skipped and failed test cases results.</li><li>This kind of files need to be opened with a web browser.</li><li>The skipped test cases are accompanied with the reason tag for the users to see why these test cases skipped.</li><li>The failed test cases have rich debug information for the users to see why these test cases failed.</li></ul></li><li>Vping test cases<ul><li>Its log is stored in <code>vping_logs/functest.vping.XXX.log</code>.</li></ul></li><li>HA test cases<ul><li>Its log is stored in <code>ha_logs/yardstick.ha.XXX.log</code>.</li></ul></li><li>Stress test cases<ul><li>Its log is stored in <code>stress_logs/bottlenecks.stress.XXX.log</code>.</li></ul></li><li>Snaps test cases<ul><li>Its log is stored in <code>snaps_logs/functest.snaps.smoke.log</code>.</li></ul></li><li>VNF test cases<ul><li>Its log is stored in <code>vnf_logs/functest.vnf.XXX.log</code>.</li></ul></li></ul><h3 id="3-5-测试失败排查"><a href="#3-5-测试失败排查" class="headerlink" title="3.5 测试失败排查"></a>3.5 测试失败排查</h3><h4 id="1-无法获取节点信息"><a href="#1-无法获取节点信息" class="headerlink" title="1)无法获取节点信息"></a>1)无法获取节点信息</h4><p>节点信息获取失败并不影响测试执行，可以在dovetail容器中执行以下命令查找失败原因</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ansible all -m setup -i XX/results/inventory.ini --tree XX/results/sut_hardware_info</span><br></pre></td></tr></table></figure><h4 id="2-dpdk错误相关"><a href="#2-dpdk错误相关" class="headerlink" title="2)dpdk错误相关"></a>2)dpdk错误相关</h4><p>我的测试环境是<code>os-nosdn-ovs-ha</code>使用了DPDK网卡，有时由于变量传递的原因导致测试过程中，虚机的flavor没有设置<code>&#39;hw:mem_page_size&#39;:&#39;large&#39;</code>，因此导致虚拟创建后无法连接而报错，可以在测试过程中查看<code>openstack flavor list --debug</code>查看是否是此问题，如果是此类问题只能报告官方修复了。</p><h4 id="3-其他测试用例失败"><a href="#3-其他测试用例失败" class="headerlink" title="3) 其他测试用例失败"></a>3) 其他测试用例失败</h4><p>通过dovetail的不清除选项保留测试过程中启动的容器，然后一步一步排查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> dovetail run --offline --debug --testcase xxx -n</span><br></pre></td></tr></table></figure><p>随后进入调用的容器，执行dovetail测试过程中调用的命令，如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">container.Container - DEBUG - Executing command: 'sudo docker run -id --privileged=true  -e INSTALLER_TYPE=unknown -e DEPLOY_SCENARIO=os-nosdn-ovs-ha -e NODE_NAME=unknown -e TEST_DB_URL=file:///home/opnfv/functest/results/functest_results.txt -e CI_DEBUG=true -e BUILD_TAG=daily-master-7c89eac6-829e-11e9-85e8-0242ac140002-functest.tempest.vm_lifecycle       -v /nfs/NFV_TEST/dovetail/data/pre_config/env_config.sh:/home/opnfv/functest/conf/env_file   -v /nfs/NFV_TEST/dovetail/data/pre_config/os_cacert:/nfs/NFV_TEST/dovetail/data/pre_config/os_cacert   -v /nfs/NFV_TEST/dovetail/data:/home/opnfv/userconfig   -v /nfs/NFV_TEST/dovetail/data/results:/home/opnfv/functest/results  -v /nfs/NFV_TEST/dovetail/data/images:/home/opnfv/functest/images opnfv/functest-smoke:opnfv-7.1.0 /bin/bash'</span><br><span class="line">...........</span><br><span class="line">container.Container - DEBUG - Executing command: 'sudo docker exec 75cd8d85a13a /bin/bash -c "run_tests -t tempest_custom -r"'</span><br></pre></td></tr></table></figure><p>可以进入到<code>75cd8d85a13a</code>容器中执行<code>run_tests -t tempest_custom -r</code>进一步排查，</p><p>对于functest，其代码目录位于容器中的<code>/usr/lib/python2.7/site-packages/functest</code>，可以修改<code>/usr/lib/python2.7/site-packages/functest/ci/logging.ini</code>中的配置来打开相应组件的日志记录，进行更深层次的排查。</p>]]></content>
    
    <summary type="html">
    
      OPNFV Hunter版本安装以及OVP测试使用
    
    </summary>
    
      <category term="OPNFV" scheme="https://louielong.github.io/categories/OPNFV/"/>
    
    
      <category term="Dovetail" scheme="https://louielong.github.io/source/tags/Dovetail/"/>
    
      <category term="OPNFV" scheme="https://louielong.github.io/source/tags/OPNFV/"/>
    
      <category term="OVP" scheme="https://louielong.github.io/source/tags/OVP/"/>
    
      <category term="Fuel" scheme="https://louielong.github.io/source/tags/Fuel/"/>
    
  </entry>
  
  <entry>
    <title>Openstack虚拟机磁盘创建超时</title>
    <link href="https://louielong.github.io/image-volume-create-timeout.html"/>
    <id>https://louielong.github.io/image-volume-create-timeout.html</id>
    <published>2019-05-09T07:39:03.000Z</published>
    <updated>2019-07-05T06:05:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>最近在创建虚拟机时VM创建非常的慢并且出现创建失败的情况，计算节点报错显示块设备映射超时，在尝试了61次207秒后失败，此时查看Openstack的卷设备创建，发现volume显示是<em>downloading</em>状态，即未创建完成。</p><blockquote><p>ERROR nova.compute.manager [req-94c955d9-9c94-4536-90e2-d21928344444 381fdf4b65aa45ef98a2ad20bc4bd079 4353036cc27542cf84e1bccf1b4bfe33 - default default] [    instance: 655eccd6-cafb-4291-b55a-c53dd000a8e4] Build of instance 655eccd6-cafb-4291-b55a-c53dd000a8e4 aborted: Volume 0e4150db-567f-4ae0-a947-8fc7a0d624f0 did not finish being created    even after we waited 207 seconds or 61 attempts. And its status is downloading.: BuildAbortException: Build of instance 655eccd6-cafb-4291-b55a-c53dd000a8e4 aborted: Volume 0e4150db-567    f-4ae0-a947-8fc7a0d624f0 did not finish being created even after we waited 207 seconds or 61 attempts. And its status is downloading.</p></blockquote><p>该错误原因是Openstack卷创建时间过长与所需卷存储设置大小有关。</p><h2 id="二、解决办法"><a href="#二、解决办法" class="headerlink" title="二、解决办法"></a>二、解决办法</h2><h3 id="方案一：增大尝试次数"><a href="#方案一：增大尝试次数" class="headerlink" title="方案一：增大尝试次数"></a>方案一：增大尝试次数</h3><p>根据[1]增大<code>block_device_allocate_retries</code>参数即可，修改<strong>计算节点</strong>的<code>/etc/nova/nova.conf</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">block_device_allocate_retries</span> <span class="string">=</span> <span class="number">180</span></span><br></pre></td></tr></table></figure><p>重启nova服务即可。</p><h3 id="方案二：使用Image-Volume-cache"><a href="#方案二：使用Image-Volume-cache" class="headerlink" title="方案二：使用Image-Volume cache"></a>方案二：使用<em>Image-Volume cache</em></h3><p>在修改<code>block_device_allocate_retries</code>时意外发现注释中有一段标注</p><blockquote><p>Number of times to retry block device allocation on failures. Starting with<br>Liberty, Cinder can use image volume cache. This may help with block device<br>allocation performance. Look at the cinder image_volume_cache_enabled<br>configuration option.</p></blockquote><p>从L版本Openstack加入了镜像券存储缓存的功能，通过这个功能可以快速创建镜像卷存储。根据文档[2][3]修改<strong>块存储节点</strong>的<code>/etc/cinder/cinder.conf</code>，加入以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">...</span><br><span class="line">cinder_internal_tenant_project_id = 4353036cc27542cf84e1bccf1b4bfe33</span><br><span class="line">cinder_internal_tenant_user_id = 381fdf4b65aa45ef98a2ad20bc4bd079</span><br><span class="line"></span><br><span class="line">[lvm]</span><br><span class="line">...</span><br><span class="line">image_volume_cache_max_size_gb = 200</span><br><span class="line">image_volume_cache_max_count = 50</span><br><span class="line">image_volume_cache_enabled = True</span><br></pre></td></tr></table></figure><p><strong>其中</strong>：</p><p><em>cinder_internal_tenant_project_id</em>和<em>cinder_internal_tenant_user_id</em>是为了配置块存储的内部租户。这个租户拥有这些缓存并且可以进行管理。这可以保护用户不必看到这些缓存，但是也没有全局隐藏。<code>cinder_internal_tenant_user_id</code>选择cinder用户可以保证admin用户看不到缓存镜像，这里设置为admin用户是便于查看修改效果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@ctl01:~$ openstack project list</span><br><span class="line">+----------------------------------+----------+</span><br><span class="line">| ID                               | Name     |</span><br><span class="line">+----------------------------------+----------+</span><br><span class="line">| 4353036cc27542cf84e1bccf1b4bfe33 | admin    |</span><br><span class="line"></span><br><span class="line">xdnsadmin@ctl01:~$ openstack user list</span><br><span class="line">+----------------------------------+----------+</span><br><span class="line">| ID                               | Name     |</span><br><span class="line">+----------------------------------+----------+</span><br><span class="line">| 381fdf4b65aa45ef98a2ad20bc4bd079 | admin    |</span><br><span class="line">| 6a4d2c1e59364bacadb8e72120b49601 | cinder   |</span><br></pre></td></tr></table></figure><p>如图所示，<code>image-88e73002-a4e6-4aff-88cf-2711aa7f24c8</code>为ubuntu-16.04镜像的缓存文件，当需要创建基于该镜像的新volume时可以快速clone，创建速度大大提升。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/20190509160626.png" alt="image-volume cache"></p><p>【参考链接】</p><p>1）<a href="https://www.topomel.com/archives/720.html" target="_blank" rel="noopener">配置block_device_allocate_retries解决卷创建超时的问题</a></p><p>2）<a href="https://docs.openstack.org/cinder/latest/admin/blockstorage-image-volume-cache.html" target="_blank" rel="noopener">Image-Volume cache</a></p><p>3）<a href="https://www.jianshu.com/p/759cb7efb844" target="_blank" rel="noopener">image-volume cache翻译</a></p>]]></content>
    
    <summary type="html">
    
      VM磁盘连接超时
    
    </summary>
    
      <category term="Openstack" scheme="https://louielong.github.io/categories/Openstack/"/>
    
    
      <category term="Openstack" scheme="https://louielong.github.io/source/tags/Openstack/"/>
    
      <category term="cinder" scheme="https://louielong.github.io/source/tags/cinder/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack Magnum安装(Rocky)</title>
    <link href="https://louielong.github.io/openstack-magnum-install.html"/>
    <id>https://louielong.github.io/openstack-magnum-install.html</id>
    <published>2019-04-12T09:44:10.000Z</published>
    <updated>2019-06-03T07:19:48.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>最近在研究OpenStack和Kubernetes融合相关的方法，通过查阅[1]发现openstack与k8s融合无外乎两种方法：一是K8S部署在OpenStack平台之上，二是K8S和OpenStack组件集成。方案一的优点是k8s容器借用虚机的多租户具有很好的隔离性，但缺点也很明显K8S使用的是在虚机网络之上又嵌套的一层overlay网络，k8s的网络将难以精细化的管控。方案二将k8s与openstack的部分组件进行集成，是的K8S与openstack具备很好的融合性，从而实现1+1&gt;2的效果。openstack中的magnum是一个典型的解决方案，通过magnum可以镜像容器编排，同时可以快速部署一个Swarm、K8S、Mesos集群。</p><p>本文仅介绍如何在openstack中安装部署magnum以及安装中遇到的问题。</p><p>此外，<strong>我还想吐槽一句</strong>，<code>OpenStack</code>社区从<code>Pike</code>版本开始弃用了<code>auth_uri</code>以及keystone的<code>35357</code>端口全部改为公开的<code>5000</code>端口和使用<code>www_authenticate_uri</code>来进行keystone认证，社区的核心组件已经更换完成，但是非核心组件并未完全跟进，如<code>Magnum-api</code>、<code>Heat</code>安装时就出现过这类问题。</p><h2 id="一、Magnum安装准备"><a href="#一、Magnum安装准备" class="headerlink" title="一、Magnum安装准备"></a>一、Magnum安装准备</h2><p>根据官方组件说明[2]，必须的组件有：</p><ul><li>Keystone-Identity service</li><li>Glance-Image service</li><li>Nova-Compute service</li><li>Neutron-Networking service</li><li>Heat-Orchestration service。</li></ul><p>可选组件有：</p><ul><li>Cinder-Block Storage service</li><li>Octavia-<a href="http://docs.openstack.org/networking-guide/config-lbaas.html" target="_blank" rel="noopener">Load Balancer as a Service (LBaaS v2)</a></li><li>Barbican-<a href="http://docs.openstack.org/project-install-guide/key-manager/draft/" target="_blank" rel="noopener">Key Manager service</a><br>以及</li><li>Swift-<a href="http://docs.openstack.org/project-install-guide/object-storage/draft/" target="_blank" rel="noopener">Object Storage service</a></li><li>Ironic-<a href="http://docs.openstack.org/project-install-guide/baremetal/draft/" target="_blank" rel="noopener">Bare Metal service</a></li></ul><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgMagnum_depends.png" alt="Magnum Depends on"></p><p><strong>本次安装使用的Rocky版本，Neutron网络架构采用的是ovs+vxlan的方式</strong></p><h2 id="二、安装Magnum"><a href="#二、安装Magnum" class="headerlink" title="二、安装Magnum"></a>二、安装Magnum</h2><p>我的openstack环境采用纯手工的方式按照官方指导文档进行安装，同样Magnum也是Manual的方式安装，系统版本为<code>Ubuntu-18.04 LTS</code>，因为Rocky版本的<code>deb</code>包只有18.04才有。Magnum-api 版本为<code>7.0.1-0ubuntu1~cloud0</code>。</p><p>按照指导手册[3]，进行安装，安装和配置方式极其简单，<strong>但是</strong>官方的配置文档没有来的及更新，以及deb包更新不及时遇到了很多奇奇怪怪的bug，在查找原因时也很难确定关键词而无法搜索到正确的答案。</p><p>按照官方文档在安装完Magnum后通过<code>openstack coe service list</code>来查看是否安装成功，如果该命令执行失败通过查看<code>/var/log/magnum/magnum-api.log</code>来排查问题。<strong>但是</strong>，该命令执行成功也不代表这Magnum的服务能够正常使用，后续使用中主要查看的是Magnum调度的日志<code>/var/log/magnum/magnum-conductor.log</code>。</p><h3 id="2-1-magnum-api-报错"><a href="#2-1-magnum-api-报错" class="headerlink" title="2.1  magnum-api 报错"></a>2.1  magnum-api 报错</h3><p>Magnum API报错 ，如下所示，根据[4]，新的稳定版代码已修复但是安装的deb包并不是最新的代码</p><blockquote><p> File “/usr/lib/python2.7/dist-packages/magnum/common/keystone.py”, line 47, in auth_url<br>      return CONF[ksconf.CFG_LEGACY_GROUP].auth_uri.replace(‘v2.0’, ‘v3’)</p><p> AttributeError: ‘NoneType’ object has no attribute ‘replace’</p></blockquote><p>解决方法如下如<a href="https://review.openstack.org/#/c/614082/" target="_blank" rel="noopener">Add support for www_authentication_uri</a>所示，添加如下patch</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/lib/python2.7/dist-packages/magnum/common/keystone.py +47</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">return</span> CONF[ksconf.CFG_LEGACY_GROUP].auth_uri.replace(<span class="string">'v2.0'</span>, <span class="string">'v3'</span>)</span></span><br><span class="line">conf = CONF[ksconf.CFG_LEGACY_GROUP]</span><br><span class="line">auth_uri = (getattr(conf, 'www_authenticate_uri', None) or</span><br><span class="line">            getattr(conf, 'auth_uri', None))</span><br><span class="line">if auth_uri:</span><br><span class="line">   auth_uri = auth_uri.replace('v2.0', 'v3')</span><br><span class="line">return auth_uri</span><br></pre></td></tr></table></figure><p>修改完成后需要重启magnum服务，否则修改的代码不生效。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service magnum-* restart</span><br></pre></td></tr></table></figure><h3 id="2-2-指定swarm集群的volumed-size"><a href="#2-2-指定swarm集群的volumed-size" class="headerlink" title="2.2 指定swarm集群的volumed size"></a>2.2 指定swarm集群的volumed size</h3><p>按照文档创建一个集群模板时出现如下报错：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ctl01:/home/xdnsadmin# openstack coe cluster template create swarm-cluster-template --image fedora-atomic-latest --external-network Provider --dns-nameserver 223.5.5.5 --master-flavorm1.small --flavor m1.small --coe swarm</span><br><span class="line">docker volume size None GB is not valid, expecting minimum value 3GB for devicemapper storage driver (HTTP 400) (Request-ID: req-392aaa81-d44c-48cc-8800-2e9697539a28)</span><br></pre></td></tr></table></figure><p>解决办法是指定一个volume size，按照官方的说法没有cinder服务也可以使用magnum，<strong>这个我并未验证</strong>，我的集群安装的cinder服务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack coe cluster template create swarm-cluster-template --image fedora-atomic-latest --external-network Provider --dns-nameserver 223.5.5.5 --master-flavor m1.small --flavor m1.small --coe swarm --docker-volume-size 10</span><br></pre></td></tr></table></figure><p>同时这里仍有一个问题，查看<em>2.5</em></p><h3 id="2-2-docker-volume-type"><a href="#2-2-docker-volume-type" class="headerlink" title="2.2 docker volume_type"></a>2.2 docker volume_type</h3><p>在安装过程出现无效的volumetype的错误，按照[5]需要设置volumetype同时进行相关配置</p><blockquote><p>ERROR heat.engine.resource     raise exception.ResourceFailure(message, self, action=self.action)<br>ERROR heat.engine.resource ResourceFailure: resources[0]: Property error: resources.docker_volume.properties.volume_type: Error validating value ‘’: The VolumeType () could not be found.</p></blockquote><p>1)Check if you have any volume types defined.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack volume type list</span><br></pre></td></tr></table></figure><p>2)If there are none, create one:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">openstack volume type create volType1 --description "Fix for Magnum" --public</span><br></pre></td></tr></table></figure><p>3)Then in  /etc/magnum/magnum.conf add this line in the [cinder] section:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">default_docker_volume_type = volType1</span><br></pre></td></tr></table></figure><h3 id="2-3-Barbican服务未安装"><a href="#2-3-Barbican服务未安装" class="headerlink" title="2.3  Barbican服务未安装"></a>2.3  <code>Barbican</code>服务未安装</h3><p>按照文档在选择<code>[certificates]</code>时使用推荐的<code>barbican</code>来存储认证证书，而我的openstack集群并未安装<code>barbican</code>服务，因此收到了如下报错，但是我当时并未发现原因所在，我一直以为是keystone的<code>publicURL endpoint</code>无法访问。</p><blockquote><p>2019-04-09 18:42:26.505 32428 ERROR magnum.conductor.handlers.common.cert_manager [req-ffd4b077-80ee-4846-bdc0-84817aa699a5 - - - - -] Failed to generate certificates for Cluster: 03cd9a72-c980-4947-b273-d9bcb88a6913: AuthorizationFailure: unexpected keystone client error occurred: publicURL endpoint for key-manager service not found<br>……….<br>2019-04-09 18:42:26.505 32428 ERROR magnum.conductor.handlers.common.cert_manager     connection = get_admin_clients().barbican()<br>2019-04-09 18:42:26.505 32428 ERROR magnum.conductor.handlers.common.cert_manager   File “/usr/lib/python2.7/dist-packages/magnum/common/exception.py”, line 65, in wrapped<br>2019-04-09 18:42:26.505 32428 ERROR magnum.conductor.handlers.common.cert_manager     % sys.exc_info()[1])<br>2019-04-09 18:42:26.505 32428 ERROR magnum.conductor.handlers.common.cert_manager AuthorizationFailure: unexpected keystone client error occurred: publicURL endpoint for key-manager service not found</p></blockquote><p>该问题的解决方法很简单，安装<code>barbican</code> 服务，或者使用<code>x509keypair</code>或<code>local</code>方式存储证书</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[certificates]</span><br><span class="line">cert_manager_type = barbican # x509keypair or local</span><br><span class="line">storage_path = /var/lib/magnum/certificates/</span><br></pre></td></tr></table></figure><p>若使用local方式，还需创建目录<code>/var/lib/magnum/certificates/</code>，并将其所属组改为magnum，<code>sudo chown -R magnum:magnum /var/lib/magnum/certificates/</code>，否则会出现如下报错：</p><blockquote><p>2019-04-10 15:40:50.675 20235 ERROR magnum.conductor.handlers.common.cert_manager [req-c31e7588-a486-4ea8-8adc-16ff8f37b284 - - - - -] Failed to generate certificates for Cluster: e579     0f96-e8c5-49c5-82f9-87faf8438585: CertificateStorageException: Could not store certificate: [Errno 2] No such file or directory: ‘/var/lib/magnum/certificates/5e8beb02-9378-4e8c-8b4d-1     cae933de665.crt’</p></blockquote><p>同时，在修复此问题的过程中查看到一个警告，该警告对于使用过程也造成了很大的影响<code>Auth plugin and its options for service user must be provided in [keystone_auth] section. Using values from [keystone_authtoken] section is deprecated.: MissingRequiredOptions: Auth plugin requires parameters which were not given: auth_url</code></p><p>需要在<code>/etc/magnum/magnum.conf</code>中 添加<code>keystone_auth</code>项 ，此外，这里的还涉及到接下来的问题<em>2.4</em></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[keystone_auth]</span><br><span class="line">auth_version = v3</span><br><span class="line">auth_url = http://10.253.0.61:5000/v3</span><br><span class="line"><span class="meta">#</span><span class="bash">www_authenticate_uri = http://ctl01:5000/v3</span></span><br><span class="line">auth_uri = http://10.253.0.61:5000</span><br><span class="line">project_domain_id = default</span><br><span class="line">project_name = service</span><br><span class="line">user_domain_id = default</span><br><span class="line">password = magnum</span><br><span class="line">username = magnum</span><br><span class="line">auth_type = password</span><br></pre></td></tr></table></figure><h3 id="2-4-swarm集群创建超时"><a href="#2-4-swarm集群创建超时" class="headerlink" title="2.4 swarm集群创建超时"></a>2.4 swarm集群创建超时</h3><p>在处理完以上的magnum conductor问题后，开始创建swarm集群，但是出现创建超时(超时时长为60分钟)集群的stack启动失败的问题。原因是按照之前的官方文档创建所有服务的endpoint都是使用<code>http://ctl01:xxx</code>而此处创建的swarm集群需要与opensatck相关服务进行通信，无法识别<code>ctl01</code>域名，解析不到服务地址IP，因此<strong>需要将所有的endpiont服务的public地址都是用公开的IP替换</strong>。</p><blockquote><p>ERROR oslo_messaging.rpc.server [req-4a8e8e2d-cae0-4113-a49d-57bb91c03b8d - - - - -] Exception during message handling: EndpointNotFound: <a href="http://ctl01:9511/v1" target="_blank" rel="noopener">http://ctl01:9511/v1</a> endpoint for identity service not found<br>….<br>ERROR oslo_messaging.rpc.server   File “/usr/lib/python2.7/dist-packages/magnum/conductor/handlers/cluster_conductor.py”, line 68, in cluster_create<br>ERROR oslo_messaging.rpc.server     cluster_driver.create_cluster(context, cluster, create_timeout)<br>….<br>ERROR oslo_messaging.rpc.server   File “/usr/lib/python2.7/dist-packages/keystoneauth1/access/service_catalog.py”, line 464, in endpoint_data_for<br>ERROR oslo_messaging.rpc.server     raise exceptions.EndpointNotFound(msg)<br>ERROR oslo_messaging.rpc.server EndpointNotFound: <a href="http://ctl01:9511/v1" target="_blank" rel="noopener">http://ctl01:9511/v1</a> endpoint for identity service not found</p></blockquote><p>使用命令<code>openstack endpoint list | grep public</code>查看相关的endpoint。</p><h3 id="2-5-swarm-cluster启动失败"><a href="#2-5-swarm-cluster启动失败" class="headerlink" title="2.5 swarm-cluster启动失败"></a>2.5 swarm-cluster启动失败</h3><p>在解决完<em>2.4</em>的问题后又迎来了新的问题，swarm-cluster集群启动创建成功，但是服务报错。这是能够通过<code>floating ip</code>登录swarm master节点使用<code>fedora</code>用户以及密钥<code>mykey</code>登录。查看<code>cloud init</code>日志<code>/var/log/cloud-init-output.log</code>，会发现如下的报错：</p><blockquote><p>/var/lib/cloud/instance/scripts/part-006: line 13: /etc/etcd/etcd.conf: No such file or directory<br>/var/lib/cloud/instance/scripts/part-006: line 26: /etc/etcd/etcd.conf: No such file or directory<br>/var/lib/cloud/instance/scripts/part-006: line 38: /etc/etcd/etcd.conf: No such file or directory</p></blockquote><p><code>etcd</code>初始失败从而导致<code>swarm-manager</code>启动失败，这个问题还是相当的严重的，因为很难排查原因，最终在opensatck的讨论邮件列表中找到的答案[6]，根本原因是<code>coe</code>使用错误，应该使用<code>swarm-mode</code>而不是安装手册里写的<code>swam</code>，这点实在是<strong>太坑人</strong>了。</p><blockquote><p>$openstack coe cluster template show docker-swarm</p><p>| docker_storage_driver | devicemapper                         |<br>| network_driver             | docker                                     |<br>| coe                               | swarm-mode                           |</p><p>Never got the “swarm” driver to work, you should use “swarm-mode” instead which uses native Docker clustering without etcd.</p></blockquote><p><strong>同时需要注意的是</strong>，使用的<code>Fedora</code>镜像必须是<code>Fedora-Atomic-27-xx</code>，<code>Frdora 27</code>之后的镜像改变较大，<code>cloud init</code>脚本无法正常工作。还需添加必要的patch <a href="https://review.openstack.org/#/c/598179/" target="_blank" rel="noopener">swarm-mode allow TCP port 2377 to swarm master node</a>，修改<code>magnum/drivers/swarm_fedora_atomic_v2/templates/swarmcluster.yaml</code>的Line 247，添加一下内容。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- protocol:</span> <span class="string">tcp</span></span><br><span class="line"><span class="attr">  port_range_min:</span> <span class="number">2377</span></span><br><span class="line"><span class="attr">  port_range_max:</span> <span class="number">2377</span></span><br></pre></td></tr></table></figure><p>随后，</p><p>1）重新创建<code>swarm-cluster</code>模板</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# openstack coe cluster template create swarm-cluster-template --image fedora-atomic-latest --external-network Provider --dns-nameserver 223.5.5.5 --master-flavor m1.small --flavor m1.small --coe swarm-mode  --docker-volume-size 10</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgcreate_swarm_cluster_template.png" alt="创建swarm-cluster集群模板"></p><p>2）启动<code>swarm-cluster</code>实例</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# openstack coe cluster create swarm-cluster --cluster-template swarm-cluster-template --master-count 1 --node-count 1 --keypair mykey</span><br></pre></td></tr></table></figure><p>等待集群实例创建完成</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgshow_swarm_cluster.png" alt="show swarm_cluster"></p><p>3）配置<code>swarm-cluster</code>访问</p><p>首先，生成证书，然后导入环境变量即可访问swarm集群(<em>控制节点需要安装docker客户端</em>)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# $(openstack coe cluster config swarm-cluster --dir /home/xdnsadmin/workplace/magnum/swarm/cluster)</span><br><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# export DOCKER_HOST=tcp://10.253.0.115:2375</span><br><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# export DOCKER_CERT_PATH=/home/xdnsadmin/workplace/magnum/swarm/cluster</span><br><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# export DOCKER_TLS_VERIFY=True</span><br><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# docker info</span><br><span class="line">Containers: 0</span><br><span class="line">.....</span><br><span class="line">Images: 0</span><br><span class="line">Server Version: 1.13.1</span><br><span class="line">Storage Driver: devicemapper</span><br><span class="line"> Pool Name: docker-docker--pool</span><br><span class="line">.....</span><br><span class="line"> Backing Filesystem: xfs</span><br><span class="line"> Data file:</span><br><span class="line"> Metadata file:</span><br><span class="line"> ......</span><br><span class="line"> Library Version: 1.02.144 (2017-10-06)</span><br><span class="line">Logging Driver: journald</span><br><span class="line">Cgroup Driver: systemd</span><br><span class="line">Plugins:</span><br><span class="line"> Volume: local</span><br><span class="line"> Network: bridge host macvlan null overlay</span><br><span class="line"> Authorization: rhel-push-plugin</span><br><span class="line"> Log:</span><br><span class="line">Swarm: active</span><br><span class="line"> NodeID: jcsq6ms8717ougczpyhgdayic</span><br><span class="line"> Is Manager: true</span><br><span class="line"> ClusterID: jxzj1kr158cuqu5ph8k2nq8pu</span><br><span class="line"> Managers: 1</span><br><span class="line"> Nodes: 2</span><br><span class="line"> Orchestration:</span><br><span class="line">  Task History Retention Limit: 5</span><br><span class="line"> Raft:</span><br><span class="line">  Snapshot Interval: 10000</span><br><span class="line">  Number of Old Snapshots to Retain: 0</span><br><span class="line">  Heartbeat Tick: 1</span><br><span class="line">  Election Tick: 3</span><br><span class="line"> Dispatcher:</span><br><span class="line">  Heartbeat Period: 5 seconds</span><br><span class="line"> CA Configuration:</span><br><span class="line">  Expiry Duration: 3 months</span><br><span class="line">  Force Rotate: 0</span><br><span class="line"> Autolock Managers: false</span><br><span class="line"> Root Rotation In Progress: false</span><br><span class="line"> Node Address: 10.0.0.8</span><br><span class="line"> Manager Addresses:</span><br><span class="line">  10.0.0.8:2377</span><br><span class="line">Runtimes: oci runc</span><br><span class="line">Default Runtime: oci</span><br><span class="line">.........</span><br><span class="line">  Profile: /etc/docker/seccomp.json</span><br><span class="line"> selinux</span><br><span class="line">Kernel Version: 4.14.18-300.fc27.x86_64</span><br><span class="line">Operating System: Fedora 27 (Atomic Host)</span><br><span class="line">OSType: linux</span><br><span class="line">Architecture: x86_64</span><br><span class="line">CPUs: 1</span><br><span class="line">Total Memory: 1.946GiB</span><br><span class="line">Name: swarm-cluster-cgjrw2xrwylk-primary-master-0.novalocal</span><br><span class="line">ID: 27CS:YZSQ:WOQJ:PMHZ:ALTJ:WMB3:IIA7:FLUC:2RIW:4CKA:FZWV:4GZA</span><br><span class="line">Docker Root Dir: /var/lib/docker</span><br><span class="line">Debug Mode (client): false</span><br><span class="line">Debug Mode (server): false</span><br><span class="line">Registry: https://index.docker.io/v1/</span><br><span class="line">Experimental: false</span><br><span class="line">Insecure Registries:</span><br><span class="line"> 127.0.0.0/8</span><br><span class="line">Live Restore Enabled: false</span><br><span class="line"></span><br><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# docker -H 10.253.0.115:2375 run busybox:latest echo 'Hello world!'</span><br><span class="line">Unable to find image 'busybox:latest' locally</span><br><span class="line">Trying to pull repository docker.io/library/busybox ...</span><br><span class="line">Trying to pull repository registry.fedoraproject.org/busybox ...</span><br><span class="line">Trying to pull repository registry.access.redhat.com/busybox ...</span><br><span class="line">Trying to pull repository docker.io/library/busybox ...</span><br><span class="line">sha256:954e1f01e80ce09d0887ff6ea10b13a812cb01932a0781d6b0cc23f743a874fd: Pulling from docker.io/library/busybox</span><br><span class="line">fc1a6b909f82: Pull complete</span><br><span class="line">Digest: sha256:954e1f01e80ce09d0887ff6ea10b13a812cb01932a0781d6b0cc23f743a874fd</span><br><span class="line">Status: Downloaded newer image for docker.io/busybox:latest</span><br><span class="line">Hello world!</span><br></pre></td></tr></table></figure><h3 id="2-6-standard-init-linux-go-178-exec-user-process-caused-“permission-denied”"><a href="#2-6-standard-init-linux-go-178-exec-user-process-caused-“permission-denied”" class="headerlink" title="2.6 standard_init_linux.go:178: exec user process caused “permission denied”"></a>2.6 standard_init_linux.go:178: exec user process caused “permission denied”</h3><p>该错误与swarm节点的存储格式有关，由于之前解决swarm无法启动而被带偏添加了<code>--docker-storage-driver overlay</code>参数在<code>swarm-cluster</code>模板中，导致容器无法启动，通过查看[7]得知与其支持的存储驱动有关</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# docker -H 10.253.0.111:2375 run busybox echo "Hello from Docker!"</span><br><span class="line">Unable to find image 'busybox:latest' locally</span><br><span class="line">Trying to pull repository docker.io/library/busybox ...</span><br><span class="line">Trying to pull repository registry.fedoraproject.org/busybox ...</span><br><span class="line">Trying to pull repository registry.access.redhat.com/busybox ...</span><br><span class="line">Trying to pull repository docker.io/library/busybox ...</span><br><span class="line">sha256:954e1f01e80ce09d0887ff6ea10b13a812cb01932a0781d6b0cc23f743a874fd: Pulling from docker.io/library/busybox</span><br><span class="line">fc1a6b909f82: Pull complete</span><br><span class="line">Digest: sha256:954e1f01e80ce09d0887ff6ea10b13a812cb01932a0781d6b0cc23f743a874fd</span><br><span class="line">Status: Downloaded newer image for docker.io/busybox:latest</span><br><span class="line">standard_init_linux.go:178: exec user process caused "permission denied"</span><br><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# docker -H 10.253.0.111:2375 ps -a</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES</span><br><span class="line">f1a60f01625f        busybox             "echo 'Hello from Do…"   10 minutes ago      Exited (1) 10 minutes ago                       elastic_kirch</span><br><span class="line">root@ctl01:/home/xdnsadmin/workplace/magnum/swarm/cluster# docker -H 10.253.0.111:2375 logs f1a60f01625f</span><br><span class="line">standard_init_linux.go:178: exec user process caused "permission denied"</span><br></pre></td></tr></table></figure><p>目前<code>Docker</code>支持如下几种storage driver：</p><table><thead><tr><th align="left">Technology</th><th align="left">Storage driver name</th></tr></thead><tbody><tr><td align="left">OverlayFS</td><td align="left">overlay</td></tr><tr><td align="left">AUFS</td><td align="left">aufs</td></tr><tr><td align="left">Btrfs</td><td align="left">btrfs</td></tr><tr><td align="left">Device Mapper</td><td align="left">devicemapper</td></tr><tr><td align="left">VFS</td><td align="left">vfs</td></tr><tr><td align="left">ZFS</td><td align="left">zfs</td></tr></tbody></table><p>详细的对比可以参看：<a href="https://blog.csdn.net/vchy_zhao/article/details/70238690" target="_blank" rel="noopener">Docker之几种storage-driver比较</a></p><p>以上，就是Openstack Rocky版本Magnum安装与排坑的过程，不说啦，刚刚安装完Rocky版本，Stein版本就放出来了，我接着去踩雷啦 - -！<strong>开源真香</strong></p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgFix_bug.gif" alt="无限 Fix Bug"></p><p>【参考链接】</p><p>1）<a href="https://www.zhihu.com/question/63576896" target="_blank" rel="noopener">如何把OpenStack和Kubernetes结合在一起来构建容器云平台</a></p><p>2）<a href="https://www.openstack.org/software/releases/rocky/components/magnum" target="_blank" rel="noopener">Magnum -Container Orchestration Engine Provisioning</a></p><p>3）<a href="https://docs.openstack.org/magnum/rocky/install/install.html" target="_blank" rel="noopener">Magnum安装手册</a></p><p>4）<a href="https://bugs.launchpad.net/ubuntu/+source/magnum/+bug/1793813" target="_blank" rel="noopener">magnum-api not working with www_authenticate_uri</a></p><p>5）<a href="https://ask.openstack.org/en/question/110729/magnum-cluster-create-k8s-cluster-error-resourcefailure/" target="_blank" rel="noopener">magnum cluster create k8s cluster Error: ResourceFailure</a></p><p>6）<a href="http://lists.openstack.org/pipermail/openstack-discuss/2018-December/000937.html" target="_blank" rel="noopener">Fwd: openstack queens magnum error</a></p><p>7）<a href="https://github.com/moby/moby/issues/26495" target="_blank" rel="noopener">standard_init_linux.go:175: exec user process caused “permission denied”</a></p>]]></content>
    
    <summary type="html">
    
      Openstack magnum 安装 -- 踩坑与填坑系列
    
    </summary>
    
      <category term="Openstack" scheme="https://louielong.github.io/categories/Openstack/"/>
    
    
      <category term="Openstack" scheme="https://louielong.github.io/source/tags/Openstack/"/>
    
  </entry>
  
  <entry>
    <title>博客图挂了 ( ˇˍˇ )</title>
    <link href="https://louielong.github.io/blog-pic-missing.html"/>
    <id>https://louielong.github.io/blog-pic-missing.html</id>
    <published>2019-04-12T07:10:08.000Z</published>
    <updated>2019-04-12T07:12:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自从接触到<code>markdown</code>文本标记语言后一直使用<a href="http://markdownpad.com/" target="_blank" rel="noopener">Markdown Pad2</a>编辑器，其直接预览以及在线生成图片链接的功能非常受用，可是该软件的<a href="http://markdownpad.com/news/2013/introducing-markdownpad-2/" target="_blank" rel="noopener">更新</a>一直停留在2014年，在我的工作电脑从win7切换成win10后预览功能就无法使用，我使用<a href="https://typora.io/" target="_blank" rel="noopener">typra</a>作为替代，但是还是保留着<code>Markdown Pad2</code>因为其在线生成图片链接的功能还能使用。终于，我担心的情况出现了 - - ，<strong><code>Markdown Pad2</code>上传的图挂了</strong>，先前写的文章图片全部看不到了。(Ps:好像不是图床<code>Imgur</code>挂了，而是<strong>国内无法访问</strong>)</p><p>没办法，只能更换图床了，这里使用<code>Github</code>作为图床，配合<a href="https://molunerfinn.com/PicGo/" target="_blank" rel="noopener">PicGo</a>上传图片。</p><p><img src="https://molunerfinn.com/PicGo/imgs/256x256--icons.png" alt="PicGo"></p><p>这里直接引用<a href="https://juejin.im/entry/5c4ec5aaf265da614420689f" target="_blank" rel="noopener">PicGo+GitHub图床，让Markdown飞</a>博客设置的内容了，不再重复配置啦。</p>]]></content>
    
    <summary type="html">
    
      可怜的博客图片无法在国内访问
    
    </summary>
    
      <category term="mix" scheme="https://louielong.github.io/categories/mix/"/>
    
    
  </entry>
  
  <entry>
    <title>Docker容器上限思考</title>
    <link href="https://louielong.github.io/think-of-docker-container-limit.html"/>
    <id>https://louielong.github.io/think-of-docker-container-limit.html</id>
    <published>2019-03-07T06:01:26.000Z</published>
    <updated>2019-07-01T06:15:31.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>在研究<code>Helm</code>时发现其创建的随机容器名跟docker类似都是左右形式的组合，因此想到Docker随机容器名的上限是多少，会不会因为随机容器名的重复而导致无法再创建随机容器名的容器实例而达到容器的上限。</p><h2 id="二、随机容器名生成"><a href="#二、随机容器名生成" class="headerlink" title="二、随机容器名生成"></a>二、随机容器名生成</h2><p>根据<a href="https://frightanic.com/computers/docker-default-container-names/" target="_blank" rel="noopener">my2cents</a>的<code>How does Docker generate default container names?</code>文章介绍，得知docker随机容器名的生成是分为左右值的，即拥有两个独立的数组。查看docker容器名生成的<a href="https://github.com/moby/moby/blob/master/pkg/namesgenerator/names-generator.go" target="_blank" rel="noopener">源代码</a>可以看到左值都是一些形容词，共计100个，右值都是一些<code>amazing man</code>，并且给出了wiki百科的介绍页面，共计235位。(统计截止到2019年3月7日)</p><p>有意思的是代码的最后一行，当容器名为<code>boring_wozniak</code>时会重新生成，<a href="https://en.wikipedia.org/wiki/Steve_Wozniak" target="_blank" rel="noopener">Steve Wozniak</a>是Apple I和Apple II的发明人，看来码代码的人十分推崇Wozniak，加了专门的彩蛋。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> name == <span class="string">"boring_wozniak"</span> <span class="comment">/* Steve Wozniak is not boring */</span> &#123;</span><br><span class="line">  <span class="keyword">goto</span> begin</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据容器名规则以及容器名实例不能重复原则使用排列组合可以得知能够创建的随机容器名实例上限是100*235-1=23499个。</p><p><strong>这种容器名生成的优势是什么？</strong></p><p>通过使用<strong>形容词-单词</strong>的组合方式生成的容器名比枯燥的hash数字更为好看也便于记忆。</p><h2 id="三、容器上限"><a href="#三、容器上限" class="headerlink" title="三、容器上限"></a>三、容器上限</h2><p>问题来了，在单一主机中到底能够运行多少个容器？</p><p>根据<code>stack overflow</code>上的<a href="https://stackoverflow.com/questions/21799382/is-there-a-maximum-number-of-containers-running-on-a-docker-host" target="_blank" rel="noopener">Is there a maximum number of containers running on a Docker host?</a>回答，翻译如下[1]</p><p>docker容器创建实例有很多的系统限制，一些重要的信息如下：</p><ul><li>容器的配置</li><li>容器里运行的程序</li><li>系统的内核版本、发行版本、docker版本</li></ul><p><strong>详细的运行限制如下</strong>：</p><ul><li>链接到虚拟网络适配器docker0网桥的设备限制：(每个网桥最多1023)</li><li>挂载联合文件系统(AUFS)和shm文件系统：(最大挂载数量1048576)</li><li>在镜像image上创建的层数layer数量：（最多127layer每个镜像）</li><li>fork出来一个docker-containerd-shim的管理进程:(每个容器平均3M左右，系统最大进程数sysctl kernel.pid_max)</li><li>docker daemon守候进程管理容器的内部数据：(~400k 每个容器)</li><li>创建内核的<em>cgroups<em>和</em>namespace</em></li><li>打开文件描述符：(启动中的容器16个左右) ulimit -n and sysctl fs.file-max</li><li>端口映射，-p将会在宿主机上为每一个映射的端口启动一个外部进程：（平均每个端口占用~4.5MB）</li><li>–net=none 和 –net=host参数将有助于减少网络消耗。</li></ul><p><strong>Container 服务</strong><br>总的资源消耗取决于你容器内运行的程序，而不是docker本身，如果你在虚拟机中运行应用程序node,ruby,python,java，内存的消耗将是主要问题。<br>1000个进程会消耗大大量的IO 链接。1000个进程同时运行也会引起大量的上下文交换，</p><p>1023 Docker busybox images</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -l -p 80 -e echo</span><br></pre></td></tr></table></figure><p>共计使用约1G的内核空间和3.5G的系统内存</p><p>1023 普通进程</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -l -p 80 -e echo</span><br></pre></td></tr></table></figure><p>将会消耗约75MB的内核空间和125MB的系统内存</p><p>此外，</p><p>连续启动1023个容器预计耗费约8分钟，关闭1023个容器预计耗费约6分钟</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>有文章做了一个实验来启动500个容器，感兴趣的可以查看<a href="https://blog.csdn.net/warrior_0319/article/details/79730191" target="_blank" rel="noopener">docker 启动500个容器测试</a></p><h2 id="四、结论"><a href="#四、结论" class="headerlink" title="四、结论"></a>四、结论</h2><p>根据以上信息可以得知随机容器名的限制不会是容器创建上限的障碍，容器实例本身所消耗的资源如内存、网络等才是最关键的影响。</p><p>【参考链接】</p><p>1）<a href="https://stackoverflow.com/questions/21799382/is-there-a-maximum-number-of-containers-running-on-a-docker-host" target="_blank" rel="noopener">Is there a maximum number of containers running on a Docker host?</a></p><p>2）<a href="https://blog.csdn.net/warrior_0319/article/details/79716901" target="_blank" rel="noopener">docker 最大container数量调研</a></p>]]></content>
    
    <summary type="html">
    
      Docker容器创建上限的思考
    
    </summary>
    
      <category term="docker" scheme="https://louielong.github.io/categories/docker/"/>
    
    
      <category term="docker" scheme="https://louielong.github.io/source/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>k8s集群安装与部署</title>
    <link href="https://louielong.github.io/k8s-cluster-deploy.html"/>
    <id>https://louielong.github.io/k8s-cluster-deploy.html</id>
    <published>2019-03-01T01:38:08.000Z</published>
    <updated>2019-07-01T05:45:03.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>一年前研究过一段时间的K8S，最开始看k8s的概念一头雾水，随着学习的深入有一些了解，跟着<a href="https://steemit.com/@cloudman6" target="_blank" rel="noopener">cloudman</a>的教程尝试搭建k8s集群，但是当时由于墙的原因搭建一个k8s集群颇为费劲，在做完实验对k8s有一个了解后就没有继续深入。最近由于新的项目需要重新研究k8s环境，正好做一下笔记，记录搭建的过程，在搭建过程中一个好消息就是k8s的docker容器在国内有镜像提供商，像阿里云、dockerhub上都有，简直是一大<strong>救星</strong>。<strong>由于有一定的知识了解，本文不再介绍k8s的基本概念和相关的命令含义。</strong></p><p>搭建过程主要参考的是molscar的<a href="https://juejin.im/post/5b8a4536e51d4538c545645c" target="_blank" rel="noopener">使用kubeadm 部署 Kubernetes(国内环境)</a>。</p><h2 id="二、安装环境准备"><a href="#二、安装环境准备" class="headerlink" title="二、安装环境准备"></a>二、安装环境准备</h2><p>k8s集群的安装，至少需要一个master节点和一个slave节点，节点配置如下</p><ul><li>操作系统要求<ul><li>Ubuntu 16.04+</li><li>Debian 9</li><li>CentOS 7</li><li>RHEL 7</li><li>Fedora 25/26 (best-effort)</li><li>HypriotOS v1.0.1+</li><li>Container Linux (tested with 1800.6.0)</li></ul></li><li>2+ GB RAM</li><li>2+ CPUs</li><li>特定的端口开放（安全组和防火墙未将其排除在外）</li><li>关闭Swap交换分区</li></ul><p>本文搭建时使用的系统是ubuntu 16.04.5，一个master节点4个slave节点，安装的是当前k8s最新的v1.13版本。</p><h3 id="2-1-docker-安装"><a href="#2-1-docker-安装" class="headerlink" title="2.1 docker 安装"></a>2.1 docker 安装</h3><h4 id="1）docker软件安装"><a href="#1）docker软件安装" class="headerlink" title="1）docker软件安装"></a>1）docker软件安装</h4><p><strong>master和slave节点都需要安装docker</strong></p><p>我的网络在教育网访问更快，这里使用<a href="https://mirrors.tuna.tsinghua.edu.cn/help/docker-ce/" target="_blank" rel="noopener">清华源</a>来进行安装</p><p>安装依赖：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common</span><br></pre></td></tr></table></figure><p>信任 Docker 的 GPG 公钥:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br></pre></td></tr></table></figure><p>对于 amd64 架构的计算机，添加软件仓库:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository \</span><br><span class="line">   <span class="string">"deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu \</span></span><br><span class="line"><span class="string">   <span class="variable">$(lsb_release -cs)</span> \</span></span><br><span class="line"><span class="string">   stable"</span></span><br></pre></td></tr></table></figure><p>最后安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install docker-ce</span><br></pre></td></tr></table></figure><p>安装完成后通过命令查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ docker --version</span><br><span class="line">Docker version 18.09.2, build 6247962</span><br></pre></td></tr></table></figure><h4 id="2）免sudo执行docker命令"><a href="#2）免sudo执行docker命令" class="headerlink" title="2）免sudo执行docker命令"></a>2）免sudo执行docker命令</h4><p>为了使普通用户免sudo使用docker命令</p><p>将当前用户加入 <code>docker</code> 组：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG docker $USER</span><br></pre></td></tr></table></figure><h4 id="3）镜像加速"><a href="#3）镜像加速" class="headerlink" title="3）镜像加速"></a>3）镜像加速</h4><p>对于使用 <a href="https://link.juejin.im?target=https%3A%2F%2Fwww.freedesktop.org%2Fwiki%2FSoftware%2Fsystemd%2F" target="_blank" rel="noopener">systemd</a> 的系统，请在 <code>/etc/docker/daemon.json</code> 中写入如下内容（如果文件不存在请新建该文件）</p><p>这里使用Docker 官方加速器 ，也可以使用阿里云或者中科大的加速</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="attr">  "registry-mirrors":</span> <span class="string">[</span></span><br><span class="line">    <span class="string">"https://registry.docker-cn.com"</span></span><br><span class="line">  <span class="string">]</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>重新启动服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><h4 id="4）禁用-swap"><a href="#4）禁用-swap" class="headerlink" title="4）禁用 swap"></a>4）禁用 swap</h4><p><strong>该步骤只需要在master节点执行即可</strong></p><blockquote><p>对于禁用<code>swap</code>内存，具体原因可以查看Github上的Issue：<a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fkubernetes%2Fissues%2F53533" target="_blank" rel="noopener">Kubelet/Kubernetes should work with Swap Enabled</a></p></blockquote><p>临时关闭方式：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo swapoff -a</span><br></pre></td></tr></table></figure><p>永久关闭方式：</p><p>编辑<code>/etc/fstab</code>文件，注释掉引用<code>swap</code>的行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> &lt;file system&gt; &lt;mount point&gt;   &lt;<span class="built_in">type</span>&gt;  &lt;options&gt;       &lt;dump&gt;  &lt;pass&gt;</span></span><br><span class="line">/dev/mapper/ubuntu--vg-root /               ext4    errors=remount-ro 0       1</span><br><span class="line"><span class="meta">#</span><span class="bash"> /boot was on /dev/sda2 during installation</span></span><br><span class="line">UUID=50bd7683-2bf2-4953-bf0b-125f74e06e52 /boot           ext2    defaults        0       2</span><br><span class="line"><span class="meta">#</span><span class="bash"> /boot/efi was on /dev/sda1 during installation</span></span><br><span class="line">UUID=6EA7-687A  /boot/efi       vfat    umask=0077      0       1</span><br><span class="line"><span class="meta">#</span><span class="bash">/dev/mapper/ubuntu--vg-swap_1 none            swap    sw              0       0</span></span><br></pre></td></tr></table></figure><p>随后重启机器</p><p>测试：输入<code>top</code> 命令，若 KiB Swap一行中 total 显示 0 则关闭成功</p><h3 id="2-2-安装-kubeadm-kubelet-以及-kubectl"><a href="#2-2-安装-kubeadm-kubelet-以及-kubectl" class="headerlink" title="2.2 安装 kubeadm, kubelet 以及 kubectl"></a>2.2 安装 kubeadm, kubelet 以及 kubectl</h3><p><strong>master和slave节点都需要安装三个软件</strong></p><h4 id="1）k8s加速安装配置"><a href="#1）k8s加速安装配置" class="headerlink" title="1）k8s加速安装配置"></a>1）k8s加速安装配置</h4><p>这里可以选择使用清华源或中科大源加速安装</p><p>首先添加软件源认证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl</span><br><span class="line">curl -s http://packages.faasx.com/google/apt/doc/apt-key.gpg | sudo apt-key add -</span><br></pre></td></tr></table></figure><p>随后添加中科大源仓库地址并安装软件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">apt-get update</span><br><span class="line">apt-get install -y kubelet kubeadm kubectl</span><br><span class="line">apt-mark hold kubelet kubeadm kubectl</span><br></pre></td></tr></table></figure><h3 id="2-3-k8s集群安装"><a href="#2-3-k8s集群安装" class="headerlink" title="2.3 k8s集群安装"></a>2.3 k8s集群安装</h3><h4 id="1）cgroups配置"><a href="#1）cgroups配置" class="headerlink" title="1）cgroups配置"></a>1）cgroups配置</h4><p>在Master节点中配置 cgroup driver</p><p>查看 Docker 使用 cgroup driver:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker info | grep -i cgroup</span><br><span class="line"><span class="meta">-&gt;</span><span class="bash"> Cgroup Driver: cgroupfs</span></span><br></pre></td></tr></table></figure><p>而 kubelet 使用的 cgroupfs 为system，不一致故有如下修正：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br></pre></td></tr></table></figure><p>加上如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"</span><br></pre></td></tr></table></figure><p>重启 kubelet</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><h4 id="2）镜像配置"><a href="#2）镜像配置" class="headerlink" title="2）镜像配置"></a>2）镜像配置</h4><p>这里使用<code>kubeadm</code>来进行自动化集群安装和配置，在安装过程中会从google仓库中下载镜像，由于墙的原因镜像下载会失败，这里预选将相应的镜像下载下来再重新改tag</p><p>Master 和 Slave 启动的核心服务分别如下：</p><table><thead><tr><th>Master 节点</th><th>Slave 节点</th></tr></thead><tbody><tr><td>etcd-master</td><td>Control plane(如：calico,fannel)</td></tr><tr><td>kube-apiserver</td><td>kube-proxy</td></tr><tr><td>kube-controller-manager</td><td>other apps</td></tr><tr><td>kube-dns</td><td></td></tr><tr><td>Control plane(如：calico,fannel)</td><td></td></tr><tr><td>kube-proxy</td><td></td></tr><tr><td>kube-scheduler</td><td></td></tr></tbody></table><p>使用如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm config images list</span><br></pre></td></tr></table></figure><p>获取当前版本kubeadm 启动需要的镜像，示例如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ kubeadm config images list</span><br><span class="line">k8s.gcr.io/kube-apiserver:v1.13.3</span><br><span class="line">k8s.gcr.io/kube-controller-manager:v1.13.3</span><br><span class="line">k8s.gcr.io/kube-scheduler:v1.13.3</span><br><span class="line">k8s.gcr.io/kube-proxy:v1.13.3</span><br><span class="line">k8s.gcr.io/pause:3.1</span><br><span class="line">k8s.gcr.io/etcd:3.2.24</span><br><span class="line">k8s.gcr.io/coredns:1.2.6</span><br></pre></td></tr></table></figure><p>从<code>dockerhub</code>的<a href="https://hub.docker.com/u/mirrorgooglecontainers" target="_blank" rel="noopener">mirrorgooglecontainers</a>仓库下载相应镜像，如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ docker pull mirrorgooglecontainers/kube-apiserver:v1.13.3</span><br><span class="line">....</span><br><span class="line">xdnsadmin@k8smaster:~$ docker tag mirrorgooglecontainers/kube-apiserver:v1.13.3 k8s.gcr.io/kube-apiserver:v1.13.3</span><br><span class="line">xdnsadmin@k8smaster:~$ docker rmi mirrorgooglecontainers/kube-apiserver:v1.13.3</span><br></pre></td></tr></table></figure><p>这里分享一个脚本来自动完成镜像下载并改tag的操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ cat k8s_images_get.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash -e</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">########################################################################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> File Name: k8s_images_get.sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Author: louie.long</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Mail: ylong@biigroup.cn</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Created Time: Mon 25 Feb 2019 05:05:54 PM CST</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Description:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">########################################################################</span></span></span><br><span class="line"></span><br><span class="line">if [ ! -x /usr/bin/kubeadm ]; then</span><br><span class="line">echo "***please install kubeadm first***"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">while read IMAGE; do</span><br><span class="line">IMG=`echo $IMAGE | sed "s/k8s\.gcr\.io/mirrorgooglecontainers/"`</span><br><span class="line">echo $IMG</span><br><span class="line">done &lt; &lt;(kubeadm config images list)</span><br></pre></td></tr></table></figure><h4 id="3）检查端口占用"><a href="#3）检查端口占用" class="headerlink" title="3）检查端口占用"></a>3）检查端口占用</h4><p>Master 节点</p><table><thead><tr><th>Protocol</th><th>Direction</th><th>Port Range</th><th>Purpose</th><th>Used By</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>6443*</td><td>Kubernetes API server</td><td>All</td></tr><tr><td>TCP</td><td>Inbound</td><td>2379-2380</td><td>etcd server client API</td><td>kube-apiserver, etcd</td></tr><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>Kubelet API</td><td>Self, Control plane</td></tr><tr><td>TCP</td><td>Inbound</td><td>10251</td><td>kube-scheduler</td><td>Self</td></tr><tr><td>TCP</td><td>Inbound</td><td>10252</td><td>kube-controller-manager</td><td>Self</td></tr></tbody></table><p>Worker节点</p><table><thead><tr><th>Protocol</th><th>Direction</th><th>Port Range</th><th>Purpose</th><th>Used By</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>Kubelet API</td><td>Self, Control plane</td></tr><tr><td>TCP</td><td>Inbound</td><td>30000-32767</td><td>NodePort Services**</td><td>All</td></tr></tbody></table><h4 id="4）初始化-kubeadm"><a href="#4）初始化-kubeadm" class="headerlink" title="4）初始化 kubeadm"></a>4）初始化 kubeadm</h4><p>在初始化过程中需要<code>coredns</code>的支持，同样需要预先下载，随着安装的k8s版本不同所需的<code>coredns</code>版本也会相应改变，可以先执行<code>kubeadm init</code>查看报错</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ docker pull docker pull coredns/coredns:1.2.6</span><br><span class="line">xdnsadmin@k8smaster:~$ docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6</span><br><span class="line">xdnsadmin@k8smaster:~$ docker rmi coredns/coredns:1.2.6</span><br></pre></td></tr></table></figure><p>集群master节点初始化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ sudo kubeadm init --apiserver-advertise-address=&lt;your ip&gt; --pod-network-cidr=10.244.0.0/16</span><br><span class="line">......</span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run (as a regular user):</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a Pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [Podnetwork].yaml"</span> with one of the addon options listed at:</span><br><span class="line">  http://kubernetes.io/docs/admin/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;<span class="built_in">hash</span>&gt;</span><br></pre></td></tr></table></figure><p><strong>init 常用主要参数：</strong></p><ul><li>–kubernetes-version: 指定Kubenetes版本，如果不指定该参数，会从google网站下载最新的版本信息。</li><li>–pod-network-cidr: 指定pod网络的IP地址范围，它的值取决于你在下一步选择的哪个网络网络插件，这里选用flannel网络因此指定<code>--pod-network-cidr=10.244.0.0/16</code>，需要注意的该CIDR不可修改。</li><li>–apiserver-advertise-address: 指定master服务发布的Ip地址，如果不指定，则会自动检测网络接口，通常是内网IP。</li><li>–feature-gates=CoreDNS: 是否使用CoreDNS，值为true/false，CoreDNS插件在1.10中提升到了Beta阶段，最终会成为Kubernetes的缺省选项。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure><p>master节点测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl https://127.0.0.1:6443 -k 或者 curl https://&lt;master-ip&gt;:6443 -k</span><br></pre></td></tr></table></figure><p>回应如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  "kind": "Status",</span><br><span class="line">  "apiVersion": "v1",</span><br><span class="line">  "metadata": &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  "status": "Failure",</span><br><span class="line">  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",</span><br><span class="line">  "reason": "Forbidden",</span><br><span class="line">  "details": &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  "code": 403</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>重置初始化</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ kubeadm reset</span><br></pre></td></tr></table></figure><p><strong>忘记了token，可以通过命令查看</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ kubeadm token list</span><br><span class="line">TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS</span><br><span class="line">zsrpzu.lh831z4t4ht3vm1k   15h       2019-03-01T09:40:53+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token</span><br></pre></td></tr></table></figure><h4 id="5）安装pod插件"><a href="#5）安装pod插件" class="headerlink" title="5）安装pod插件"></a>5）安装pod插件</h4><p>插件可选类型有很多，参考<a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network" target="_blank" rel="noopener">官方pod网络插件</a>，这里选用<code>flannel</code>网络，网络选择在后续可以修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><h4 id="6）节点加入"><a href="#6）节点加入" class="headerlink" title="6）节点加入"></a>6）节点加入</h4><p>在slave节点执行一下命令即可自动加入集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</span><br></pre></td></tr></table></figure><p>查看节点加入情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ kubectl get nodes</span><br><span class="line">NAME        STATUS   ROLES    AGE     VERSION</span><br><span class="line">k8slave1    Ready    &lt;none&gt;   8h      v1.13.3</span><br><span class="line">k8slave2    Ready    &lt;none&gt;   7h59m   v1.13.3</span><br><span class="line">k8slave3    Ready    &lt;none&gt;   7h58m   v1.13.3</span><br><span class="line">k8slave4    Ready    &lt;none&gt;   7h58m   v1.13.3</span><br><span class="line">k8smaster   Ready    master   8h      v1.13.3</span><br></pre></td></tr></table></figure><p>若有节点未加入查看相关节点的镜像是否下载成功。</p><h2 id="三、dashboard安装"><a href="#三、dashboard安装" class="headerlink" title="三、dashboard安装"></a>三、dashboard安装</h2><h3 id="3-1-dashboard安装"><a href="#3-1-dashboard安装" class="headerlink" title="3.1 dashboard安装"></a>3.1 dashboard安装</h3><p>k8s集群安装后可以安装dashboard面板便于查看</p><p>官方教程：<a href="https://link.juejin.im?target=https%3A%2F%2Fgithub.com%2Fkubernetes%2Fdashboard%2Fwiki%2FInstallation" target="_blank" rel="noopener">github.com/kubernetes/…</a></p><h4 id="1）预备镜像"><a href="#1）预备镜像" class="headerlink" title="1）预备镜像"></a>1）预备镜像</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下面为插件的镜像</span></span><br><span class="line">k8s.gcr.io/heapster-amd64:v1.5.4</span><br><span class="line">k8s.gcr.io/heapster-grafana-amd64:v5.0.4</span><br><span class="line">k8s.gcr.io/heapster-influxdb-amd64:v1.5.2</span><br></pre></td></tr></table></figure><p>同样的方式先从<code>dockerhub</code>上下载镜像再改tag</p><p>安装dashboard，<strong>不同版本配置模板的链接会有所修改，以官方的安装链接为准</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</span><br></pre></td></tr></table></figure><p>安装过程中可能出现dashboard重新下载而无法连接<code>gcr.k8s.io</code>的情况，则将部署配置文件下载下来手动修改镜像名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml</span><br><span class="line">xdnsadmin@k8smaster:~$ vim kubernetes-dashboard.yaml</span><br><span class="line">xdnsadmin@k8smaster:~$ kubectl create -f kubernetes-dashboard.yaml</span><br></pre></td></tr></table></figure><h4 id="2）配置访问端口"><a href="#2）配置访问端口" class="headerlink" title="2）配置访问端口"></a>2）配置访问端口</h4><p>将<code>type: ClusterIP</code>改成<code>type: NodePort</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ kubectl -n kube-system edit service kubernetes-dashboard</span><br><span class="line"><span class="meta">#</span><span class="bash"> 编辑内容如下：</span></span><br><span class="line">  ports:</span><br><span class="line">  - nodePort: 32576</span><br><span class="line">    port: 443</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 8443</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure><p>查询dashboard状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~/workplace/k8s/dashboard$ kubectl -n kube-system get service kubernetes-dashboard</span><br><span class="line">NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kubernetes-dashboard   NodePort   10.97.229.208   &lt;none&gt;        443:31090/TCP   7h47m</span><br></pre></td></tr></table></figure><p>可以看到k8s将集群内部的<code>10.97.229.208:443</code>端口映射到<code>31090</code>端口，通过浏览器访问<code>https://&lt;Mater IP&gt;:31090</code></p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgHmVNgrV.png" alt="dashboard login"></p><h4 id="3）配置admin"><a href="#3）配置admin" class="headerlink" title="3）配置admin"></a>3）配置admin</h4><p>创建<code>kubernetes-dashboard-admin.yaml</code>并填入以下内容</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kubernetes-dashboard-admin</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kubernetes-dashboard-admin</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kubernetes-dashboard-admin</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><p>之后：<code>kubectl create -f kubernetes-dashboard-admin.yml</code></p><h4 id="4-登录方式"><a href="#4-登录方式" class="headerlink" title="4) 登录方式"></a>4) 登录方式</h4><p><strong>Kubeconfig登录</strong></p><p>创建 admin 用户</p><p>file: admin-role.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">rbac.authorization.kubernetes.io/autoupdate:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line">    <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="string">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br></pre></td></tr></table></figure><p>随后<code>kubectl create -f admin-role.yaml</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~/workplace/k8s/dashboard$ kubectl -n kube-system get secret|grep admin-token</span><br><span class="line">admin-token-rjzkq                                kubernetes.io/service-account-token   3      4h40m</span><br><span class="line">kubernetes-dashboard-admin-token-2k7mr           kubernetes.io/service-account-token   3      4h41m</span><br><span class="line">xdnsadmin@k8smaster:~/workplace/k8s/dashboard$ kubectl -n kube-system describe secret admin-token-rjzkq</span><br><span class="line">Name:         admin-token-rjzkq</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name: admin</span><br><span class="line">              kubernetes.io/service-account.uid: e109d87b-3b1c-11e9-84b9-dcda8043f2e7</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">ca.crt:     1025 bytes</span><br><span class="line">namespace:  11 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1yanprcSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImUxMDlkODdiLTNiMWMtMTFlOS04NGI5LWRjZGE4MDQzZjJlNyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.vAasQjLsTSXhaPAVYbB-7m5QSyMxpcTvUveSi6KJFLvZPR1HILA3m7riAEqWnlX4dg8d02JSpPD0rPXLY636j7ppMN64RJcqMF_Ik3OhbSQpPwpx3LF2h94EwGsCKpqdbzlgKG31OOwO4OibHGiCn4DwIZGIZAFP4qx_CMtDg1d4XMOQgYGXKXmgPY98KJ5hxhzrC3lwf7qopiQ3RuyYn9fvho2ZLyy2X0_6r1a1ZZGJSscdlykd7q9u-i5MLl6GTxnjnj5SH_KI4tXP8TknXSTegBreL45Sd_pn5dhGmo4msWB_jOBqi6K5pX3vxm9cjNKibSZ1TuVgf3LFy0VmNw</span><br></pre></td></tr></table></figure><p>设置 Kubeconfig文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">copy ~/.kube/config Kubeconfig</span><br><span class="line">vim Kubeconfig</span><br><span class="line"># 将上文中获取的token加入其中</span><br></pre></td></tr></table></figure><p>内容如下：</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgqp8kCvV.png" alt="Kubeconfig file"></p><p><strong>Token登录</strong></p><p>获取token:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~/workplace/k8s/dashboard$ kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token</span><br></pre></td></tr></table></figure><p>获取admin-token:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~/workplace/k8s/dashboard$ kubectl -n kube-system describe secret/$(kubectl -n kube-system get secret | grep kubernetes-dashboard-admin | awk '&#123;print $1&#125;') | grep token</span><br></pre></td></tr></table></figure><h3 id="3-2-集成-heapster监控"><a href="#3-2-集成-heapster监控" class="headerlink" title="3.2 集成 heapster监控"></a>3.2 集成 heapster监控</h3><p>当前<code>heapster</code>已经被废弃，但是还有维护，这里也仍然可以使用。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img26da14a552f12f76_hd.jpg" alt="heapster"></p><h4 id="1）安装-heapster"><a href="#1）安装-heapster" class="headerlink" title="1）安装 heapster"></a>1）安装 heapster</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir heapster</span><br><span class="line">cd heapster</span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml</span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml</span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml</span><br><span class="line">wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml</span><br></pre></td></tr></table></figure><p>之后修改 heapster.yaml</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--source=kubernetes:https://10.0.0.1:6443   --------改成自己的ip</span><br><span class="line">--sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--source=kubernetes.summary_api:https://kubernetes.default.svc?inClusterConfig=false&amp;kubeletHttps=true&amp;kubeletPort=10250&amp;insecure=true&amp;auth=</span><br><span class="line">--sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086</span><br></pre></td></tr></table></figure><p><strong>修改镜像为<code>mirrorgooglecontainers</code></strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i "s/k8s.gcr.io/mirrorgooglecontainers/"  *.yaml</span><br></pre></td></tr></table></figure><p><strong>添加heapster api访问权限</strong>，修改heapster-rbac.yaml文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~/workplace/k8s/heapster$ cat heapster-rbac.yaml</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:heapster</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line"><span class="meta">#</span><span class="bash"> 增加以下内容</span></span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster-kubelet-api</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:kubelet-api-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><p>随后部署应用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f .</span><br></pre></td></tr></table></figure><p>效果如下：</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgxwKiU9e.png" alt="dashboard1"></p><h2 id="四、测试应用部署"><a href="#四、测试应用部署" class="headerlink" title="四、测试应用部署"></a>四、测试应用部署</h2><p>选用<a href="https://link.juejin.im?target=https%3A%2F%2Fmicroservices-demo.github.io%2F" target="_blank" rel="noopener">Sock Shop</a>进行应用部署测试，演示一个袜子商城，以微服务的形式部署起来</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl create namespace sock-shop</span><br><span class="line"></span><br><span class="line">kubectl apply -n sock-shop -f "https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true"</span><br></pre></td></tr></table></figure><p>等待 Pod 变为 Running 状态，便安装成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~/workplace/k8s/heapster$ kubectl -n sock-shop get deployment</span><br><span class="line">NAME           READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">carts          1/1     1            1           137m</span><br><span class="line">carts-db       1/1     1            1           137m</span><br><span class="line">catalogue      1/1     1            1           137m</span><br><span class="line">catalogue-db   1/1     1            1           137m</span><br><span class="line">front-end      1/1     1            1           137m</span><br><span class="line">orders         1/1     1            1           137m</span><br><span class="line">orders-db      1/1     1            1           137m</span><br><span class="line">payment        1/1     1            1           137m</span><br><span class="line">queue-master   1/1     1            1           137m</span><br><span class="line">rabbitmq       1/1     1            1           137m</span><br><span class="line">shipping       1/1     1            1           137m</span><br><span class="line">user           1/1     1            1           137m</span><br><span class="line">user-db        1/1     1            1           137m</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgbmCoQw9.png" alt="sockshop1"></p><p>官方已将其端口做了映射，不需要修改即可直接访问。如果未做端口映射手动修改即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xdnsadmin@k8smaster:~$ kubectl -n sock-shop edit svc front-end</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">  selector:</span><br><span class="line">    name: front-end</span><br><span class="line">  sessionAffinity: None</span><br><span class="line"><span class="meta">  #</span><span class="bash"> 将ClusterIP 改成 NodePort</span></span><br><span class="line">  type: NodePort</span><br><span class="line">status:</span><br><span class="line">  loadBalancer: &#123;&#125;</span><br></pre></td></tr></table></figure><p>如果加购物车功能够跑通，就说明集群搭建成功</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgZ1mXBQQ.png" alt="sockshop"></p><p>卸载 socks shop: <code>kubectl delete namespace sock-shop</code></p>]]></content>
    
    <summary type="html">
    
      k8s集群部署与安装
    
    </summary>
    
      <category term="k8s" scheme="https://louielong.github.io/categories/k8s/"/>
    
    
      <category term="k8s" scheme="https://louielong.github.io/source/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Compass4nfv 安装 OPNFV Gambia</title>
    <link href="https://louielong.github.io/using-compass4nfv-install-opnfv.html"/>
    <id>https://louielong.github.io/using-compass4nfv-install-opnfv.html</id>
    <published>2019-01-23T07:00:29.000Z</published>
    <updated>2019-07-01T03:56:04.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-前言"><a href="#一-前言" class="headerlink" title="一 前言"></a>一 前言</h2><p>OPNFV社区保持着每半年发布一个大版本的劲头，在2018年11月发布了第7个版本 Gambia，在观察社区中各个安装软件后，由于compass项目会支持较多的部署策略及新特性[1]，如k8s等。此外，compass也是少有的<em>支持最少一个计算节点和一个控制节点</em>的OPNFV部署工具。本次尝试使用compass4nfv来部署OPNFV的Gambia版本。在部署过程中也遇到了很多的坑，同样的官方的部署指导文档总会在很自然的忽略掉一些很关键的点，这里选择记录一下部署安装以及排错的过程。</p><p>Ps：compass4nfv的部署真的是一波N多折，修复一个问题又出现另一个问题，时不时的网络下载失败让人崩溃至极。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgFix_bug.gif" alt="fix bug"></p><h2 id="二-软件准备"><a href="#二-软件准备" class="headerlink" title="二 软件准备"></a>二 软件准备</h2><h3 id="2-1-克隆代码"><a href="#2-1-克隆代码" class="headerlink" title="2.1 克隆代码"></a>2.1 克隆代码</h3><p>首先需要克隆compass4nfv的代码，链接地址如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://gerrit.opnfv.org/gerrit/compass4nfv</span><br></pre></td></tr></table></figure><p>随后切换到Gambia的稳定分支版本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b stable/gambia</span><br></pre></td></tr></table></figure><h3 id="2-2-离线部署软件镜像准备"><a href="#2-2-离线部署软件镜像准备" class="headerlink" title="2.2 离线部署软件镜像准备"></a>2.2 离线部署软件镜像准备</h3><p>Compass4nfv支持离线部署模式，离线部署需要预先下载以下镜像系统，链接为：<a href="https://artifacts.opnfv.org/compass4nfv.html" target="_blank" rel="noopener">https://artifacts.opnfv.org/compass4nfv.html</a></p><p>然后选定相应的版本镜像即可，这里选择<a href="https://artifacts.opnfv.org/compass4nfv/gambia/opnfv-2018-11-19_08-25-04.tar.gz" target="_blank" rel="noopener">compass4nfv/gambia/opnfv-2018-11-19_08-25-04.tar.gz</a></p><p>准备好以上安装文件后，修改部署配置文件</p><p>1) 部署脚本配置</p><p><code>compass4nfv/deploy.sh</code>中指明镜像路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export TAR_URL=file:///home/opnfv/opnfv-2018-11-19_08-25-04.tar.gz</span><br></pre></td></tr></table></figure><p>接下来就要进行相应网络及节点的配置。</p><h2 id="三-安装部署"><a href="#三-安装部署" class="headerlink" title="三 安装部署"></a>三 安装部署</h2><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgGhF8HQE.gif" alt="水滴特效"></p><p>不同于Fuel的MAAS和saltstack部署，Compass使用的是cobbler和ansible部署，不了解<a href="http://cobbler.github.io/" target="_blank" rel="noopener">cobbler</a>的可以查阅下相关资料。不同版本的安装可能会稍有不同，请以官方指导手册为准。</p><h3 id="3-1-节点配置"><a href="#3-1-节点配置" class="headerlink" title="3.1 节点配置"></a>3.1 节点配置</h3><p>部署中可以参考代码仓库中默认的其他节点网络及相应配置<code>compass4nfv/deploy/conf/hardware_environment</code></p><p>下面分别详细介绍网络配置<code>network.yml</code>和部署策略<code>os-nosdn-nofeature-ha.yml</code></p><h4 id="3-1-1-网络配置文件"><a href="#3-1-1-网络配置文件" class="headerlink" title="3.1.1  网络配置文件"></a>3.1.1  网络配置文件</h4><p>部署中的节点网络配置都在此文件进行描述，实际的硬件网络拓扑可以参看先前的Fuel部署，依旧是第一块网卡作为PXE，第二块网卡作为External网络，第三块网卡作为私有网络通信，承载存储、管理、虚机间通信等，需要注意的是以上的所有的网卡都可以合一，即最少一块网卡也能部署。下面的网络配置字面意思也很好理解，这里不做过多介绍了，详细介绍参考官方<a href="https://opnfv-compass4nfv.readthedocs.io/en/stable-gambia/release/installation/configure-network.html" target="_blank" rel="noopener">Configure network</a>，PXE网络设置为<code>10.20.0.0/24</code>（后续安装中发现管理网络也在这个网段），外网设置为<code>192.168.20.0/24</code>，需要额外指明的是租户网络和存储网络配置的有vlan_tag，必须在交换机上配置好。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">opnfv@ubuntu:~/compass4nfv/deploy/conf/hardware_environment/bii-pod1$ cat network.yml</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">nic_mappings: []</span><br><span class="line">bond_mappings: []</span><br><span class="line"></span><br><span class="line">provider_net_mappings:</span><br><span class="line">  - name: br-provider #必须为此名否则后续安装ovs配置网桥会报错</span><br><span class="line">    network: physnet</span><br><span class="line">    interface: eth1</span><br><span class="line">    type: ovs</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line"></span><br><span class="line">sys_intf_mappings:</span><br><span class="line"><span class="meta">  #</span><span class="bash"> PXE Network</span></span><br><span class="line">  - name: mgmt</span><br><span class="line">    interface: eth0</span><br><span class="line">    type: normal</span><br><span class="line">    vlan_tag: None</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line">      - compute</span><br><span class="line"></span><br><span class="line">  - name: tenant</span><br><span class="line">    interface: eth2</span><br><span class="line">    type: normal</span><br><span class="line">    vlan_tag: 101</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line">      - compute</span><br><span class="line"></span><br><span class="line">  - name: storage</span><br><span class="line">    interface: eth2</span><br><span class="line">    type: normal</span><br><span class="line">    vlan_tag: 102</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line">      - compute</span><br><span class="line"></span><br><span class="line">  - name: external</span><br><span class="line">    interface: eth1</span><br><span class="line">    type: normal</span><br><span class="line">    vlan_tag: None</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line">      - compute</span><br><span class="line"></span><br><span class="line">ip_settings:</span><br><span class="line">  - name: mgmt</span><br><span class="line">    ip_ranges:</span><br><span class="line">      - - "10.20.0.50"</span><br><span class="line">        - "10.20.0.100"</span><br><span class="line">    dhcp_ranges:</span><br><span class="line">      - - "10.20.0.10"</span><br><span class="line">        - "10.20.0.49"</span><br><span class="line">    cidr: "10.20.0.0/24"</span><br><span class="line">    gw: "10.20.0.1"</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line">      - compute</span><br><span class="line"></span><br><span class="line">  - name: tenant</span><br><span class="line">    ip_ranges:</span><br><span class="line">      - - "192.168.102.1"</span><br><span class="line">        - "192.168.102.50"</span><br><span class="line">    cidr: "192.168.102.0/24"</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line">      - compute</span><br><span class="line"></span><br><span class="line">  - name: storage</span><br><span class="line">    ip_ranges:</span><br><span class="line">      - - "192.168.103.1"</span><br><span class="line">        - "192.168.103.50"</span><br><span class="line">    cidr: "192.168.103.0/24"</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line">      - compute</span><br><span class="line"></span><br><span class="line">  - name: external</span><br><span class="line">    ip_ranges:</span><br><span class="line">      - - "192.168.20.10"</span><br><span class="line">        - "192.168.20.50"</span><br><span class="line">    cidr: "192.168.20.0/24"</span><br><span class="line">    gw: "192.168.20.1"</span><br><span class="line">    role:</span><br><span class="line">      - controller</span><br><span class="line">      - compute</span><br><span class="line"></span><br><span class="line">internal_vip:</span><br><span class="line">  ip: 10.20.0.100</span><br><span class="line">  netmask: "24"</span><br><span class="line">  interface: mgmt</span><br><span class="line"></span><br><span class="line">public_vip:</span><br><span class="line">  ip: 192.168.20.103</span><br><span class="line">  netmask: "24"</span><br><span class="line">  interface: external</span><br><span class="line"></span><br><span class="line">onos_nic: eth2</span><br><span class="line">tenant_net_info:</span><br><span class="line">  type: vxlan</span><br><span class="line">  range: "1000:1050"</span><br><span class="line">  provider_network: None</span><br><span class="line"></span><br><span class="line">public_net_info:</span><br><span class="line">  enable: "True"</span><br><span class="line">  network: ext-net</span><br><span class="line">  type: flat</span><br><span class="line">  segment_id: 50</span><br><span class="line">  subnet: ext-subnet</span><br><span class="line">  provider_network: physnet</span><br><span class="line">  router: router-ext</span><br><span class="line">  enable_dhcp: "False"</span><br><span class="line">  no_gateway: "False"</span><br><span class="line">  external_gw: "192.168.20.1"</span><br><span class="line">  floating_ip_cidr: "192.168.20.0/24"</span><br><span class="line">  floating_ip_start: "192.168.20.105"</span><br><span class="line">  floating_ip_end: "192.168.20.200"</span><br></pre></td></tr></table></figure><h4 id="3-1-2-策略配置文件"><a href="#3-1-2-策略配置文件" class="headerlink" title="3.1.2 策略配置文件"></a>3.1.2 策略配置文件</h4><p>该文件主要是电源管理以及针对各个节点的角色分配，详细介绍见官方<a href="https://opnfv-compass4nfv.readthedocs.io/en/stable-gambia/release/installation/bmdeploy.html" target="_blank" rel="noopener">Nodes Configuration (Bare Metal Deployment)</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">opnfv@ubuntu:~/compass4nfv/deploy/conf/hardware_environment/bii-pod1$ cat os-nosdn-nofeature-ha.yml</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">TYPE: baremetal</span><br><span class="line">FLAVOR: cluster</span><br><span class="line">POWER_TOOL: ipmitool   #电源管理工具</span><br><span class="line"></span><br><span class="line">ipmiUser: root</span><br><span class="line">ipmiVer: '2.0'</span><br><span class="line"></span><br><span class="line">hosts:</span><br><span class="line">  - name: ctl01</span><br><span class="line">    mac: '44:A8:42:1A:49:A5'</span><br><span class="line">    interfaces:</span><br><span class="line">      - eth0: '44:a8:42:14:cd:0d'</span><br><span class="line">    ipmiIp: 192.168.20.203</span><br><span class="line">    ipmiPass: admin</span><br><span class="line">    roles:</span><br><span class="line">      - controller</span><br><span class="line">      - ha</span><br><span class="line">      - ceph-adm</span><br><span class="line">      - ceph-mon</span><br><span class="line"></span><br><span class="line">  - name: ctl02</span><br><span class="line">    mac: '44:A8:42:1A:76:2C'</span><br><span class="line">    interfaces:</span><br><span class="line">      - eth0: '44:a8:42:15:1b:e6'</span><br><span class="line">    ipmiIp: 192.168.20.204</span><br><span class="line">    ipmiPass: admin</span><br><span class="line">    roles:</span><br><span class="line">      - controller</span><br><span class="line">      - ha</span><br><span class="line">      - ceph-mon</span><br><span class="line"></span><br><span class="line">  - name: ctl03</span><br><span class="line">    mac: '44:A8:42:13:D5:1B'</span><br><span class="line">    interfaces:</span><br><span class="line">      - eth0: '44:a8:42:14:fc:1a'</span><br><span class="line">    ipmiIp: 192.168.20.205</span><br><span class="line">    ipmiPass: admin</span><br><span class="line">    roles:</span><br><span class="line">      - controller</span><br><span class="line">      - ha</span><br><span class="line">      - ceph-mon</span><br><span class="line"></span><br><span class="line">  - name: cmp001</span><br><span class="line">    mac: '44:A8:42:1A:70:BE'</span><br><span class="line">    interfaces:</span><br><span class="line">      - eth0: '44:a8:42:14:ee:64'</span><br><span class="line">    ipmiIp: 192.168.20.201</span><br><span class="line">    ipmiPass: admin</span><br><span class="line">    roles:</span><br><span class="line">      - compute</span><br><span class="line">      - ceph-osd</span><br><span class="line"></span><br><span class="line">  - name: cmp002</span><br><span class="line">    mac: '44:A8:42:1A:76:26'</span><br><span class="line">    interfaces:</span><br><span class="line">      - eth0: '44:a8:42:14:cb:31'</span><br><span class="line">    ipmiIp: 192.168.20.202</span><br><span class="line">    ipmiPass: admin</span><br><span class="line">    roles:</span><br><span class="line">      - compute</span><br><span class="line">      - ceph-osd</span><br></pre></td></tr></table></figure><h4 id="3-1-3-其他配置"><a href="#3-1-3-其他配置" class="headerlink" title="3.1.3 其他配置"></a>3.1.3 其他配置</h4><p>1) DNS及时区设置</p><p>考虑到网络等原因可以自定义DNS和时区，修改配置文件<code>compass4nfv/deploy/conf/compass.conf</code>中的DNS和时区，同时在这里我们也可以看到compass的web界面的账号和密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export COMPASS_DECK_PORT="5050"</span><br><span class="line"></span><br><span class="line">export COMPASS_USER_EMAIL="admin@huawei.com"</span><br><span class="line">export COMPASS_USER_PASSWORD="admin"</span><br></pre></td></tr></table></figure><p>访问安装节点的5050端口即可打开页面查看部署节点信息</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgVEG8XIG.jpg" alt="compass登录界面"></p><p>2）docker镜像配置</p><p>compass在部署过程中会安装docker并下载相应镜像，可以配置国内源加速镜像下载</p><p>修改<code>compass4nfv/deploy/prepare.sh</code>中的docker配置部分，增加中科大的docker镜像源或者其他源均可。(采用离线部署时所需的docker镜像都打包下载了，所以是否修改不影响)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    sudo cat &lt;&lt; EOF &gt; /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  "registry-mirrors": ["https://docker.mirrors.ustc.edu.cn/"],</span><br><span class="line">  "storage-driver": "devicemapper"</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="3-2-部署脚本配置"><a href="#3-2-部署脚本配置" class="headerlink" title="3.2 部署脚本配置"></a>3.2 部署脚本配置</h3><p>部署脚本为<code>deploy.sh</code>文件，主要修改的地方节选如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Set OS version <span class="keyword">for</span> target hosts</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Ubuntu16.04 or CentOS7</span></span><br><span class="line">export OS_VERSION=xenial     #指定待部署节点的系统版本</span><br><span class="line"><span class="meta">#</span><span class="bash"> Set ISO image corresponding to your code</span></span><br><span class="line">export TAR_URL=file:///home/opnfv/download/opnfv-6.2-m.tar.gz   #指定下载的系统安装文件</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> DEPLOY_HARBOR=<span class="string">"true"</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> HABOR_VERSION=<span class="string">"1.5.0"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Set hardware deploy jumpserver PXE NIC</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You need to comment out it when virtual deploy.</span></span><br><span class="line">export INSTALL_NIC=eth0  #指定PXE网卡</span><br><span class="line"><span class="meta">#</span><span class="bash"> DHA is your dha.yml<span class="string">'s path</span></span></span><br><span class="line">export DHA=/home/opnfv/compass4nfv/deploy/conf/hardware_environment/bii-pod1/os-nosdn-nofeature-ha.yml  #指定部署策略模板</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> NETWORK is your network.yml<span class="string">'s path</span></span></span><br><span class="line">export NETWORK=/home/opnfv/compass4nfv/deploy/conf/hardware_environment/bii-pod1/network.yml     #指定待部署节点的网络配置</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">export</span> OPENSTACK_VERSION=<span class="variable">$&#123;OPENSTACK_VERSION:-ocata&#125;</span></span></span><br><span class="line">export OPENSTACK_VERSION=queens  #指定部署的Openstack版本</span><br></pre></td></tr></table></figure><h3 id="3-3-部署opnfv"><a href="#3-3-部署opnfv" class="headerlink" title="3.3 部署opnfv"></a>3.3 部署opnfv</h3><p>本节会分析并记录部署中出现的问题以及排查解决办法。</p><p>进入节点的方法，首先进入到<code>compass-tasks</code>容器中，然后直接使用ssh登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">opnfv@ubuntu:~$ docker exec -it compass-tasks bash</span><br><span class="line"></span><br><span class="line">[root@compass-tasks /]# ssh root@10.20.0.53</span><br><span class="line">Warning: Permanently added '10.20.0.53' (ECDSA) to the list of known hosts.</span><br><span class="line">Welcome to Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-87-generic x86_64)</span><br><span class="line"></span><br><span class="line"> * Documentation:  https://help.ubuntu.com</span><br><span class="line"> * Management:     https://landscape.canonical.com</span><br><span class="line"> * Support:        https://ubuntu.com/advantage</span><br><span class="line">Last login: Fri Nov 23 17:23:21 2018</span><br><span class="line"></span><br><span class="line">root@cmp001:~#</span><br></pre></td></tr></table></figure><p>在后续的排错中也发现了节点可以使用密码登录，密码记录在<code>compass4nfv/deploy/adapters/cobbler/kickstarts/default16.seed</code>中的<code>root-password</code>一行，为<code>user/passwd：root/root</code>。</p><p><strong>【Note】</strong></p><p>在后续部署中出现错误可以查看对应的ansible任务<em>task</em>，首先在<code>compass4nfv/deploy/adapters/ansible</code>目录下查看对应的<code>task</code>详细执行命令，其次在<code>compass-tasks</code>的容器中的<code>/etc/ansible</code>目录下查看。</p><h4 id="3-3-1-ubuntu安装无法自动跳过NTP检测"><a href="#3-3-1-ubuntu安装无法自动跳过NTP检测" class="headerlink" title="3.3.1 ubuntu安装无法自动跳过NTP检测"></a>3.3.1 ubuntu安装无法自动跳过NTP检测</h4><p>我在安装过程中一直被ubuntu安装时的“setting up the clock”卡住，无法自动跳过，只能手动跳过，这个是ubuntu 16.04的一个bug，可能是跟配置网络的其他ntp服务器有关。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img9CJUyM5.png" alt="setting_time_check"></p><p>根据[2]修改系统安装时的prseed文件，屏蔽掉系统安装时的ntp检查，修改<code>compass4nfv/deploy/adapters/cobbler/kickstarts</code>下的<code>default16.seed</code>文件中的</p><blockquote><p>d-i clock-setup/ntp boolean true</p></blockquote><p>改成</p><blockquote><p>d-i clock-setup/ntp boolean false</p></blockquote><p><strong>【Note】</strong></p><p><em>Fuel</em>部署工具使用的是Debian系的MAAS来进行无人值守安装系统（免费版可以安装ubuntu，但是安装centos则需要付费），而<em>Compass4nfv</em>使用的是RedHat系的<em>kickstart</em>和<em>cobbler</em>来进行无人值守部署系统，深入阅读可以查看<a href="http://www.zyops.com/autoinstall-kickstart/" target="_blank" rel="noopener">KICKSTART无人值守安装</a><br>，<a href="https://www.jianshu.com/p/97ef50f06f05" target="_blank" rel="noopener">Ubuntu系统批量自动安装</a>或者其他相关资料。</p><h4 id="3-3-2-本地ubuntu源配置"><a href="#3-3-2-本地ubuntu源配置" class="headerlink" title="3.3.2 本地ubuntu源配置"></a>3.3.2 本地ubuntu源配置</h4><p>由于ubuntu的本地源使用率较高，考虑到网络带宽以及资源复用的方面，可以在本地建立ubuntu源，ubuntu源的搭建可以参考我的之前文章<a href="http://ylong.net.cn/MAAS+ubuntu-local-repo.html" target="_blank" rel="noopener">MAAS+ubuntu私有源环境搭建</a>。这里为了加快安装后系统的ubuntu软件安装，使用本地源进行软件安装。</p><p>主要修改的如下两个文件的仓库IP地址</p><p>1）compass4nfv/deploy/adapters/ansible/roles/config-compute/vars/main.yml</p><p>2）compass4nfv/deploy/adapters/ansible/roles/config-controller/vars/main.yml</p><blockquote><p>LOCAL_REPOSITORY_IP: “192.168.137.222”</p></blockquote><p>将IP地址修改为本地镜像仓库地址，Compass在部署过程中会去检测这个IP的连通性，若IP地址不通仍然使用ubuntu官方的镜像源地址，如文件<code>compass4nfv/deploy/adapters/ansible/roles/config-controller/templates/sources.list.official</code>所示。</p><h4 id="3-3-3-ubuntu源更新禁用IPv6设置"><a href="#3-3-3-ubuntu源更新禁用IPv6设置" class="headerlink" title="3.3.3 ubuntu源更新禁用IPv6设置"></a><del>3.3.3 ubuntu源更新禁用IPv6设置</del></h4><p>我的网络是带有IPv6，ubuntu在源更新时会优先走IPv6，但是IPv6网络无法更新(部署执行中报错，但是我登陆节点直接执行<code>apt update</code>却能成功，目前原因未知)只能现行禁用apt源更新走IPv6[3].</p><p>解决方法1：</p><p>增加配置文<code>件99force-ipv4</code>，加入<code>Acquire::ForceIPv4 &quot;true&quot;;</code>，配置文件放置在<code>compass4nfv/deploy/adapters/ansible/roles/config-controller/files</code></p><p>随后在<code>compass4nfv/deploy/adapters/ansible/roles/config-controller/vars/Ubuntu.yml</code>的任务中添加拷贝文件的命令</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- name:</span> <span class="string">apt</span> <span class="string">force</span> <span class="string">IPv4</span></span><br><span class="line"><span class="attr">  copy:</span></span><br><span class="line"><span class="attr">    src:</span> <span class="number">99</span><span class="string">force-ipv4</span></span><br><span class="line"><span class="attr">    dst:</span> <span class="string">/etc/apt/apt.conf.d/99force-ipv4</span></span><br></pre></td></tr></table></figure><p>解决办法2：</p><p>使用alias在apt命令后增加<code>-o Acquire::ForceIPv4=true</code>选项，修改<code>compass4nfv/deploy/adapters/ansible/roles/config-controller/vars/Ubuntu.yml</code>，在<code>name: add apt.conf</code>前添加</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- name:</span> <span class="string">add</span> <span class="string">apt-get</span> <span class="string">alias</span></span><br><span class="line"><span class="attr">  shell:</span> <span class="string">"echo "</span><span class="string">alias</span> <span class="string">apt-get='apt-get</span> <span class="bullet">-o</span> <span class="attr">Acquire::ForceIPv4=true'"</span> <span class="string">&gt;&gt;</span> <span class="string">/etc/bash.bashrc"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">- name:</span> <span class="string">add</span> <span class="string">apt.conf</span></span><br><span class="line"><span class="attr">  copy:</span></span><br><span class="line"><span class="attr">    src:</span> <span class="string">apt.conf</span></span><br><span class="line"><span class="attr">    dest:</span> <span class="string">/etc/apt/apt.conf</span></span><br><span class="line"><span class="attr">  when:</span> <span class="string">offline_deployment</span> <span class="string">is</span> <span class="string">defined</span> <span class="string">and</span> <span class="string">offline_deployment</span> <span class="string">==</span> <span class="string">"Enable"</span></span><br></pre></td></tr></table></figure><p><strong>【NOTE】</strong></p><p>同时也能注意到<code>compass/deploy/adapters/ansible/roles/config-compute/files/apt.conf</code>为了避免apt软件安装中出现<code>[y/N]</code>这样的交互，采用以下参数来配置apt安装，相关资料链接<a href="https://superuser.com/questions/164553/automatically-answer-yes-when-using-apt-get-install" target="_blank" rel="noopener">传送门</a></p><blockquote><p>APT::Get::Assume-Yes “true”;<br>APT::Get::force-yes “true”;</p></blockquote><h4 id="3-3-4-apt源更新时网络未连通"><a href="#3-3-4-apt源更新时网络未连通" class="headerlink" title="3.3.4 apt源更新时网络未连通"></a>3.3.4 apt源更新时网络未连通</h4><p>我在安装过程中出现无法ping通内网仓库IP的以及后续初始apt更新源失败的情况，原因是网卡重启后网络未连通导致，这里适当延长了网卡重启后的时间，修改<code>compass4nfv/deploy/adapters/ansible/roles/config-controller/tasks/Ubuntu.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- name:</span> <span class="string">check</span> <span class="string">apt</span> <span class="string">source</span></span><br><span class="line"><span class="attr">  shell:</span> <span class="string">"sleep 60 &amp;&amp; ping -c 2 <span class="template-variable">&#123;&#123; LOCAL_REPOSITORY_IP &#125;&#125;</span> &gt; /dev/null"</span></span><br><span class="line"><span class="attr">  register:</span> <span class="string">checkresult</span></span><br><span class="line"><span class="attr">  ignore_errors:</span> <span class="string">"true"</span></span><br></pre></td></tr></table></figure><p>后续的安装中除了多次出现无法下载<em>Openstack<em>官方的</em>Git</em><a href="https://git.openstack.org/openstack" target="_blank" rel="noopener">仓库代码</a>而报错外，并未遇到什么大的问题，而对于这个问题我暂时也没有太好的解决办法，只能进入容器内部尝试查看具体的网络错误原因。</p><p>compass4nfv在安装OPNFV采用的是lxc的容器安装的，会在每个节点生成对应服务的lxc容器，可以通过<code>lxc-attach --name  ctl02_repo_container-5df4d773</code> 进入到容器进行查看。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@ctl02:/openstack/ctl02_repo_container-5df4d773/repo/openstackgit# lxc-ls</span><br><span class="line">ctl02_aodh_container-95ac1216 ctl02_ceilometer_central_container-fe83768b ctl02_ceph-mon_container-6b5f3bdb ctl02_cinder_api_container-edae48f8</span><br><span class="line">ctl02_galera_container-4514e3cb  ctl02_glance_container-c8150d02 ctl02_gnocchi_container-add47e23 ctl02_heat_api_container-2a7289ca</span><br><span class="line">ctl02_horizon_container-c48e853e ctl02_keystone_container-b6189735   ctl02_memcached_container-33866197  ctl02_neutron_server_container-8638a6eb</span><br><span class="line">ctl02_nova_api_container-7221202a ctl02_rabbit_mq_container-e8f31a4e ctl02_repo_container-5df4d773 ctl02_rsyslog_container-97cbc806</span><br><span class="line">ctl02_tacker_container-84a1e0b9 ctl02_utility_container-d08003d6</span><br></pre></td></tr></table></figure><p>同样的，会将openstack官方的稳定版代码下载到本地的<code>/openstack/ctl02_repo_container-5df4d773/repo/openstackgit</code>对应项目目录下，而对应容器内部则是在<code>/var/www/repo/opensatck</code>目录下。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ctl02-repo-container-5df4d773:/var/www/repo/openstackgit# ls</span><br><span class="line">aodh dragonflow heat-dashboard magnum-ui networking-sfc neutron-fwaas-dashboard nova octavia-dashboard spice-html5</span><br><span class="line">ceilometer glance horizon networking-bgpvpn neutron neutron-lbaas nova-lxd rally    tacker</span><br><span class="line">cinder gnocchi ironic-ui networking-calico neutron-dynamic-routing neutron-lbaas-dashboard nova-powervm requirements tempest</span><br><span class="line">designate-dashboard heat keystone networking-odl neutron-fwaas neutron-vpnaas novnc sahara-dashboard trove-dashboard</span><br></pre></td></tr></table></figure><h4 id="3-3-5-ceph安装错误"><a href="#3-3-5-ceph安装错误" class="headerlink" title="3.3.5 ceph安装错误"></a>3.3.5 ceph安装错误</h4><p>由于国内网络原因在ceph的安装过程中会由于网络不通或者下载过慢超时等出现ceph安装出错，这里修改ceph的源[4][5]，可以选择的国内源有：</p><ul><li>阿里镜像源<a href="http://mirrors.aliyun.com/ceph" target="_blank" rel="noopener">http://mirrors.aliyun.com/ceph</a></li><li>网易镜像源<a href="http://mirrors.163.com/ceph" target="_blank" rel="noopener">http://mirrors.163.com/ceph</a></li><li>中科大镜像源<a href="http://mirrors.ustc.edu.cn/ceph" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/ceph</a></li></ul><p>这里以阿里云的源为例，各大源只有<code>rethat</code>系和<code>debian</code>系两种分类，这里的<code>ubuntu 16.04</code>使用的是<code>debian-luminous</code>的版本，查看<code>ctl</code>节点新增的apt源可以确认，后续随着安装使用的ubuntu版本升级会有所改变，需要灵活处理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@10.20.0.51</span><br><span class="line">...</span><br><span class="line">root@10.20.0.51's password:</span><br><span class="line">...</span><br><span class="line">You have new mail.</span><br><span class="line">Last login: Tue Dec 11 10:02:26 2018 from 10.20.0.1</span><br><span class="line">root@ctl02:~# lxc-attach --name ctl02_ceph-mon_container-2a24ec76</span><br><span class="line">root@ctl02-ceph-mon-container-2a24ec76:~# cat /etc/apt/sources.list.d/download_ceph_com_debian_luminous.list</span><br><span class="line"><span class="meta">#</span><span class="bash">deb http://download.ceph.com/debian-luminous xenial main</span></span><br><span class="line">deb http://mirrors.ustc.edu.cn/ceph/debian-luminous/ xenial main</span><br></pre></td></tr></table></figure><p>安装的源码修改如下:</p><p>由于这部分操作是在容器<code>compass-tasks</code>中操作的，可以在部署源码中添加修改，也可以在部署过程中直接在容器中修改</p><p>方法一：</p><p>1）新增文件</p><p>   路径为<code>compass4nfv/util/docker-compose/roles/compass/files/debian_community_repository.yml</code></p>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">opnfv@ubuntu:~/compass4nfv/util/docker-compose/roles/compass/files$ cat debian_community_repository.yml</span><br><span class="line">---</span><br><span class="line">- name: configure debian ceph community repository stable key</span><br><span class="line">  apt_key:</span><br><span class="line">    url: http://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">    state: present</span><br><span class="line"></span><br><span class="line">- name: configure debian ceph stable community repository</span><br><span class="line">  apt_repository:</span><br><span class="line">    repo: "deb http://mirrors.ustc.edu.cn/ceph/debian-luminous/ xenial main"</span><br><span class="line">    state: present</span><br><span class="line">    update_cache: yes</span><br><span class="line">  changed_when: false</span><br></pre></td></tr></table></figure><p>2）增加ansible的task</p><p>修改文件<code>compass4nfv/deploy/deploy_host.sh</code>，在<code>export AYNC_TIMEOUT=20</code>后增加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker cp \</span><br><span class="line"><span class="meta">$</span><span class="bash">COMPASS_DIR/util/docker-compose/roles/compass/files/debian_community_repository.yml \</span></span><br><span class="line">compass-tasks:/etc/ansible/roles/ceph-ansible/roles/ceph-common/tasks/installs/debian_community_repository.yml</span><br></pre></td></tr></table></figure><p>方法二：</p><p>由于部署ceph在靠后的阶段，因此可以在容器<code>compass-tasks</code>起来后直接修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:/home/opnfv/compass4nfv/deploy# docker exec -it compass-tasks bash</span><br><span class="line">[root@compass-tasks /]# cat /etc/ansible/roles/ceph-ansible/roles/ceph-common/tasks/installs/debian_community_repository.yml</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">- name: configure debian ceph community repository stable key</span><br><span class="line">  apt_key:</span><br><span class="line">    url: "http://mirrors.aliyun.com/ceph/keys/release.asc"</span><br><span class="line">    state: present</span><br><span class="line"></span><br><span class="line">- name: configure debian ceph stable community repository</span><br><span class="line">  apt_repository:</span><br><span class="line">    repo: "deb http://mirrors.aliyun.com/ceph/debian-luminous/ xenial main"</span><br><span class="line">    state: present</span><br><span class="line">    update_cache: yes</span><br><span class="line">  changed_when: false</span><br></pre></td></tr></table></figure><h4 id="3-3-6-无法下载HaTop"><a href="#3-3-6-无法下载HaTop" class="headerlink" title="3.3.6 无法下载HaTop"></a>3.3.6 无法下载HaTop</h4><p>依然是网络原因导致节点无法下载Hatop安装文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fatal: [ctl02 -&gt; localhost]: FAILED! =&gt; &#123;"changed": false, "failed": true, "msg": "Failed to connect to storage.googleapis.com at port 443: [      Errno 99] Cannot assign requested address"&#125;</span><br></pre></td></tr></table></figure><p>可以预选下载<a href="https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/hatop/hatop-0.7.7.tar.gz" target="_blank" rel="noopener">Hatop</a>放在本地的一个文件服务器上，直接修改<code>compass-tasks</code>中<code>/etc/ansible/roles/haproxy_server/defaults/main.yml</code>的<code>haproxy_hatop_download_url</code>为自定义的文件服务器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:~# docker exec -it compass-tasks bash</span><br><span class="line">[root@compass-tasks /]# vim /etc/ansible/roles/haproxy_server/defaults/main.yml</span><br><span class="line">...</span><br><span class="line">haproxy_hatop_download_url: "https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/hatop/hatop-0.7.7.tar.gz"</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>或者跟<code>3.3.5</code>节一样在文件<code>compass4nfv/deploy/deploy_host.sh</code>，在<code>export AYNC_TIMEOUT=20</code>后增加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker exec compass-tasks bash -c \</span><br><span class="line">"sed -i 's/https:\/\/stor[^ ]*gz/http:\/\/192.168.6.231\/download\/hatop-0.7.7.tar.gz/' /etc/ansible/roles/haproxy_server/defaults/main.yml"</span><br></pre></td></tr></table></figure><h4 id="3-3-7-部署超时"><a href="#3-3-7-部署超时" class="headerlink" title="3.3.7 部署超时"></a>3.3.7 部署超时</h4><p>compass在部署时设定了超时时间为300分钟即5个小时，但是由于由内网络下载等原因会导致部署超时，出现如下错误</p><blockquote><p>Traceback (most recent call last):<br>File “/home/opnfv/compass4nfv/deploy/client.py”, line 1136, in <module><br> main()<br>File “/home/opnfv/compass4nfv/deploy/client.py”, line 1131, in main<br> deploy()<br>File “/home/opnfv/compass4nfv/deploy/client.py”, line 1086, in deploy<br> client.get_installing_progress(cluster_id, ansible_print)<br>File “/home/opnfv/compass4nfv/deploy/client.py”, line 1029, in get_installing_progress<br> _get_installing_progress()<br>File “/home/opnfv/compass4nfv/deploy/client.py”, line 1026, in _get_installing_progress<br> raise RuntimeError(“installation timeout”)<br>RuntimeError: installation timeout</module></p></blockquote><p>我在部署时其时长都会超过300分钟，因此这里加大部署超时时间，修改<code>compass4nfv/deploy/conf/baremetal.conf</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export DEPLOYMENT_TIMEOUT="1000"</span><br></pre></td></tr></table></figure><h4 id="3-3-8-openstack官方仓库clone失败"><a href="#3-3-8-openstack官方仓库clone失败" class="headerlink" title="3.3.8 openstack官方仓库clone失败"></a>3.3.8 openstack官方仓库clone失败</h4><p>compass部署过程中会从openstack的<a href="https://git.openstack.org/cgit" target="_blank" rel="noopener">官方仓库</a>下载稳定分支的代码用于部署，同样是国内网络的原因会出现clone失败的情况，如下所示当计数变为1时仍未下载完成就会部署失败。</p><blockquote><p>2018-12-20 06:49:01,341 p=33580 u=root |  FAILED - RETRYING: Wait for git clones to complete (1 retries left).<br>2018-12-20 06:49:06,574 p=33580 u=root |  FAILED - RETRYING: Wait for git clones to complete (1 retries left).<br>2018-12-20 06:49:11,796 p=33580 u=root |  FAILED - RETRYING: Wait for git clones to complete (1 retries left).</p></blockquote><p>上述错误并不是每次都会出现，为了简单解决我选择将下载成功的代码打包备份到Master部署节点，随后在安装过程中拷贝至控制节点解压，<del>需要等到控制节点上的<code>ctl02_repo_container-xxxx</code>容器（由于ctl02是默认的主控制节点，因此只用传给ctl02即可）创建成功后才行</del>，这里将拷贝和解压直接集成到部署代码中。</p><p>压缩代码，将<code>repo</code>目录下的其他文件夹删除只留下openstackgit，随后将整个repo目录打包为<code>repo.tar.gz</code>压缩代码(800M起)。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@ctl02:/openstack/ctl02_repo_container-10dffa61/repo# ls</span><br><span class="line">links  openstackgit  os-releases  pkg-cache  pools  repo_prepost_cmd.sh  venvs</span><br><span class="line">root@ctl02:/openstack/ctl02_repo_container-10dffa61# tar czf repo.tar.gz repo</span><br><span class="line">root@ctl02:/openstack/ctl02_repo_container-10dffa61# ls -ahl repo.tar.gz</span><br><span class="line">-rw-r--r-- 1 root root 831M Jan 16 14:56 /home/opnfv/repo.tar.gz</span><br></pre></td></tr></table></figure><p>将<code>repo.tar.gz</code>上传至master节点待用，修改部署脚本，在部署中首先将<code>repo.tar.gz</code>拷贝到<code>compass-tasks</code>容器中，随后游部署脚本将其拷贝到ctl02_repo_container-xxxxx容器中解压到<code>/var/www</code>目录下</p><p>1)首先创建ansible任务，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:/home/opnfv/compass4nfv# cat deploy/adapters/ansible/customer/git_repo_pre.yml</span><br><span class="line"></span><br><span class="line">- name: scp repo.tar.gz</span><br><span class="line">  copy:</span><br><span class="line">    src: /opt/ansible_plugins/repo.tar.gz</span><br><span class="line">    dest: /root/repo.tar.gz</span><br><span class="line">    mode: 0644</span><br><span class="line">  ignore_errors: yes</span><br><span class="line"></span><br><span class="line">- name: ungzip repo.tar.gz</span><br><span class="line">  shell:</span><br><span class="line">    tar xf /root/repo.tar.gz -C /var/www/ --remove-files</span><br><span class="line">  ignore_errors: yes</span><br></pre></td></tr></table></figure><p>2)修改部署脚本</p><p>在文件<code>compass4nfv/deploy/deploy_host.sh</code>，在<code>export AYNC_TIMEOUT=20</code>后增加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> git repo</span></span><br><span class="line">cp -a /home/opnfv/repo.tar.gz /home/opnfv/compass4nfv/work/deploy/docker/ansible_plugins</span><br><span class="line">docker cp \</span><br><span class="line"><span class="meta">$</span><span class="bash">COMPASS_DIR/deploy/adapters/ansible/customer/git_repo_pre.yml \</span></span><br><span class="line">compass-tasks:/etc/ansible/roles/repo_build/tasks/git_repo_pre.yml</span><br><span class="line">docker exec compass-tasks bash -c \</span><br><span class="line">"sed -i '/Create package directories/i\- include: git_repo_pre.yml' /etc/ansible/roles/repo_build/tasks/repo_build_prepare.yml"</span><br></pre></td></tr></table></figure><h4 id="3-3-9-修改节点external网络的DNS"><a href="#3-3-9-修改节点external网络的DNS" class="headerlink" title="3.3.9 修改节点external网络的DNS"></a>3.3.9 修改节点external网络的DNS</h4><p>打开文件<code>deploy/adapters/ansible/roles/config-compute/templates/ifcfg-br-external</code>，修改dns配置即可，比如使用国内阿里云的dns，同理修改控制节点文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DNS1=223.5.5.5</span><br><span class="line">DNS2=8.8.8.8</span><br></pre></td></tr></table></figure><h4 id="3-3-10-lxc镜像下载失败"><a href="#3-3-10-lxc镜像下载失败" class="headerlink" title="3.3.10 lxc镜像下载失败"></a>3.3.10 lxc镜像下载失败</h4><blockquote><p>2018-12-21 02:46:31,836 p=4989 u=root |  FAILED - RETRYING: Ensure image has been pre-staged (1 retries left).<br>2018-12-21 02:46:36,923 p=4989 u=root |  FAILED - RETRYING: Ensure image has been pre-staged (1 retries left).<br>2018-12-21 02:46:37,015 p=4989 u=root |  FAILED - RETRYING: Ensure image has been pre-staged (1 retries left).<br>2018-12-21 02:46:37,017 p=4989 u=root |  FAILED - RETRYING: Ensure image has been pre-staged (1 retries left).</p></blockquote><p>又是一个网络引起的部署错误，每次部署都会去下载最新的<code>ubuntu lxc</code>镜像，由于网络原因，有时能够下载成功，有时下载失败，如图所示的小水管，虽然只有70M，但是也会出现无法在限定时间300s内下载完成</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgHk9mbqz.png" alt="lxc镜像下载"></p><p>这里使用<a href="https://mirrors.tuna.tsinghua.edu.cn/lxc-images/images/ubuntu/xenial/amd64/default/" target="_blank" rel="noopener">清华源的lxc镜像</a>，其会自动同步LXC官方镜像，并保持相同的链接格式(对链接格式感兴趣的可以看3.3.10.1小节)，镜像地址在<code>compass-tasks</code>镜像的<code>/etc/ansible/roles/lxc_hosts/defaults/main.yml</code>第160行。</p><p>由于部署工具会依据官方的镜像列表筛选出符合要求的最新镜像，</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aria2c --max-connection-per-server=4 --allow-overwrite=true --dir=/tmp --out=index-system --check-certificate=true  https://mirrors.tuna.tsinghua.edu.cn/lxc-images/meta/1.0/index-system</span><br></pre></td></tr></table></figure><p>这里在容器启动后修改，同样是修改文件<code>compass4nfv/deploy/deploy_host.sh</code>，在<code>export AYNC_TIMEOUT=20</code>后增加以下内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker exec compass-tasks bash -c \</span><br><span class="line">"sed -i 's/us.images.linuxcontainers.org/mirrors.tuna.tsinghua.edu.cn\/lxc-images/' /etc/ansible/roles/lxc_hosts/defaults/main.yml"</span><br></pre></td></tr></table></figure><p>修改后的测试，如图所示，直接从80K/s飙到了9.4M/s</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgqgny2Yv.png" alt="清华源LXC镜像下载"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aria2c --max-connection-per-server=4 --allow-overwrite=true --dir=/tmp/test --out=rootfs.tar.xz --check-certificate=true https://mirrors.tuna.tsinghua.edu.cn/lxc-images/images/ubuntu/xenial/amd64/default/20190106_07:43/rootfs.tar.xz</span><br></pre></td></tr></table></figure><h5 id="3-3-10-1-LXC官方镜像下载链接格式说明"><a href="#3-3-10-1-LXC官方镜像下载链接格式说明" class="headerlink" title="3.3.10.1 LXC官方镜像下载链接格式说明"></a>3.3.10.1 LXC官方镜像下载链接格式说明</h5><p>由于LXC官方会更新最新的镜像，因此下载的链接会稍有不同，LXC镜像链接：<a href="https://us.images.linuxcontainers.org/" target="_blank" rel="noopener">Linux Containers - Image server</a></p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgkNMLCVd.png" alt="LXC ubuntu镜像列表"></p><p>以<code>ubuntu;xenial;amd64;default;20190106_07:43;</code>为例，其下载链接组成为<code>ubuntu/xenial/amd64/default/20190106_07:43</code>，完整的下载地址为：</p><p><code>https://us.images.linuxcontainers.org/images/ubuntu/xenial/amd64/default/20190106_07:43/rootfs.tar.xz</code></p><h4 id="3-3-11-lxc网桥断开连接"><a href="#3-3-11-lxc网桥断开连接" class="headerlink" title="3.3.11 lxc网桥断开连接"></a>3.3.11 lxc网桥断开连接</h4><p>在安装的后期阶段由于external网络从linux bridge切换到ovs会出现lxc容器无法连接网络而导致安装失败，这里采用lxc网络自检工具定时检测网络状态，在lxc容器创建完成后，分别在<strong>三个控制节点</strong>添加定时任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@ctl02:~# crontab -e</span><br><span class="line"></span><br><span class="line">*/3 * * * * /bin/echo -n "$(date +"\%F \%T") " &gt;&gt; /tmp/lxc-veth-check;/usr/local/bin/lxc-veth-check &gt;&gt; /tmp/lxc-veth-check</span><br></pre></td></tr></table></figure><p>亦可以修改部署脚本自动添加lxc veth网卡检查</p><ul><li>1）创建文件<code>compass4nfv/deploy/adapters/ansible/custome/lxc_veth_check.yml</code>，写入如下内容</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- name:</span> <span class="string">Add</span> <span class="string">lxc</span> <span class="string">veth</span> <span class="string">check</span> <span class="string">cron</span> <span class="string">job</span></span><br><span class="line"><span class="attr">  cron:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">check</span> <span class="string">lxc</span> <span class="string">veth</span></span><br><span class="line"><span class="attr">    minute:</span> <span class="string">"*/3"</span></span><br><span class="line"><span class="attr">    job:</span> <span class="string">'*/3 * * * * /bin/echo -n "$(date +"\%F \%T") " &gt;&gt; /tmp/lxc-veth-check;/usr/local/bin/lxc-veth-check &gt;&gt; /tmp/lxc-veth-check'</span></span><br><span class="line"><span class="attr">    state:</span> <span class="string">present</span></span><br></pre></td></tr></table></figure><ul><li>2）修改部署脚本拷贝<code>lxc_veth_check.yml</code>并增加ansible部署</li></ul><p>修改<code>compass4nfv/deploy/deploy_host.sh</code>，在<code>export AYNC_TIMEOUT=20</code>后增加以下内容</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> lxc veth check</span></span><br><span class="line">docker cp \</span><br><span class="line"><span class="meta">$</span><span class="bash">COMPASS_DIR/deploy/adapters/ansible/custome/lxc_veth_check.yml \</span></span><br><span class="line">compass-tasks:/etc/ansible/roles/lxc_container_create/tasks/lxc_veth_check.yml</span><br><span class="line">docker exec compass-tasks bash -c \</span><br><span class="line">"echo '- include: lxc_veth_check.yml' &gt;&gt; /etc/ansible/roles/lxc_container_create/tasks/main.yml"</span><br></pre></td></tr></table></figure><h2 id="四-使用"><a href="#四-使用" class="headerlink" title="四 使用"></a>四 使用</h2><p>在费劲千辛万苦终于安装好OPNFV后下面要面对的就是如何使用，可以参考compass的使用手册[6]</p><h3 id="4-1-Dashboard登录"><a href="#4-1-Dashboard登录" class="headerlink" title="4.1 Dashboard登录"></a>4.1 Dashboard登录</h3><p>根据之前<code>network.yml</code>中配置的<code>public_vip</code>地址进行访问即可，账号密码在控制节点的<code>openrc</code>文件中，选择一个控制节点即可看到，如果没有可以去容器<code>ctl02_utility_container-xxxx</code>中查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">root@ctl02:~# ls</span><br><span class="line">openrc</span><br><span class="line">root@ctl02:~# cat openrc</span><br><span class="line"><span class="meta">#</span><span class="bash"> Ansible managed</span></span><br><span class="line">export LC_ALL=C</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> COMMON CINDER ENVS</span></span><br><span class="line">export CINDER_ENDPOINT_TYPE=internalURL</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> COMMON NOVA ENVS</span></span><br><span class="line">export NOVA_ENDPOINT_TYPE=internalURL</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> COMMON OPENSTACK ENVS</span></span><br><span class="line">export OS_ENDPOINT_TYPE=internalURL</span><br><span class="line">export OS_INTERFACE=internalURL</span><br><span class="line">export OS_USERNAME=admin</span><br><span class="line">export OS_PASSWORD='292f46714fed1f472e885efbbbb8b8f0a459ccea'</span><br><span class="line">export OS_PROJECT_NAME=admin</span><br><span class="line">export OS_TENANT_NAME=admin</span><br><span class="line">export OS_AUTH_URL=http://10.20.0.100:5000/v3</span><br><span class="line">export OS_NO_CACHE=1</span><br><span class="line">export OS_USER_DOMAIN_NAME=Default</span><br><span class="line">export OS_PROJECT_DOMAIN_NAME=Default</span><br><span class="line">export OS_REGION_NAME=RegionOne</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> For openstackclient</span></span><br><span class="line">export OS_IDENTITY_API_VERSION=3</span><br><span class="line">export OS_AUTH_VERSION=3</span><br><span class="line"></span><br><span class="line">root@ctl02:~# lxc-ls</span><br><span class="line">ctl02_aodh_container-6124c4b3               ctl02_ceilometer_central_container-27019a4c ctl02_ceph-mon_container-85c1ae54           ctl02_cinder_api_container-be31235f</span><br><span class="line">ctl02_galera_container-88babcd8             ctl02_glance_container-babbb22f             ctl02_gnocchi_container-61f866d0            ctl02_heat_api_container-56ac3d29</span><br><span class="line">ctl02_horizon_container-68362989            ctl02_keystone_container-29eec14f           ctl02_memcached_container-9b21550c          ctl02_neutron_server_container-0e1ade82</span><br><span class="line">ctl02_nova_api_container-60be045b           ctl02_rabbit_mq_container-301a6ec0          ctl02_repo_container-0b096846               ctl02_rsyslog_container-aa15bf1b</span><br><span class="line">ctl02_tacker_container-2d6471ad             ctl02_utility_container-bd689ebc</span><br><span class="line">root@ctl02:~# lxc-attach --name  ctl02_utility_container-bd689ebc</span><br><span class="line">root@ctl02-utility-container-bd689ebc:~# ls</span><br><span class="line">openrc</span><br></pre></td></tr></table></figure><p><strong>【未完待续】</strong></p><h2 id="【参考链接】"><a href="#【参考链接】" class="headerlink" title="【参考链接】"></a>【参考链接】</h2><p>1）<a href="https://opnfv-compass4nfv.readthedocs.io/en/stable-gambia/release/installation/index.html#compass4nfv-installation" target="_blank" rel="noopener">Compass 介绍</a></p><p>2）<a href="https://bugs.launchpad.net/ubuntu/+source/debian-installer/+bug/1558166" target="_blank" rel="noopener">ubuntu 时钟设置bug</a></p><p>3）<a href="http://forum.ubuntu.org.cn/viewtopic.php?t=474276" target="_blank" rel="noopener">ubuntu apt禁用IPv6</a></p><p>4）<a href="http://xiaoquqi.github.io/blog/2016/06/19/deploy-ceph-using-china-mirror/" target="_blank" rel="noopener">使用国内源部署ceph</a></p><p>5）<a href="http://blog.51cto.com/opencloud/1948400" target="_blank" rel="noopener">ceph国内源</a></p><p>6）<a href="https://wiki.opnfv.org/display/compass4nfv/Containerized+Compass" target="_blank" rel="noopener">compass部署opnfv使用手册</a></p>]]></content>
    
    <summary type="html">
    
      使用compass4nfv安装opnfv Gambia版本
    
    </summary>
    
      <category term="OPNFV" scheme="https://louielong.github.io/categories/OPNFV/"/>
    
    
      <category term="OPNFV" scheme="https://louielong.github.io/source/tags/OPNFV/"/>
    
      <category term="compass4nfv" scheme="https://louielong.github.io/source/tags/compass4nfv/"/>
    
  </entry>
  
  <entry>
    <title>IETF介绍及RFC Draft撰写</title>
    <link href="https://louielong.github.io/How_to_write_RFC_draft.html"/>
    <id>https://louielong.github.io/How_to_write_RFC_draft.html</id>
    <published>2018-12-27T09:15:34.000Z</published>
    <updated>2019-12-17T06:17:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>最近由于工作需要，要写一篇RFC draft，谨以此文记录一下过程中查到的资料以及相关工作。</p><p>在通信和计算机行业一谈到标准提到最多的就是RFC，没接触RFC之前一直不明白RFC到底是什么。RFC的英文是“Request for Comments”，即“请求建议”，当某家机构或团体开发出了一套标准或提出对某种标准的设想，想要征询外界的意见时，就会在Internet上发放一份RFC，对这一问题感兴趣的人可以阅读该RFC并提出自己的意见；绝大部分网络标准的指定都是以RFC的形式开始，经过大量的论证和修改过程，由主要的标准化组织所指定的，但在RFC中所收录的文件并不都是正在使用或为大家所公认的，也有很大一部分只在某个局部领域被使用或并没有被采用，一份RFC具体处于什么状态都在文件中作了明确的标识。</p><p><strong>IETF</strong>互联网工程任务小组（<strong>英语：Internet Engineering Task Force</strong>）负责互联网标准的开发和推动。它的组织形式主要是大量负责特定议题的工作组，每个都有一个指定主席（或者若干副主席）。工作组再用主题组织为<strong>领域</strong>（area）；每个领域都有一个<strong>领域指导</strong>（area director，AD），大多数领域还有两个副AD；AD任命工作组主席。AD和IETF主席构成<strong>Internet Engineering Steering Group</strong>（IESG），负责IETF的整体运作。[1]</p><p>关于IETF的更多详细介绍可以查看《IETF之道》[2][3]，里面详细介绍了IETF的运作及相关组织架构。下面摘录一下专业术语如下：</p><table><thead><tr><th>术语</th><th>含义</th><th>Meaning</th></tr></thead><tbody><tr><td>AD</td><td>领域负责人</td><td>Area Director</td></tr><tr><td>BCP</td><td>当前最佳实践</td><td>Best Current Practice</td></tr><tr><td>BOF</td><td>专题讨论会</td><td>Birds of a Feather</td></tr><tr><td>FAQ</td><td>常见问题</td><td>Frequently Asked Question(s)</td></tr><tr><td>FYI</td><td>仅供参考（RFC）</td><td>For Your Information (RFC)</td></tr><tr><td>IAB</td><td>互联网架构委员会</td><td>Internet Architecture Board</td></tr><tr><td>IAD</td><td>IETF行政主管</td><td>IETF Administrative Director</td></tr><tr><td>IANA</td><td>互联网号码分配机构</td><td>Internet Assigned Numbers Authority</td></tr><tr><td>IAOC</td><td>IETF行政监督委员会</td><td>IETF Administrative Oversight Committee</td></tr><tr><td>IASA</td><td>IETF行政支持活动</td><td>IETF Administrative Support Activity</td></tr><tr><td>ICANN</td><td>互联网名称与数字地址分配机构</td><td>Internet Corporation for Assigned Names and Numbers</td></tr><tr><td>I-D</td><td>互联网草案</td><td>Internet-Draft</td></tr><tr><td>IESG</td><td>互联网工程指导组</td><td>Internet Engineering Steering Group</td></tr><tr><td>IETF</td><td>互联网工程任务组</td><td>Internet Engineering Task Force</td></tr><tr><td>IPR</td><td>知识产权</td><td>Intellectual property rights</td></tr><tr><td>IRTF</td><td>互联网研究任务组</td><td>Internet Research Task Force</td></tr><tr><td>ISOC</td><td>互联网协会</td><td>Internet Society</td></tr><tr><td>RFC</td><td>IETF正式发布的文稿名称</td><td>Request for Comments</td></tr><tr><td>STD</td><td>标准（RFC）</td><td>Standard (RFC)</td></tr><tr><td>WG</td><td>工作组</td><td>Working Group</td></tr></tbody></table><h2 id="二、撰写RFC"><a href="#二、撰写RFC" class="headerlink" title="二、撰写RFC"></a>二、撰写RFC</h2><p>在撰写RFC之前需要先了解一下RFC的流程，首先需要撰写一个符合IETF格式的文档xml、txt、pdf格式都可以，但是推荐使用xml格式，后续借助xml2rfc能够更好的转换成标准格式草案。</p><h3 id="2-1-RFC流程"><a href="#2-1-RFC流程" class="headerlink" title="2.1 RFC流程"></a>2.1 RFC流程</h3><p>RFC处理过程：一个RFC文件在成为官方标准前一般至少要经历三个阶段：建议标准、草案标准、因特网标准。</p><blockquote><p>第一步RFC的出版是作为一个Internet草案发布，可以阅读并对其进行注释。准备一个RFC草案，我们要求作者先阅读IETF的一个文档”Considerations for  Internet Drafts”. 它包括了许多关于RFC以及Internet草案格式的有用信息。作者还应阅读另外一个相关的文档RFC 2223 “Instructions to Authors”一旦文档有了一个ID号后，你就可以向<a href="mailto:rfc-editor@rfc-editor.org" target="_blank" rel="noopener">rfc-editor@rfc-editor.org</a>发送e-mail，说你觉得这个文档还可以，能够作为一个有价值或有经验的RFC文档。RFC编辑将会向IESG请求查阅该文档并给其加上评论和注释。你可以通过RFC队列来了解你的文档的进度。一旦你的文档获得通过，RFC编辑就会将其编辑并出版。如果该文档不能出版，则会有email通知作者是什么原因。作者有48个小时来校对RFC编辑的意见。我们强烈建议作者要检测拼写错误和丢字的错误，应该确保有引用，联系和更新相关的信息。如你的文档是一个MIB，我们则要你对你的代码作最后一次检测。一旦RFC文档出版，我们就不会对其进行更改，因此你应该对你的文档仔细的检查。有时个别的文档会被正从事同一个项目的IETF工作组收回，如是这种情况，则该作者会被要求和IETF进行该文档的开发。在IETF中, Area  Directors (ADs) 负责相关的几个工作组。这些工作者所开发的文档将由ADs进行校阅，然后才作为RFC的出版物。如要获得关于如何写RFC文档和关于RFC的Internet标准制定过程的更多详细信息，请各位参见：<a href="https://tools.ietf.org/html/rfc2223" target="_blank" rel="noopener">RFC 2223</a> “Instructions to RFC Authors”。</p></blockquote><p>流程图如下：</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgUTjMKtQ.jpg" alt="RFC处理流程"></p><h3 id="2-2-RFC种类"><a href="#2-2-RFC种类" class="headerlink" title="2.2 RFC种类"></a>2.2 RFC种类</h3><p>撰写草案并希望草案成为IETF标准的人必须阅读<a href="https://tools.ietf.org/html/rfc2026" target="_blank" rel="noopener">BCP9</a>，以便能够跟进其文档在整个过程中的进展情况。您可以在IETF Datatracker上跟踪文档进展情况，网址为<a href="http://datatracker.ietf.org" target="_blank" rel="noopener">http://datatracker.ietf.org</a>。 BCP9（以及对它进行更新的各种其他文档）对常常被人误解、甚至是被资深IETF参与者误解的话题进行详细论述： 不同类型的RFC将经历不同的流程并有不同的排名。有六种RFC：</p><table><thead><tr><th>英文</th><th>中文</th></tr></thead><tbody><tr><td>Proposed standards</td><td>建议的标准</td></tr><tr><td>Internet standards (sometimes called “full standards”)</td><td>互联网标准（有时称为“完全标准”）</td></tr><tr><td>Best current practices (BCP) documents</td><td>当前最佳实践 (BCP) 文档</td></tr><tr><td>Informational documents</td><td>信息性文档</td></tr><tr><td>Experimental documents</td><td>实验性文档</td></tr><tr><td>Historic documents</td><td>历史性文档</td></tr></tbody></table><p><strong>[NOTE]</strong>Only the first two, proposed and full, are standards within the IETF. A good summary of this can be found in the aptly titled [<a href="https://datatracker.ietf.org/doc/rfc1796" target="_blank" rel="noopener">RFC 1796</a>], “Not All RFCs Are Standards”.</p><p><strong>[备注]</strong>只有前面两种标准（建议的标准和完全标准）属于IETF内的标准。在题为《并非所有RFC都是标准》<a href="http://www6.ietf.org/tao-translated-zh.html#RFC1796" target="_blank" rel="noopener">RFC 1796</a>的文档中可找到相关摘要。</p><h3 id="2-3-如何粗暴的撰写一篇IETF-Internet-Draft"><a href="#2-3-如何粗暴的撰写一篇IETF-Internet-Draft" class="headerlink" title="2.3 如何粗暴的撰写一篇IETF Internet Draft"></a>2.3 如何粗暴的撰写一篇IETF Internet Draft</h3><p>撰写一篇标准的IETF draft除了需要对想要撰写的Draft内容有规划外还需要了解Draft提交的标准格式，但是其相关内容繁杂且啰嗦(如果想优雅的撰写一篇RFC Draft请参阅<a href="#jump0">如何优雅的撰写一篇RFC Dratf</a>)，本着太长不看原则，这里使用简单“粗暴”的方式完成一篇标准格式的Draft就是接下来要介绍的(ps.这里的粗暴仅只将Draft格式转换为IETF的RFC格式)</p><p>首先需要参照标准的RFC格式自行完善想要写的RFC draft内容可以是word格式，主要包括：摘要（Abstract）、说明（Introduction）、正文(Content)、引用（Reference）、作者信息（Author’s Address）、致谢（Acknowledgement）等。需要额外指出的是作图必须用ASCII码的形式画图，内容必须是<strong>全英文</strong>，尤其是<strong>标点符号</strong>这点会在xml转换成rfc时尤为重要。</p><h4 id="2-3-1-xml格式套用"><a href="#2-3-1-xml格式套用" class="headerlink" title="2.3.1 xml格式套用"></a>2.3.1 xml格式套用</h4><p>选择一份RFC draft的xml格式作为模板将写好的文档套用进去，如使用<a href="https://www.ietf.org/id/draft-wang-nfvrg-network-slice-diverse-standards-00.xml" target="_blank" rel="noopener">draft-wang-nfvrg-network-slice-diverse-standards-00.xml</a>，将其保存在本地，推荐使用<code>notepad++</code>或者<code>sublime txt</code>打开，当然如果你熟练使用<code>记事本</code>也没问题，关于xml格式的官方说明见<a href="http://xml2rfc.tools.ietf.org/rfc7749.html" target="_blank" rel="noopener">RFC7749</a>，下面做简要说明。</p><h5 id="2-3-1-1-xml模板头"><a href="#2-3-1-1-xml模板头" class="headerlink" title="2.3.1.1 xml模板头"></a>2.3.1.1 xml模板头</h5><p>以下部分为xml模板头文件，除了文档名<code>docName</code>其他不需要更改。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="US-ASCII"?&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- This is built from a template for a generic Internet Draft. Suggestions for</span></span><br><span class="line"><span class="comment">     improvement welcome - write to Brian Carpenter, brian.e.carpenter @ gmail.com</span></span><br><span class="line"><span class="comment">     This can be converted using the Web service at http://xml.resource.org/ --&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE rfc SYSTEM "rfc2629.dtd"&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- You want a table of contents --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Use symbolic labels for references --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- This sorts the references --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- Change to "yes" if someone has disclosed IPR for the draft --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- This defines the specific filename and version number of your draft (and inserts the appropriate IETF boilerplate --&gt;</span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>rfc sortrefs=<span class="string">"yes"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>rfc toc=<span class="string">"yes"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>rfc symrefs=<span class="string">"yes"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>rfc compact=<span class="string">"yes"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>rfc subcompact=<span class="string">"no"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>rfc topblock=<span class="string">"yes"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="php"><span class="meta">&lt;?</span>rfc comments=<span class="string">"no"</span><span class="meta">?&gt;</span></span></span><br><span class="line"><span class="tag">&lt;<span class="name">rfc</span> <span class="attr">category</span>=<span class="string">"info"</span></span></span><br><span class="line"><span class="tag">     <span class="attr">docName</span>=<span class="string">"draft-long-nfv-nfv-decoupling-test-00"</span></span></span><br><span class="line"><span class="tag">     <span class="attr">ipr</span>=<span class="string">"trust200902"</span>&gt;</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgW1fXOks.png" alt="IETF Draft xml template"></p><h5 id="2-3-1-2-Draft文档命名"><a href="#2-3-1-2-Draft文档命名" class="headerlink" title="2.3.1.2 Draft文档命名"></a>2.3.1.2 Draft文档命名</h5><p>Draft的命名遵循如下的格式</p><blockquote><p>draft name: draft-’authorname’-‘groupname’-‘drafttopic’-‘version’.xml<br>  (example: draft-long-nfvrg-nfv-decoupling-test-00.xml</p></blockquote><ul><li>draft：表明这只是一个draft草案</li><li>authorname：作者名，通常是第一作者的姓氏</li><li>groupname：想提交草案审核讨论的工作组</li><li>drafttopic：草案核心话题内容，多个关键词之间用<code>-</code>间隔</li><li>version：版本号，初始版本号为<code>00</code>，随着后续的改进，版本号逐次增加</li></ul><h5 id="2-3-1-3-作者信息"><a href="#2-3-1-3-作者信息" class="headerlink" title="2.3.1.3 作者信息"></a>2.3.1.3 作者信息</h5><p>作者的信息，如下所示基本都是正常填空即可</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">author</span> <span class="attr">fullname</span>=<span class="string">"xxx"</span> <span class="attr">initials</span>=<span class="string">"Y."</span> <span class="attr">surname</span>=<span class="string">"Long"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">organization</span>&gt;</span>xxxx<span class="tag">&lt;/<span class="name">organization</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">address</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">postal</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">street</span>&gt;</span>xxxxxx<span class="tag">&lt;/<span class="name">street</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">city</span>&gt;</span>Beijing<span class="tag">&lt;/<span class="name">city</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">code</span>&gt;</span>101111<span class="tag">&lt;/<span class="name">code</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">country</span>&gt;</span>P. R. China<span class="tag">&lt;/<span class="name">country</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">postal</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">email</span>&gt;</span>xxx@xxx<span class="tag">&lt;/<span class="name">email</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">address</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br></pre></td></tr></table></figure><p>效果图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgTy92Cp4.png" alt="作者信息"></p><h5 id="2-3-1-4-文档信息"><a href="#2-3-1-4-文档信息" class="headerlink" title="2.3.1.4 文档信息"></a>2.3.1.4 文档信息</h5><p>文档信息包括：</p><ul><li>date：提交日期，这个关乎到6个月草案过期时间的计算</li><li>area：领域，草案所属的领域</li><li>workgroup：草案打算提交的工作组</li><li>keyword：关键词</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">date</span> <span class="attr">day</span>=<span class="string">"26"</span> <span class="attr">month</span>=<span class="string">"Dec"</span> <span class="attr">year</span>=<span class="string">"2018"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">area</span>&gt;</span>Networking<span class="tag">&lt;/<span class="name">area</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">workgroup</span>&gt;</span>nfvrg<span class="tag">&lt;/<span class="name">workgroup</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">keyword</span>&gt;</span>NFV Decoupling Test<span class="tag">&lt;/<span class="name">keyword</span>&gt;</span></span><br></pre></td></tr></table></figure><h5 id="2-3-1-5-正文格式"><a href="#2-3-1-5-正文格式" class="headerlink" title="2.3.1.5 正文格式"></a>2.3.1.5 正文格式</h5><ul><li>文档正文，正文内容使用<code>&lt;middle&gt;xxxx &lt;/middle&gt;</code>格式</li><li>章节，使用<code>&lt;section&gt;xxx&lt;/section&gt;</code>格式即可，多级标题直接嵌套即可</li><li>段落，使用<code>&lt;t&gt;xxx&lt;/t&gt;</code>的格式，保证tag闭合即可</li><li>文章内引用，使用<code>&lt;xref target=&quot;ETSI_NFV_GS_002&quot;/&gt;</code>，需要指出的是参考文献需要显示列出该参考文献信息，否则会在xml转rfc时出现<code>warnning</code></li></ul><h5 id="2-3-1-6-列表"><a href="#2-3-1-6-列表" class="headerlink" title="2.3.1.6 列表"></a>2.3.1.6 列表</h5><p>列表格式如下所示，使用<code>&lt;list&gt; &lt;/list&gt;</code>标签</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">t</span>&gt;</span><span class="tag">&lt;<span class="name">list</span> <span class="attr">style</span>=<span class="string">"symbols"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">t</span>&gt;</span>Virtualized Network Function (VNF).<span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">t</span>&gt;</span>Element Management (EM).<span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">t</span>&gt;</span>NFV Infrastructure, including: Hardware and virtualized resources,</span><br><span class="line">        and Virtualization Layer.<span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">t</span>&gt;</span>Virtualized Infrastructure Manager(s) (VIM).<span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">t</span>&gt;</span>NFV Orchestrator.<span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">t</span>&gt;</span>VNF Manager(s).<span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">t</span>&gt;</span>Service, VNF and Infrastructure Description.</span><br><span class="line">        Operations and Business Support Systems (OSS/BSS).<span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">list</span>&gt;</span><span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br></pre></td></tr></table></figure><p>效果图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgtUEFgvZ.png" alt="ETSI NFV architecture"></p><p>对于列表的格式使用<code>style</code>标签，可选参数有</p><blockquote><p>“empty”</p><p>For unlabeled list items; it can also be used for  indentation purposes (this is the default value when there is an  enclosing list where the style is specified).</p><p>“hanging”</p><p>For lists where the items are labeled with a piece of text.</p><p>The label text is specified in the “hangText” attribute of the &lt;<a href="http://xml2rfc.tools.ietf.org/rfc7749.html#element.t" target="_blank" rel="noopener">t</a>&gt; element (<a href="http://xml2rfc.tools.ietf.org/rfc7749.html#element.t.attribute.hangText" target="_blank" rel="noopener">Section 2.38.2</a>).</p><p>“letters”</p><p>For  ordered lists using letters as labels (lowercase letters followed by a  period; after “z”, it rolls over to a two-letter format). For nested  lists, processors usually flip between uppercase and lowercase.</p><p>“numbers”</p><p>For ordered lists using numbers as labels.</p><p>“symbols”</p><p>For unordered (bulleted) lists.</p><p>The  style of the bullets is chosen automatically by the processor (some  implementations allow overriding the default using a Processing  Instruction).</p></blockquote><p>2.3.1.7 图示格式</p><p>图例格式如下所示，采用<code>&lt;figure&gt; &lt;/figure&gt;</code>标签，配合<code>&lt;artwork&gt;&lt;/artwork&gt;</code> CDATA表示直接显示的数据</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">t</span>&gt;</span><span class="tag">&lt;<span class="name">figure</span>  <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artwork</span> <span class="attr">name</span>=<span class="string">"Network Function Virtualization"</span>&gt;</span>&lt;![CDATA[</span><br><span class="line">                                   +----------------+</span><br><span class="line">                                   |    Vendor      +--+</span><br><span class="line">                                   |      C         |  |</span><br><span class="line">                                   +------+---------+  |</span><br><span class="line">                                          |            |</span><br><span class="line">                +---------------+  +------+---------+  |</span><br><span class="line">                |    Vendor     +--+    Vendor      |  |</span><br><span class="line">                |      H        |  |      H         |  |</span><br><span class="line">                +------+--------+  +------+---------+  |</span><br><span class="line">                       |                  |            |</span><br><span class="line">                +------+------------------+---------+  |</span><br><span class="line">                |              Vendor               +--+</span><br><span class="line">                |                F                  |</span><br><span class="line">                +-----------------------------------+</span><br><span class="line">              Figure 3: NFV decoupling test architecture]]&gt;<span class="tag">&lt;/<span class="name">artwork</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">figure</span>&gt;</span><span class="tag">&lt;/<span class="name">t</span>&gt;</span></span><br></pre></td></tr></table></figure><p>效果图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgDBCpFP1.png" alt="图例"></p><p>2.3.1.8 表格格式</p><p>表格格式如下所示，采用<code>&lt;texttable&gt; &lt;/texttable&gt;</code>标签，标题使用<code>&lt;ttcol&gt; &lt;/ttcol&gt;</code>,单元格使用<code>&lt;c&gt; &lt;/c&gt;</code></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">texttable</span> <span class="attr">align</span>=<span class="string">"center"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ttcol</span>&gt;</span>Testcases<span class="tag">&lt;/<span class="name">ttcol</span>&gt;</span><span class="tag">&lt;<span class="name">ttcol</span>&gt;</span>Vendor F<span class="tag">&lt;/<span class="name">ttcol</span>&gt;</span><span class="tag">&lt;<span class="name">ttcol</span>&gt;</span>Vendor H<span class="tag">&lt;/<span class="name">ttcol</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">c</span>&gt;</span>Onboard<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">c</span>&gt;</span>Instantiate<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">c</span>&gt;</span>Scale in(manual)<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">c</span>&gt;</span>Scale out(manual)<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">c</span>&gt;</span>Terminate<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span><span class="tag">&lt;<span class="name">c</span>&gt;</span>PASS<span class="tag">&lt;/<span class="name">c</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">texttable</span>&gt;</span></span><br></pre></td></tr></table></figure><p>效果图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgyhiLozn.png" alt="表格"></p><h5 id="2-3-1-7-引用"><a href="#2-3-1-7-引用" class="headerlink" title="2.3.1.7 引用"></a>2.3.1.7 引用</h5><p>正文后的标签<code>&lt;back&gt;&lt;/back&gt;</code>，填写引用文献信息，文献的格式参见<a href="https://www.rfc-editor.org/ref-example/" target="_blank" rel="noopener">Reference Examples</a>，需要给出文章名、作者、时间等信息。</p><p>1）RFC文档引用方式</p><p>如果是标准的RFC文献可以使用<a href="https://www.rfc-editor.org/rfc-index2.html" target="_blank" rel="noopener">官方参考</a>直接导出。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgVNPjSPy.png" alt="IETF Ref 示例"></p><p>格式如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="php"><span class="meta">&lt;?</span>rfc <span class="keyword">include</span>=<span class="string">"https://www.rfc-editor.org/refs/bibxml/reference.RFC.8174.xml"</span><span class="meta">?&gt;</span></span></span><br></pre></td></tr></table></figure><p>效果图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgF1yEJkc.png" alt="RFC 8174 引用效果图"></p><p>2）非RFC文档引用</p><p>非RFC文档需要手动填写相关信息，如下所示</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">reference</span> <span class="attr">anchor</span>=<span class="string">"ETSI_GS_NFV_002"</span> <span class="attr">target</span>=<span class="string">"https://www.etsi.org/deliver/etsi_gs/NFV/001_099/002/01.02.01_60/gs_NFV002v010201p.pdf"</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">front</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">title</span>&gt;</span>ETSI GS NFV 002: "Network Functions Virtualisation (NFV); Architectural Framework"<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">author</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">organization</span>&gt;</span>ETSI NFV ISG<span class="tag">&lt;/<span class="name">organization</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">author</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">date</span> <span class="attr">month</span>=<span class="string">"December"</span> <span class="attr">year</span>=<span class="string">"2014"</span>/&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">front</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">reference</span>&gt;</span></span><br></pre></td></tr></table></figure><p>效果图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img0ZBvLkX.png" alt="非RFC文档引用"></p><p>2.3.1.8 其他注意事项</p><p><strong>转义符</strong>，文章中出现的如下字符必须转义，否则在转换过程中或出错，注意字符后的<code>;</code>也是需要的</p><table><thead><tr><th>转义后字符</th><th>转义前字符</th></tr></thead><tbody><tr><td><code>&amp;amp;</code> 或 <code>&amp;#38;</code></td><td>&amp;</td></tr><tr><td><code>&amp;lt;</code> 或 <code>&amp;#60;</code></td><td>&lt;</td></tr><tr><td><code>&amp;gt;</code> 或 <code>&amp;#62;</code></td><td>&gt;</td></tr><tr><td><code>&amp;quot;</code></td><td>“</td></tr><tr><td><code>&amp;nbsp;</code></td><td>空格</td></tr><tr><td><code>&amp;copy;</code></td><td>版权符 &copy;</td></tr><tr><td><code>&amp;reg;</code></td><td>注册符 &reg;</td></tr><tr><td><strong>【Note】</strong>其中&lt;和&gt;最常用，对于I-D内容中包含的xml元素时，需要对&lt;和&gt;进行转义。</td><td></td></tr></tbody></table><p><strong>还有最重要的是务必不要出现中文字符，在后续检测中报错的很大一部分问题出在未知的中文标点符号中。</strong></p><h4 id="2-3-2-XML转RFC"><a href="#2-3-2-XML转RFC" class="headerlink" title="2.3.2 XML转RFC"></a>2.3.2 XML转RFC</h4><p>在完成以上步骤后，需要进行xml转rfc检测，检测过后的xml格式才能提交，简单点的是使用线上的<a href="http://xml2rfc.tools.ietf.org/" target="_blank" rel="noopener">xml2rfc</a>工具进行debug测试</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img4yiaq9L.png" alt="xml2rfc debug"></p><p>如果出现问题就看debug信息，最基本的错误是tag没有闭合，中文字符，引用错误等。</p><p>如果转换没有问题，则可以提交审阅了，链接：<a href="https://datatracker.ietf.org/submit/" target="_blank" rel="noopener">Internet-Draft submission</a></p><h3 id="2-4-如何优雅的撰写一篇IETF-Draft"><a href="#2-4-如何优雅的撰写一篇IETF-Draft" class="headerlink" title="2.4 如何优雅的撰写一篇IETF Draft"></a>2.4 <span id="jump0">如何优雅的撰写一篇IETF Draft</span></h3><p>想要优雅的撰写Draft，你只需要仔细阅读以下文献即可</p><ul><li><a href="https://www.ietf.org/standards/ids/" target="_blank" rel="noopener">Internet-Drafts</a>，I-D指南</li><li><a href="https://www.ietf.org/standards/ids/guidelines/" target="_blank" rel="noopener">Guidelines to Authors of Internet-Drafts</a>，撰写Draft指南</li><li><a href="http://xml2rfc.tools.ietf.org/rfc7749.html" target="_blank" rel="noopener">RFC 7749, The “xml2rfc” Version 2 Vocabular</a>，xml2rfc格式说明</li><li><a href="https://tools.ietf.org/html/rfc5385" target="_blank" rel="noopener">RFC5385</a>，介绍了如何从word文档转换为I-D RFC的标准文档格式</li><li><a href="https://www.rfc-editor.org/materials/tools_ids_rfcs_94.pdf" target="_blank" rel="noopener">IETF RFC tools</a>，RFC工具</li></ul><h2 id="【参考链接】"><a href="#【参考链接】" class="headerlink" title="【参考链接】"></a>【参考链接】</h2><p>1）<a href="https://zh.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E4%BB%BB%E5%8A%A1%E7%BB%84" target="_blank" rel="noopener">IETFwiki介绍</a></p><p>2）<a href="http://www6.ietf.org/tao-translated-zh.html#rfcs.ids" target="_blank" rel="noopener">IETF之道中文</a></p><p>3）<a href="https://www.ietf.org/about/participate/tao/" target="_blank" rel="noopener">The Tao of IETF</a></p><p>4）<a href="https://www.wxwenku.com/d/105646092" target="_blank" rel="noopener">互联网最新研究方向与IETF</a></p><p>5）<a href="http://blog.donews.com/Linyi/archive/2009/03/09/1475585.aspx" target="_blank" rel="noopener">撰写IETF Internet Draft的技巧</a></p>]]></content>
    
    <summary type="html">
    
      RFC Internet Draft 撰写
    
    </summary>
    
      <category term="IETF" scheme="https://louielong.github.io/categories/IETF/"/>
    
    
      <category term="IETF" scheme="https://louielong.github.io/source/tags/IETF/"/>
    
      <category term="RFC" scheme="https://louielong.github.io/source/tags/RFC/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 博客美化配置</title>
    <link href="https://louielong.github.io/hexo_conf.html"/>
    <id>https://louielong.github.io/hexo_conf.html</id>
    <published>2018-09-13T08:44:38.000Z</published>
    <updated>2019-07-01T05:31:59.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><p>使用Hexo一年多了，页面样式主题都是好久之前的一直没有更改过。闲来想换换新货，体验下新的代码，同时加入一些羡慕已久的页面特效，本文记录一下升级的过程。以一个新建立的blog开始记录，至于Hexo的安装则跳过了。</p><h2 id="2-初始blog"><a href="#2-初始blog" class="headerlink" title="2 初始blog"></a>2 初始blog</h2><p>为了记录自定义的修改这里预先建立的一个blog的git仓库，方便记录修改的地方。</p><p>使用Hexo命令建立一个初始blog，随后加入到仓库中。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">louie@ubuntu:~/workplace$ hexo init blog</span><br></pre></td></tr></table></figure><p>下载主题，这里使用next主题，使用v6.1.0的版本方便后续升级也便于记录修改起始的版本。</p><p><img src="https://raw.githubusercontent.com/theme-next/hexo-theme-next/master/source/images/logo.svg?sanitize=true" alt="Next Themes"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">louie@ubuntu:~/workplace/blog/themes$ git clone --branch v6.1.0 https://github.com/theme-next/hexo-theme-next.git next</span><br></pre></td></tr></table></figure><p>Next 主题官网更新后给出了一波自定义教程，总结的非常全面，可以直接参考官方的配置：<a href="http://theme-next.iissnan.com/getting-started.html#third-party-services" target="_blank" rel="noopener">传送门</a></p><h2 id="3-配置、美化blog"><a href="#3-配置、美化blog" class="headerlink" title="3 配置、美化blog"></a>3 配置、美化blog</h2><h3 id="3-1-配置-config-yml"><a href="#3-1-配置-config-yml" class="headerlink" title="3.1 配置_config.yml"></a>3.1 配置<code>_config.yml</code></h3><p>修改默认主题</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Site</span></span><br><span class="line"></span><br><span class="line"><span class="attr">title:</span> <span class="string">Louie's</span> <span class="string">Blog</span></span><br><span class="line"><span class="attr">subtitle:</span> <span class="string">O</span> <span class="string">ever</span> <span class="string">youthful,</span> <span class="string">O</span> <span class="string">ever</span> <span class="string">powerful.</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">一只会说666的程序媛鼓励师</span></span><br><span class="line"><span class="attr">author:</span> <span class="string">Louie</span> <span class="string">Long</span></span><br><span class="line"><span class="attr">language:</span> <span class="string">zh-Hans</span></span><br><span class="line"><span class="attr">timezone:</span> <span class="string">Asia/Shanghai</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"></span><br><span class="line"><span class="attr"> theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure><p>设置网站图标，在souce目录下创建一个uploads目录(因为next搜索指定了<a href="https://github.com/theme-next/hexo-theme-next/blob/master/_config.yml#L192-L195" target="_blank" rel="noopener">目录路径)</a>，将图片等都放置在这个目录同一管理。修改next主题的配置文件<code>themes/next/_config.yml</code>，avatar为图像图片，favicon为网站icon图片 ，推荐一个好用的<a href="https://tool.lu/favicon/" target="_blank" rel="noopener">ico 制作网站</a>。apple和safari的像素要高许多可以参考<a href="https://github.com/theme-next/hexo-theme-next/tree/master/source/images" target="_blank" rel="noopener">原主题ico</a>大小。</p><blockquote><p>favicon:<br>  small: /uploads/favicon-16x16.ico<br>  medium: /uploads/favicon-32x32.ico<br>  apple_touch_icon: /uploads/favicon-48x48.ico<br>  safari_pinned_tab: /uploads/favicon-48x48.ico</p><p>avatar: /uploads/shadian.png</p></blockquote><p>blog首页底部的版权信息，直接修改<code>themes/next/_config.yml</code>的<code>footer</code>内容</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="comment"># Specify the date when the site was setup.</span></span><br><span class="line">  <span class="comment"># If not defined, current year will be used.</span></span><br><span class="line"><span class="attr">  since:</span> <span class="number">2015</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Icon between year and copyright info.</span></span><br><span class="line"><span class="attr">  icon:</span></span><br><span class="line">    <span class="comment"># Icon name in fontawesome, see: https://fontawesome.com/v4.7.0/icons</span></span><br><span class="line">    <span class="comment"># `heart` is recommended with animation in red (#ff0000).</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">user</span></span><br><span class="line">    <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line"><span class="attr">     animated:</span> <span class="literal">true</span></span><br><span class="line">     <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line"><span class="attr">     color:</span> <span class="string">"#808080"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># If not defined, will be used `author` from Hexo main config.</span></span><br><span class="line"><span class="attr">   copyright:</span></span><br><span class="line">  <span class="comment"># -------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Hexo link (Powered by Hexo).</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   powered:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">   theme:</span></span><br><span class="line">     <span class="comment"># Theme &amp; scheme info link (Theme - NexT.scheme).</span></span><br><span class="line"><span class="attr">     enable:</span> <span class="literal">true</span></span><br><span class="line">     <span class="comment"># Version info of NexT after scheme info (vX.X.X).</span></span><br><span class="line"><span class="attr">     version:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>根据需求自行修改，英文也很简单易懂^-^。</p><h3 id="3-2-创建标签等页面"><a href="#3-2-创建标签等页面" class="headerlink" title="3.2 创建标签等页面"></a>3.2 创建标签等页面</h3><p>创建标签、关于、分类、归档等页面，只需要调用hexo的命令在<code>blog/source</code>目录下生成指定的目录文件即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo new page "tags"</span><br><span class="line">hexo new page "about"</span><br><span class="line">hexo new page "categories"</span><br></pre></td></tr></table></figure><p>随后即会在source目录下生成相应的目录及文件，修改对应的文件如<code>about/index.md</code>，将comments设为<code>false</code>，禁止在该页面进行评论。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat categories/index.md</span><br><span class="line"> ---</span><br><span class="line">title: 分类</span><br><span class="line">date: 日期</span><br><span class="line">type: "categories"</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>设置完成后可以通过<code>hexo g</code>和<code>hexo s</code>进行效果查看，这里不再赘述了。同时需要在主题中打开相应的开关<code>themes/next/_config.yml</code></p><blockquote><p>about: /about/ || user<br>tags: /tags/ || tags<br>categories: /categories/ || th<br>archives: /archives/ || archive<br>commonweal: /404/ || heartbeat</p></blockquote><h3 id="3-3-设置首页文章显示篇数"><a href="#3-3-设置首页文章显示篇数" class="headerlink" title="3.3 设置首页文章显示篇数"></a>3.3 设置首页文章显示篇数</h3><p>安装相关插件，可以看到<code>package.json</code>中的信息有修改，显示新加入的插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-generator-index</span><br><span class="line">npm install --save hexo-generator-archive</span><br><span class="line">npm install --save hexo-generator-tag</span><br></pre></td></tr></table></figure><p>修改全局配置文件<code>blog/_config.yml</code>，增加以下内容</p><blockquote><p>index_generator:<br>  per_page: 10</p><p>archive_generator:<br>  per_page: 20<br>  yearly: true<br>  monthly: true</p><p>tag_generator:<br>  per_page: 10</p></blockquote><p>其中<code>per_page</code>字段是期望每页显示的篇数。<code>index</code>, <code>archive</code>及<code>tag</code>开头分表代表主页，归档页面和标签页面。</p><h3 id="3-3-增加字数统计功能"><a href="#3-3-增加字数统计功能" class="headerlink" title="3.3 增加字数统计功能"></a>3.3 增加字数统计功能</h3><p>Next主题集成了<a href="https://github.com/theme-next/hexo-symbols-count-time" target="_blank" rel="noopener">Word Count 功能</a>，首先安装插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-symbols-count-time --save</span><br></pre></td></tr></table></figure><p>插件主要功能：</p><ul><li>字数统计:WordCount</li><li>阅读时长预计:Min2Read</li></ul><p>安装完插件后仅需要修改根配置文件<code>blog/_config.yml</code>即可</p><blockquote><p>symbols_count_time:<br>  symbols: true<br>  time: true<br>  total_symbols: true<br>  total_time: true</p></blockquote><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgCA9U4yU.jpg" alt="hexo word count"></p><h3 id="3-4-增加站内搜索功能"><a href="#3-4-增加站内搜索功能" class="headerlink" title="3.4 增加站内搜索功能"></a>3.4 增加站内搜索功能</h3><p>Next主题支持Google、Algolia、Swiftype三种搜索，这里选用Algolia搜索。设置方式也很多，随便搜一下就可以找到。首先需要到<a href="https://www.algolia.com/" target="_blank" rel="noopener">Algolia官网</a>进行注册，然后获得API Key填入根配置文件中。</p><p>参考<a href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/ALGOLIA-SEARCH.md" target="_blank" rel="noopener">Algolia GitHub</a>链接，安装algolia插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-algolia</span><br></pre></td></tr></table></figure><p>在<code>blog/_config.yml</code>配置文件添加如下信息</p><blockquote><p>algolia:<br>  applicationID: ‘Application ID’<br>  apiKey: ‘Search-only API key’<br>  indexName: ‘indexName’<br>  chunkSize: 5000</p></blockquote><p>将<code>Application ID</code>、<code>Search-only API key</code>以及<code>indexName</code>都替换成自己的信息。同时需要将next主题中algolia搜索开关打开</p><blockquote><p>algolia_search:</p><p> enable: true</p></blockquote><p>执行下述命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> HEXO_ALGOLIA_INDEXING_KEY=Search-Only API key <span class="comment"># Use Git Bash</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> HEXO_ALGOLIA_INDEXING_KEY=Search-Only API key <span class="comment"># Use Windows command line</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo clean</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> hexo algolia</span></span><br></pre></td></tr></table></figure><h3 id="3-5-添加首页文章以摘要显示"><a href="#3-5-添加首页文章以摘要显示" class="headerlink" title="3.5 添加首页文章以摘要显示"></a>3.5 添加首页文章以摘要显示</h3><p>开启主题的摘要功能，<code>length</code>表示显示摘要的截取字符串长度。</p><blockquote><p>auto_excerpt:<br>  enable: true<br>  length: 150</p></blockquote><h3 id="3-6-添加统计"><a href="#3-6-添加统计" class="headerlink" title="3.6 添加统计"></a>3.6 添加统计</h3><h4 id="3-6-1-文章阅读统计及热度"><a href="#3-6-1-文章阅读统计及热度" class="headerlink" title="3.6.1 文章阅读统计及热度"></a>3.6.1 文章阅读统计及热度</h4><p>如图所示的效果，看起来很酷炫</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgQjU61uL.jpg" alt="文章阅读统计"></p><p>参考<a href="https://github.com/theme-next/hexo-leancloud-counter-security" target="_blank" rel="noopener">leanclond next配置指南</a>，首先安装<code>leanclound</code>插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-leancloud-counter-security --save</span><br></pre></td></tr></table></figure><p>这里需要用到leancloud，注册及配置查考链接：<a href="https://www.jianshu.com/p/702a7aec4d00" target="_blank" rel="noopener">传送门</a>，首先注册账号，经过注册创建应用等过程后拿到key填入到next主题中</p><blockquote><p>leancloud_visitors:<br>  enable: true<br>  app_id: M8d5yPXQ5l0kwFz1bO6cF<strong><strong>***</strong></strong><br>  app_key: QEarlHm5c8XcuF22*****</p></blockquote><p>将统计结果修改为热度，修改<code>themes/next/languages/zh-CN.yml</code>文件中<code>post</code>段中<code>view</code>中文翻译为<code>热度</code>。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgbABWCvv.jpg" alt="view字段"></p><p>打开<code>themes/next/layout/_macro/post.swig</code>搜索<code>leancloud-visitors-count</code>字段，添加<code>&lt;span&gt;sheshid&lt;/span&gt;</code></p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgTBcABwW.jpg" alt="摄氏度符号添加"></p><p>后续还有一些关于Leanclound的设置，为节省篇幅，请直接访问：<a href="https://leaferx.online/2018/02/11/lc-security/" target="_blank" rel="noopener">传送门</a></p><h4 id="3-6-2-添加全站访问量统计"><a href="#3-6-2-添加全站访问量统计" class="headerlink" title="3.6.2 添加全站访问量统计"></a>3.6.2 添加全站访问量统计</h4><p>全站访客统计使用<a href="http://ibruce.info/2015/04/04/busuanzi/" target="_blank" rel="noopener">不蒜子</a>，在<code>themes/next/layout/_partials/footer.swig</code>中添加如下内容。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">async</span> <span class="attr">src</span>=<span class="string">"//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"busuanzi_container_site_pv"</span>&gt;</span></span><br><span class="line">| 本站总访问量<span class="tag">&lt;<span class="name">span</span> <span class="attr">style</span>=<span class="string">"color:#FA8072"</span> <span class="attr">id</span>=<span class="string">"busuanzi_value_site_pv"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span>次</span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"busuanzi_container_site_uv"</span>&gt;</span></span><br><span class="line">| 本站访客数<span class="tag">&lt;<span class="name">span</span> <span class="attr">style</span>=<span class="string">"color:#FA8072"</span> <span class="attr">id</span>=<span class="string">"busuanzi_value_site_uv"</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span>人次</span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-7-修改文章底部的那个带-号的标签"><a href="#3-7-修改文章底部的那个带-号的标签" class="headerlink" title="3.7 修改文章底部的那个带#号的标签"></a>3.7 修改文章底部的那个带#号的标签</h3><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img5cleWRI.jpg" alt="标签"></p><p>修改文件<code>themes/next/layout/_macro/post.swig</code>，找到<code>rel=&quot;tag&quot;&gt;#</code>，将<code>#</code>替换为<code>&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;</code>即可。</p><h3 id="3-8-文章末尾效果"><a href="#3-8-文章末尾效果" class="headerlink" title="3.8 文章末尾效果"></a>3.8 文章末尾效果</h3><p>1)添加“本文结束”标记</p><p>效果图：</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgwIS1EFZ.jpg" alt="致谢"></p><p>创建文件<code>themes/next/layout/_macro/passage-end-tag.swig</code>，填入一下内容</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if not is_index %&#125;</span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">blockquote</span> <span class="attr">class</span>=<span class="string">"blockquote-center"</span> <span class="attr">style</span>=<span class="string">"color: ##32CD32;font-size:18px;"</span>&gt;</span></span><br><span class="line">------ 本文结束 ------<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>随后修改<code>themes/next/layout/_macro/post.swig</code>，在``之前添加</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if not is_index %&#125;</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    &#123;% include 'passage-end-tag.swig' %&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>2）添加文末版权声明</p><p>效果如下</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgh2tADWv.jpg" alt="版权声明"></p><p>在<code>themes/next/layout/_macro/</code>生成一个<code>custome-cc.swig</code>文件，填入一下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if not is_index %&#125;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;div style=&quot;border: 1px solid black&quot;&gt;</span><br><span class="line">&lt;div style=&quot;margin-left:10px&quot;&gt;</span><br><span class="line">&lt;span style=&quot;font-weight:blod&quot;&gt;版权声明&lt;/span&gt;</span><br><span class="line">&lt;img src=&quot;/uploads/cc.png&quot; &gt;</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">&lt;p style=&quot;font-size: 10px;line-height: 30px&quot;&gt;&lt;a href=&quot;http://ylong.net.cn&quot; style=&quot;color:#258FC6&quot;&gt;Louie&apos;s Blog&lt;/a&gt; by &lt;a href=&quot;http://ylong.net.cn&quot; style=&quot;color:#258FC6&quot;&gt;louie long&lt;/a&gt; is licensed under a &lt;a href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; style=&quot;color:#258FC6&quot;&gt;Creative Commons BY-NC-ND 4.0 International License&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">由&lt;a href=&quot;http://ylong.net.cn&quot; style=&quot;color:#258FC6&quot;&gt;Louie Long&lt;/a&gt;创作并维护的&lt;a href=&quot;ylong.net.cn&quot; style=&quot;color:#258FC6&quot;&gt;Louie&apos;s Blog&lt;/a&gt;博客采用&lt;a href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; style=&quot;color:#258FC6&quot;&gt;创作共用保留署名-非商业-禁止演绎4.0国际许可证&lt;/a&gt;。&lt;br/&gt;</span><br><span class="line">本文首发于&lt;a href=&quot;http://ylong.net.cn&quot; style=&quot;color:#258FC6&quot;&gt;Louie&apos;s Blog&lt;/a&gt; 博客（ &lt;a href=&quot;http://ylong.net.cn&quot; style=&quot;color:#258FC6&quot;&gt;http://ylong.net.cn&lt;/a&gt; ），版权所有，侵权必究。</span><br><span class="line">&lt;br/&gt;</span><br><span class="line">转载请注明作者和链接地址&lt;a href=&quot;http://ylong.net.cn&quot; style=&quot;color:#258FC6&quot;&gt;http://ylong.net.cn&lt;/a&gt;, 如对文章内容有疑问请联系邮箱（ &lt;a style=&quot;color:#258FC6&quot;&gt;longyu805@163.com&lt;/a&gt; ）。&lt;/p&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>需要在<code>source/uploads</code>下上传图片如下</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgARwLVLp.png" alt="cc image"></p><p>同样修改<code>themes/next/layout/_macro/post.swig</code>，在<code>post-footer</code>之前添加</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">  &#123;% if not is_index %&#125;</span><br><span class="line">    &#123;% include 'custome-cc.swig' %&#125;</span><br><span class="line">  &#123;% endif %&#125;</span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-9-动态背景"><a href="#3-9-动态背景" class="headerlink" title="3.9 动态背景"></a>3.9 动态背景</h3><p>next主题集成了集中动态背景效果，如下所示</p><p>1) canvas_nest</p><p>next 5.1.1以上可以直接在主题配置文件中将<code>canvas_nest: false</code>改成<code>canvas_nest: true</code>，随后下载动态背景js即可</p><p>参考链接：<a href="https://github.com/theme-next/theme-next-canvas-nest" target="_blank" rel="noopener">传送门</a></p><p>下载代码带主题的lib库中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/theme-next/theme-next-canvas-nest themes/next/source/lib/canvas-nest</span><br></pre></td></tr></table></figure><p>随后使能<code>canvas_nest</code>即可，随后生成看效果，有几个参数可以自己修改<code>themes/next/source/lib/canvas-nest/canvas-nest.min.js</code></p><table><thead><tr><th align="left">参数</th><th align="left">含义</th></tr></thead><tbody><tr><td align="left">zIndex</td><td align="left">背景的z-index属性，css属性用于控制所在层的位置，默认：-1</td></tr><tr><td align="left">opacity</td><td align="left">线条透明度（0~1）, 默认：0.5</td></tr><tr><td align="left">color</td><td align="left">线条颜色, 默认: ‘0,0,0’；三个数字分别为(R,G,B)</td></tr><tr><td align="left">count</td><td align="left">线条的总数量, 默认: 99</td></tr></tbody></table><p>2) 3-D 效果</p><p>参考链接：<a href="https://github.com/theme-next/theme-next-three" target="_blank" rel="noopener">传送门</a></p><p>同样是下载相应的js脚本开启开关即可，非常简单</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/theme-next/theme-next-three themes/next/source/lib/three</span><br></pre></td></tr></table></figure><p>随后只需要开启单个效果开关</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># JavaScript 3D library.</span></span><br><span class="line"><span class="comment"># Dependencies: https://github.com/theme-next/theme-next-three</span></span><br><span class="line"><span class="comment"># three_waves</span></span><br><span class="line"><span class="attr">three_waves:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment"># canvas_lines</span></span><br><span class="line"><span class="attr">canvas_lines:</span> <span class="literal">true</span></span><br><span class="line"><span class="comment"># canvas_sphere</span></span><br><span class="line"><span class="attr">canvas_sphere:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><h3 id="3-10-自定义样式修改"><a href="#3-10-自定义样式修改" class="headerlink" title="3.10 自定义样式修改"></a>3.10 自定义样式修改</h3><h4 id="3-10-1-文章内链接样式"><a href="#3-10-1-文章内链接样式" class="headerlink" title="3.10.1 文章内链接样式"></a>3.10.1 文章内链接样式</h4><p>修改<code>themes/next/source/css/_custom/custom.styl</code>，在末尾添加如下css样式</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 文章内链接文本样式</span><br><span class="line"><span class="selector-class">.post-body</span> <span class="selector-tag">p</span> <span class="selector-tag">a</span>&#123;</span><br><span class="line">  <span class="attribute">color</span>: <span class="number">#0593d3</span>;</span><br><span class="line">  <span class="attribute">border-bottom</span>: none;</span><br><span class="line">  <span class="attribute">border-bottom</span>: <span class="number">2px</span> solid <span class="number">#0593d3</span>;</span><br><span class="line">  &amp;:hover &#123;</span><br><span class="line">    <span class="selector-tag">color</span>: <span class="selector-id">#fc6423</span>;</span><br><span class="line">    <span class="selector-tag">border-bottom</span>: <span class="selector-tag">none</span>;</span><br><span class="line">    <span class="selector-tag">border-bottom</span>: 1<span class="selector-tag">px</span> <span class="selector-tag">solid</span> <span class="selector-id">#fc6423</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>颜色可以自定义修改。</p><h4 id="3-10-2-文章内-样式"><a href="#3-10-2-文章内-样式" class="headerlink" title="3.10.2 文章内``样式"></a>3.10.2 文章内``样式</h4><p>修改<code>themes/next/source/css/_custom/custom.styl</code>，添加以下内容</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// Custom styles.</span><br><span class="line"><span class="selector-tag">code</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#ff7600</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="number">#fbf7f8</span>;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">2px</span>;</span><br><span class="line">&#125;</span><br><span class="line">// 大代码块的自定义样式</span><br><span class="line"><span class="selector-class">.highlight</span>, <span class="selector-tag">pre</span> &#123;</span><br><span class="line">    <span class="attribute">margin</span>: <span class="number">5px</span> <span class="number">0</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">5px</span>;</span><br><span class="line">    <span class="attribute">border-radius</span>: <span class="number">3px</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="selector-class">.highlight</span>, <span class="selector-tag">code</span>, <span class="selector-tag">pre</span> &#123;</span><br><span class="line">    <span class="attribute">border</span>: <span class="number">1px</span> solid <span class="number">#d6d6d6</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-10-3-主页文章添加阴影效果"><a href="#3-10-3-主页文章添加阴影效果" class="headerlink" title="3.10.3 主页文章添加阴影效果"></a>3.10.3 主页文章添加阴影效果</h4><p>修改<code>themes/next/source/css/_custom/custom.styl</code>，添加以下内容</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 主页文章添加阴影效果</span><br><span class="line"> <span class="selector-class">.post</span> &#123;</span><br><span class="line">   <span class="attribute">margin-top</span>: <span class="number">60px</span>;</span><br><span class="line">   <span class="attribute">margin-bottom</span>: <span class="number">60px</span>;</span><br><span class="line">   <span class="attribute">padding</span>: <span class="number">25px</span>;</span><br><span class="line">   <span class="attribute">-webkit-box-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">5px</span> <span class="built_in">rgba</span>(202, 203, 203, .5);</span><br><span class="line">   <span class="attribute">-moz-box-shadow</span>: <span class="number">0</span> <span class="number">0</span> <span class="number">5px</span> <span class="built_in">rgba</span>(202, 203, 204, .5);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="3-11-侧边栏社交小图标设置"><a href="#3-11-侧边栏社交小图标设置" class="headerlink" title="3.11 侧边栏社交小图标设置"></a>3.11 侧边栏社交小图标设置</h3><p>打开主题配置文件<code>themes/next/_config.yml</code>，搜索<code>Social</code>，在<a href="http://fontawesome.io/icons/" target="_blank" rel="noopener">图标库</a>找自己喜欢的小图标，并将名字复制在<code>||</code>之后即可。</p><blockquote><p>GitHub: <a href="https://github.com/louielong" target="_blank" rel="noopener">https://github.com/louielong</a> || github</p></blockquote><h3 id="3-12-顶部加载条特效"><a href="#3-12-顶部加载条特效" class="headerlink" title="3.12 顶部加载条特效"></a>3.12 顶部加载条特效</h3><p>next的主题中已经预置的相关配置，仅仅只需要打开相应的开关并下载lib库即可，方法参见：<a href="https://github.com/theme-next/theme-next-pace" target="_blank" rel="noopener">pace特效传送门</a></p><p>克隆代码带主题的lib库中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/theme-next/theme-next-pace themes/next/source/lib/pace</span><br></pre></td></tr></table></figure><p>随后修改<code>themes/next/_config.yml</code>，将<code>pace: false</code>改为<code>pace: true</code>，然后选择满意的特效选项即可。</p><blockquote><p>pace_theme: pace-theme-flash</p></blockquote><h3 id="3-13-开启打赏功能"><a href="#3-13-开启打赏功能" class="headerlink" title="3.13 开启打赏功能"></a>3.13 开启打赏功能</h3><p>仅需要开启打赏功能并上传二维码即可</p><p>修改<code>themes/next/_config.yml</code>中的<code>Reward</code>字段下开启一个标签，并上传图二维码至<code>source/uploads</code>或者<code>themes/next/source/images</code>目录下，并相应的修改路径即可。</p><p>开启后发现打赏字体闪动很频繁碍眼，可以注释掉，<code>themes/next/source/css/_common/components/post/post-reward.styl</code>，如下注释掉或者修改 roll后的参数值，降低闪动频率。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 注释文字闪动函数</span></span><br><span class="line"><span class="comment">#wechat:hover p&#123;</span></span><br><span class="line"><span class="comment">    animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">    -webkit-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">    -moz-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">#alipay:hover p&#123;</span></span><br><span class="line"><span class="comment">    animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">    -webkit-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">    -moz-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">#bitcoin:hover p &#123;</span></span><br><span class="line"><span class="comment">    animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">    -webkit-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">    -moz-animation: roll 0.1s infinite linear;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h3 id="3-14-添加RSS订阅"><a href="#3-14-添加RSS订阅" class="headerlink" title="3.14 添加RSS订阅"></a>3.14 添加RSS订阅</h3><p>安装RSS订阅插件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-generator-feed</span><br></pre></td></tr></table></figure><p>搜索<code>rss</code>字样，添加<code>rss: /atom.xml</code>，重新<code>hexo g</code>生成一遍即可。</p><h3 id="3-15-添加SEO搜索"><a href="#3-15-添加SEO搜索" class="headerlink" title="3.15 添加SEO搜索"></a>3.15 添加SEO搜索</h3><p>本来想添加SEO加速搜索的，但是登录到<strong>百毒</strong>上一看需要这么多信息，果断放弃了，顺带给一个鄙视的眼神。</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgVq98g8w.jpg" alt="百度SEO注册"></p><h3 id="3-16-自定义默认生成文章头部模板"><a href="#3-16-自定义默认生成文章头部模板" class="headerlink" title="3.16 自定义默认生成文章头部模板"></a>3.16 自定义默认生成文章头部模板</h3><p>当新创建文章是希望可以默认生成一些头部字段，修改<code>scaffolds/post.md</code>文件增加一些自定义填充字段。</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:</span><br><span class="line">keywords:</span><br><span class="line">categories:</span><br><span class="line">description:</span><br><span class="line">summary_img:</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>使用<code>hexo new test</code>创建文章时则会自动填充相应信息，如下图所示</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgNSYFNAp.jpg" alt="文章模板"></p><p>若想在主页显示文章的总结图片，在<code>themes/next/layout/_macro/post.swig</code>中的<code>post.description</code>前加入如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if post.summary_img  %&#125;</span><br><span class="line">  &lt;div class=&quot;out-img-topic&quot;&gt;</span><br><span class="line">   &lt;img src=&#123;&#123; post.summary_img &#125;&#125; class=&quot;img-topic&quot;&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>重新生成博客即可看到文章缩略图。</p><h3 id="3-17-博客字体修改"><a href="#3-17-博客字体修改" class="headerlink" title="3.17 博客字体修改"></a>3.17 博客字体修改</h3><p>博客的字体配置在<code>themes/next/_config.yml</code>找到<code>Font Settings</code>，修改对应字段可以配置相应的字体效果。默认使用的是<a href="https://www.google.com/fonts" target="_blank" rel="noopener">谷歌的字体</a>。</p><p>如我想修改代码段的字体格式</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Font settings for &lt;code&gt; and code blocks.</span></span><br><span class="line"><span class="attr">codes:</span></span><br><span class="line"><span class="attr">  external:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  family:</span> <span class="string">PT</span> <span class="string">Mono</span></span><br><span class="line"><span class="attr">  size:</span> <span class="number">14</span></span><br></pre></td></tr></table></figure><h3 id="3-18-显示网站副标题"><a href="#3-18-显示网站副标题" class="headerlink" title="3.18 显示网站副标题"></a>3.18 显示网站副标题</h3><p>Next的Mist主题默认隐藏了副标题的显示，通过修改<code>themes/next/source/css/_schemes/Mist/_logo.styl</code>可以打开副标题的显示。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.site-subtitle</span> &#123; <span class="attribute">display</span>: yes; &#125;</span><br></pre></td></tr></table></figure><h3 id="3-19-文章图片居中"><a href="#3-19-文章图片居中" class="headerlink" title="3.19 文章图片居中"></a>3.19 文章图片居中</h3><p>修改Next主题文件<code>themes/next/source/css/_schemes/Mist/_posts-expanded.styl</code>，找到<code>.posts-expand</code>中的<code>.post-body img</code>加入<code>auto</code>字段。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.post-body</span> <span class="selector-tag">img</span> &#123; <span class="attribute">margin</span>: <span class="number">0</span>, auto; &#125;</span><br></pre></td></tr></table></figure><h3 id="3-20-点击爱心效果"><a href="#3-20-点击爱心效果" class="headerlink" title="3.20 点击爱心效果"></a>3.20 点击爱心效果</h3><p>在<code>themes/next/source/js/src</code>下新建文件<code>clicklove.js</code>，随后将<a href="http://7u2ss1.com1.z0.glb.clouddn.com/love.js" target="_blank" rel="noopener">链接</a>下的代码拷贝粘贴到<code>clicklove.js</code>文件中</p><p>在<code>themes/next/layout/_layout.swig</code>文件末尾添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 页面点击小红心 --&gt;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/clicklove.js&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/img7hCt0FX.jpg" alt="点击爱心"></p><h3 id="3-21-博客背景图片"><a href="#3-21-博客背景图片" class="headerlink" title="3.21 博客背景图片"></a>3.21 博客背景图片</h3><p>统一修改next预留的自定义文件<code>themes/next/source/css/_custom/custom.styl</code></p><p>1)静态图片</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">body &#123;</span><br><span class="line">    background:url(https:<span class="comment">//XXX);</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>设置静态背景图片</p><p>2)动态图片</p><p><strong>unsplash</strong>网站提供大量高清图片，可随机选择</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">body</span> &#123;</span><br><span class="line">    <span class="attribute">background</span>:<span class="built_in">url</span>(https://source.unsplash.com/random/1600x900);</span><br><span class="line">    <span class="attribute">background-repeat</span>: no-repeat;</span><br><span class="line">    <span class="attribute">background-attachment</span>:fixed;</span><br><span class="line">    <span class="attribute">background-position</span>:<span class="number">50%</span> <span class="number">50%</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>不透明度设定</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.main-inner</span> &#123;</span><br><span class="line">    <span class="attribute">margin-top</span>: <span class="number">60px</span>;</span><br><span class="line">    <span class="attribute">padding</span>: <span class="number">60px</span> <span class="number">60px</span> <span class="number">60px</span> <span class="number">60px</span>;</span><br><span class="line">    <span class="attribute">background</span>: <span class="number">#fff</span>;</span><br><span class="line">    <span class="attribute">opacity</span>: <span class="number">0.8</span>;</span><br><span class="line">    <span class="attribute">min-height</span>: <span class="number">500px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>background: #FFF  代表白色背景色</li><li>opacity： 0.8  代表不透明度</li></ul><h3 id="3-22-侧边栏背景修改"><a href="#3-22-侧边栏背景修改" class="headerlink" title="3.22 侧边栏背景修改"></a>3.22 侧边栏背景修改</h3><p>给侧边栏添加背景图片，只需要修改<code>themes/next/source/css/_common/components/sidebar/sidebar.styl</code>的<code>background</code>的值即可，同时为了让图片平铺可以增加一些设置。同样的，图片可以是在线的也可以是本地。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">background: url('/uploads/sidebar.jpg');</span><br><span class="line"><span class="selector-tag">box-shadow</span>: <span class="selector-tag">none</span>;</span><br><span class="line"><span class="selector-tag">background-repeat</span>: <span class="selector-tag">round</span>;</span><br></pre></td></tr></table></figure><p>【Note】</p><p>由于使用的黑色的背景，这里将侧边栏的字体设置为白色，需要修改一些配置文件</p><p>1)<code>themes/next/source/css/_common/components/sidebar/sidebar-toc.styl</code>的Line 23，此处影响的是<code>文章目录</code>字体颜色</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">color: $whitesmoke;</span><br></pre></td></tr></table></figure><p>2)修改<code>themes/next/source/css/_variables/base.styl</code>的 line 265，此处影响的是<code>站点概览</code>的字体颜色</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$sidebar-nav-color                    = $whitesmoke</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Hexo 博客自定义配置
    
    </summary>
    
      <category term="Hexo" scheme="https://louielong.github.io/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="https://louielong.github.io/source/tags/Hexo/"/>
    
      <category term="Blog" scheme="https://louielong.github.io/source/tags/Blog/"/>
    
  </entry>
  
  <entry>
    <title>MySQL双主复制 + keepalived故障转移实现</title>
    <link href="https://louielong.github.io/mysql-ha.html"/>
    <id>https://louielong.github.io/mysql-ha.html</id>
    <published>2018-06-06T07:38:12.000Z</published>
    <updated>2019-07-01T03:24:50.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><p>MySQL复制中较常见的复制架构有“一主一从”、“一主多从”、“双主”、“多级复制”和“多主环形机构”等，在项目实施中遇到需要进行故障转移的需求：两台服务器每台都安装MySQL，当一个MySQL服务器故障时另一个MySQL服务器能够继续提供服务，这要求两个MySQL之间能够进行数据复制同时需要监控两台服务器的状态。</p><p>本次使用MySQL的双主复制以及keepalived的HA机制来实现。</p><h2 id="2-环境准备"><a href="#2-环境准备" class="headerlink" title="2 环境准备"></a>2 环境准备</h2><p>两台服务器：<br>服务器MySQL-HA-1(主) 192.168.10.101<br>服务器MySQL-HA-2(主) 192.168.10.102<br>虚拟服务节点IP       192.168.10.103<br>Mysql版本：mysql  Ver 14.14 Distrib 5.7.21<br>System OS：ubuntu 16.04</p><h2 id="3-mysql数据库配置"><a href="#3-mysql数据库配置" class="headerlink" title="3 mysql数据库配置"></a>3 mysql数据库配置</h2><h3 id="3-1-mysql数据库介绍"><a href="#3-1-mysql数据库介绍" class="headerlink" title="3.1 mysql数据库介绍"></a>3.1 mysql数据库介绍</h3><p><img src="https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/w%3D268%3Bg%3D0/sign=e35e494a6159252da3171a020ca06406/ac6eddc451da81cb037c289d5366d016082431c3.jpg" alt="mysql"></p><p>MySQL是一个<a href="https://baike.baidu.com/item/%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F" target="_blank" rel="noopener"><strong>关系型数据库管理系统</strong></a><strong>，</strong>由瑞典MySQL AB 公司开发，目前属于 <a href="https://baike.baidu.com/item/Oracle" target="_blank" rel="noopener">Oracle</a> 旗下产品。MySQL 是最流行的<a href="https://baike.baidu.com/item/%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F" target="_blank" rel="noopener">关系型数据库管理系统</a>之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件。</p><p>MySQL是一种关系数据库管理系统，关系数据库将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了速度并提高了灵活性。</p><p>MySQL所使用的 SQL 语言是用于访问<a href="https://baike.baidu.com/item/%E6%95%B0%E6%8D%AE%E5%BA%93" target="_blank" rel="noopener">数据库</a>的最常用标准化语言[1]。</p><h3 id="3-1-安装数据库"><a href="#3-1-安装数据库" class="headerlink" title="3.1 安装数据库"></a>3.1 安装数据库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install -y mysql-server</span><br></pre></td></tr></table></figure><p>【Tips】：安装过程需要输入数据库密码，出于后续部署的自动化考虑，希望自动化部署中不被打断，解决这个问题有两种方法：</p><p>1) 预先设置密码</p><p>使用<code>debconf-set-selections</code>工具预先将密码写入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo debconf-set-selections &lt;&lt;&lt; 'mysql-server mysql-server/root_password password your_password'</span><br><span class="line">sudo debconf-set-selections &lt;&lt;&lt; 'mysql-server mysql-server/root_password_again password your_password'</span><br><span class="line">sudo apt-get -y install mysql-server</span><br></pre></td></tr></table></figure><p>将<code>your_password</code>替换为想要设置的mysql root账户密码，针对不同的mysql版本会有相应的改变，参见<a href="https://stackoverflow.com/questions/7739645/install-mysql-on-ubuntu-without-a-password-prompt" target="_blank" rel="noopener">传送门</a></p><p>2)静默安装</p><p>使用如下命令通过非交互的方式静默安装mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo DEBIAN_FRONTEND=noninteractive apt-get install -y mysql-server</span><br></pre></td></tr></table></figure><p>在安装完成后mysql是没有密码的，root用户下是可以免密进入命令行的，然后再修改mysql的root用户访问密码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -e"SET PASSWORD FOR 'root'@'localhost' = PASSWORD('passwd');"</span><br></pre></td></tr></table></figure><p>将<code>passwd</code>替换为自己的密码即可。</p><p>3.2 数据库初始化配置</p><p>在修改配置前最好先备份mysql配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /etc/mysql/mysql.conf.d/mysqld.cnf  /etc/mysql/mysql.conf.d/mysqld.cnf-bak</span><br></pre></td></tr></table></figure><p>1）设置时区</p><p>在<code>mysqld.cnf</code>的<code>[mysqld]</code>后加入<code>default-time-zone = &#39;+8:00&#39;</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i "/^\[mysqld\]/a\default-time-zone = \'+8:00\'" /etc/mysql/mysql.conf.d/mysqld.cnf</span><br></pre></td></tr></table></figure><h3 id="3-3-修改数据库配置"><a href="#3-3-修改数据库配置" class="headerlink" title="3.3 修改数据库配置"></a>3.3 修改数据库配置</h3><p>设置需要同步的数据库，此处以数据库<code>test</code>为例</p><p>1）修改MySQL-HA-1服务器数据库配置</p><p>主要修改的地方如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bind-address= ::    # 指定允许数据库访问的IP，"::"表明允许v4和v6访问</span><br><span class="line"></span><br><span class="line">server-id= 1                     # 指定mysql的编号 该编号作为数据库集群的识别号因此不能冲突</span><br><span class="line">log_bin= /var/log/mysql/mysql-bin.log   # 开启二进制log文件</span><br><span class="line">binlog_format           = mixed</span><br><span class="line">relay-log               = relay-bin</span><br><span class="line">relay-log-index         = slave-relay-bin.index</span><br><span class="line">auto-increment-offset   = 2     # 设置自增长偏移，这里有两个数据库因此每次增长为2</span><br><span class="line">auto-increment-increment = 1    # 设置自增长起始</span><br><span class="line">binlog_do_db            = test  # 设置需要同步的数据库名称</span><br><span class="line">binlog_do_db            = test1</span><br></pre></td></tr></table></figure><p>完整的配置文件如下，这里将原配置文件的注释屏蔽掉</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~# cat /etc/mysql/mysql.conf.d/mysqld.cnf | grep -v '#'</span><br><span class="line"></span><br><span class="line">[mysqld_safe]</span><br><span class="line">socket= /var/run/mysqld/mysqld.sock</span><br><span class="line">nice= 0</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">user= mysql</span><br><span class="line">pid-file= /var/run/mysqld/mysqld.pid</span><br><span class="line">socket= /var/run/mysqld/mysqld.sock</span><br><span class="line">port= 3306</span><br><span class="line">basedir= /usr</span><br><span class="line">datadir= /var/lib/mysql</span><br><span class="line">tmpdir= /tmp</span><br><span class="line">lc-messages-dir= /usr/share/mysql</span><br><span class="line">skip-external-locking</span><br><span class="line">bind-address= ::</span><br><span class="line">key_buffer_size= 16M</span><br><span class="line">max_allowed_packet= 16M</span><br><span class="line">thread_stack= 192K</span><br><span class="line">thread_cache_size       = 8</span><br><span class="line">myisam-recover-options  = BACKUP</span><br><span class="line">query_cache_limit= 1M</span><br><span class="line">query_cache_size        = 16M</span><br><span class="line">general_log_file        = /var/log/mysql/mysql.log</span><br><span class="line">general_log             = 1</span><br><span class="line">log_error = /var/log/mysql/error.log</span><br><span class="line">server-id= 1</span><br><span class="line">log_bin= /var/log/mysql/mysql-bin.log</span><br><span class="line">binlog_format           = mixed</span><br><span class="line">expire_logs_days        = 10</span><br><span class="line">max_binlog_size         = 100M</span><br><span class="line">relay-log               = relay-bin</span><br><span class="line">relay-log-index         = slave-relay-bin.index</span><br><span class="line">auto-increment-offset   = 2</span><br><span class="line">auto-increment-increment = 1</span><br><span class="line">binlog_do_db            = test</span><br><span class="line">binlog_do_db            = test1</span><br></pre></td></tr></table></figure><p>其他参数含义可参考<a href="http://www.jb51.net/article/48082.htm" target="_blank" rel="noopener">mysql配置文件详解</a></p><p>2）修改MySQL-HA-2服务器数据库配置</p><p>修改HA-2节点的数据库配置文件，大致内容一致，只是在<code>server-id</code>和<code>auto-increment-increment</code>需要修改。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">root@HA-2:~# cat /etc/mysql/mysql.conf.d/mysqld.cnf | grep -v '#'</span><br><span class="line">[mysqld_safe]</span><br><span class="line">socket= /var/run/mysqld/mysqld.sock</span><br><span class="line">nice= 0</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">user= mysql</span><br><span class="line">pid-file= /var/run/mysqld/mysqld.pid</span><br><span class="line">socket= /var/run/mysqld/mysqld.sock</span><br><span class="line">port= 3306</span><br><span class="line">basedir= /usr</span><br><span class="line">datadir= /var/lib/mysql</span><br><span class="line">tmpdir= /tmp</span><br><span class="line">lc-messages-dir= /usr/share/mysql</span><br><span class="line">skip-external-locking</span><br><span class="line">bind-address= ::</span><br><span class="line">key_buffer_size= 16M</span><br><span class="line">max_allowed_packet= 16M</span><br><span class="line">thread_stack= 192K</span><br><span class="line">thread_cache_size       = 8</span><br><span class="line">myisam-recover-options  = BACKUP</span><br><span class="line">query_cache_limit= 1M</span><br><span class="line">query_cache_size        = 16M</span><br><span class="line">general_log_file        = /var/log/mysql/mysql.log</span><br><span class="line">general_log             = 1</span><br><span class="line">log_error = /var/log/mysql/error.log</span><br><span class="line">server-id= 2            # server id设置为2</span><br><span class="line">log_bin= /var/log/mysql/mysql-bin.log</span><br><span class="line">binlog_format           = mixed</span><br><span class="line">expire_logs_days        = 10</span><br><span class="line">max_binlog_size         = 100M</span><br><span class="line">relay-log               = relay-bin</span><br><span class="line">relay-log-index         = slave-relay-bin.index</span><br><span class="line">auto-increment-offset   = 2</span><br><span class="line">auto-increment-increment = 2    # 增长起始设置为2</span><br><span class="line">binlog_do_db            = test</span><br><span class="line">binlog_do_db            = test1</span><br></pre></td></tr></table></figure><p>修改完成后<strong>两个节点</strong>的数据库都需要重启</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql restart</span><br></pre></td></tr></table></figure><p>【Note】</p><p>为了提升数据库的写入性能，可以适当修改同步数据间隔，加入以下参数[4]</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">innodb_flush_log_at_trx_commit=2</span><br><span class="line">sync_binlog=1000</span><br></pre></td></tr></table></figure><p>有关两个参数的含义可以查看链接：<a href="http://blog.itpub.net/22664653/viewspace-1063134/" target="_blank" rel="noopener">传送门</a></p><h3 id="3-3-设置主从数据库"><a href="#3-3-设置主从数据库" class="headerlink" title="3.3 设置主从数据库"></a>3.3 设置主从数据库</h3><p>1)将HA-1设置为HA-2的主数据库</p><p>首先在HA-1节点数据库创建同步账户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~# mysql -uroot -p</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> GRANT REPLICATION SLAVE,REPLICATION CLIENT ON *.* TO sync@<span class="string">'192.168.10.102'</span> IDENTIFIED BY <span class="string">'sync'</span>;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> flush privileges;</span></span><br></pre></td></tr></table></figure><p>随后查看数据库状态信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show master status;</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| mysql-bin.000001 |    465 | test,test1    |                  |                   |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>记录下<code>File</code>和<code>Position</code>两个参数值，这是数据库主从同步的关键，也是告诉从数据自那哪个起点开始同步</p><p>在<strong>HA-2</strong>节点的数据库输入以下信息，切记是在HA-2节点输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; change master to master_host=&apos;192.168.10.101&apos;,master_user=&apos;sync&apos;,master_password=&apos;sync&apos;,master_log_file=&apos;mysql-bin.000001&apos;,master_log_pos=465;</span><br></pre></td></tr></table></figure><p>随后在HA-2节点开启从机同步即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> start slave</span></span><br></pre></td></tr></table></figure><p>然后在HA-2节点查看mysql的slave信息，确保下述两个值为yes</p><blockquote><p>Slave_IO_Running:Yes<br>Slave_SQL_Running:Yes</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show slave status\G;</span></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 192.168.10.101</span><br><span class="line">                  Master_User: copyuser</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000001</span><br><span class="line">          Read_Master_Log_Pos: 465</span><br><span class="line">               Relay_Log_File: relay-bin.00002</span><br><span class="line">                Relay_Log_Pos: 146</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000001</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">              Replicate_Do_DB:</span><br><span class="line">          Replicate_Ignore_DB:</span><br><span class="line">           Replicate_Do_Table:</span><br><span class="line">       Replicate_Ignore_Table:</span><br><span class="line">      Replicate_Wild_Do_Table:</span><br></pre></td></tr></table></figure><p>2)将HA-2设置为HA-1的主数据库</p><p>设置过程与上一步一致，首先创建同步账户随后添加主数据库信息，需要注意的是IP地址的修改</p><p>在HA-2上创建同步账户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@HA-2:~# mysql -uroot -p</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> GRANT REPLICATION SLAVE,REPLICATION CLIENT ON *.* TO sync@<span class="string">'192.168.10.101'</span> IDENTIFIED BY <span class="string">'sync'</span>;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> flush privileges;</span></span><br></pre></td></tr></table></figure><p>随后查看数据库状态信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show master status;</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| mysql-bin.000002 |    465 | test,test1    |                  |                   |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>记录下<code>File</code>和<code>Position</code>两个参数值，这是数据库主从同步的关键，也是告诉从数据自那哪个起点开始同步</p><p>在<strong>HA-1</strong>节点的数据库输入以下信息，切记是在HA-1节点输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; change master to master_host=&apos;192.168.10.102&apos;,master_user=&apos;sync&apos;,master_password=&apos;sync&apos;,master_log_file=&apos;mysql-bin.000001&apos;,master_log_pos=465;</span><br></pre></td></tr></table></figure><p>随后开启HA-1的从机同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> start slave</span></span><br></pre></td></tr></table></figure><p>然后在HA-1节点查看mysql的slave信息，确保下述两个值为yes</p><blockquote><p>Slave_IO_Running:Yes<br>Slave_SQL_Running:Yes</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show slave status\G;</span></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 192.168.10.102</span><br><span class="line">                  Master_User: copyuser</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000002</span><br><span class="line">          Read_Master_Log_Pos: 465</span><br><span class="line">               Relay_Log_File: relay-bin.00003</span><br><span class="line">                Relay_Log_Pos: 146</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000002</span><br><span class="line">             Slave_IO_Running: Yes</span><br><span class="line">            Slave_SQL_Running: Yes</span><br><span class="line">              Replicate_Do_DB:</span><br><span class="line">          Replicate_Ignore_DB:</span><br><span class="line">           Replicate_Do_Table:</span><br><span class="line">       Replicate_Ignore_Table:</span><br><span class="line">      Replicate_Wild_Do_Table:</span><br></pre></td></tr></table></figure><p>【Note】:</p><p>若因为输入错误或网络变动等其他原因导致同步出错需要先停止同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> stop slave</span></span><br></pre></td></tr></table></figure><p>然后重置从机同步</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> reset slave</span></span><br></pre></td></tr></table></figure><p>再重复上述的主从同步配置，但需要注意的是主从同步并不会同步原来的数据，只会同步从当前时刻起始的binlog的数据库操作记录，如果同步中断后仍有数据写入会导致两个数据库的数据起始内容不一致，这时需要先停止数据库写入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; LOCK TABLES;</span><br><span class="line">mysql&gt; FLUSH TABLES;</span><br></pre></td></tr></table></figure><p>然后备份出数据库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysqldump -uroot -p&lt;passwd&gt; &lt;table&gt; &gt; mysql_table_bak.sql</span><br></pre></td></tr></table></figure><p>先<code>drop table &lt;tablename&gt;</code>再将数据导入到另一台数据库服务器中</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p&lt;passwd&gt; &lt;table&gt; &lt; mysql_table_bak.sql</span><br></pre></td></tr></table></figure><p>在配置完主从同步后解锁表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; UNLOCK TABLES</span><br></pre></td></tr></table></figure><h3 id="3-4-测试数据库同步"><a href="#3-4-测试数据库同步" class="headerlink" title="3.4 测试数据库同步"></a>3.4 测试数据库同步</h3><p>在HA-1节点创建一个数据库<code>test</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database test;</span><br></pre></td></tr></table></figure><p>查看HA-2主机是否同步了HA-1上的数据变化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| test          |</span><br><span class="line">| mysql              |</span><br><span class="line">| performance_schema |</span><br><span class="line">| sys                |</span><br><span class="line">+--------------------+</span><br><span class="line">5 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>可以看出HA-2节点的数据库同步了HA-1节点的数据库，在配置成双主复制后任一节点数据库发生改变另一节点数据库都会进行同步。</p><p>在配置完成数据库后若想对数据库进行访问只能访问单一节点数据库的IP，如果希望访问一个固定IP让数据库并能够实现故障自动切换就需要配合keepalived或者HAproxy进行代理。</p><h2 id="4-keepalived-安装"><a href="#4-keepalived-安装" class="headerlink" title="4 keepalived 安装"></a>4 keepalived 安装</h2><h3 id="4-1-keepalived介绍"><a href="#4-1-keepalived介绍" class="headerlink" title="4.1 keepalived介绍"></a>4.1 keepalived介绍</h3><p><img src="https://www.keepalived.org/images/ka-header-new.png" alt="keepalived"></p><p>Keepalived软件起初是专为LVS负载均衡软件设计的，用来管理并监控LVS集群系统中各个服务节点的状态，后来又加入了可以实现高可用的VRRP功能。因此，Keepalived除了能够管理LVS软件外，还可以作为其他服务（例如：Nginx、Haproxy、MySQL等）的高可用解决方案软件。</p><p>Keepalived软件主要是通过VRRP协议实现高可用功能的。VRRP是Virtual Router RedundancyProtocol(虚拟路由器冗余协议）的缩写，VRRP出现的目的就是为了解决静态路由单点故障问题的，它能够保证当个别节点宕机时，整个网络可以不间断地运行。</p><p>所以，Keepalived 一方面具有配置管理LVS的功能，同时还具有对LVS下面节点进行健康检查的功能，另一方面也可实现系统网络服务的高可用功能。</p><p>这里借用博客[4]的有关keepalived的集群工作原理示意图</p><p><img src="https://raw.githubusercontent.com/louielong/blogPic/master/imgiZyFaCC.png" alt="keepalived状态切换示意图"></p><p>Keepalived高可用对之间是通过 VRRP进行通信的， VRRP是遑过竞选机制来确定主备的，主的优先级高于备，因此，工作时主会优先获得所有的资源，备节点处于等待状态，当主挂了的时候，备节点就会接管主节点的资源，然后顶替主节点对外提供服务。</p><p>在 Keepalived服务对之间，只有作为主的服务器会一直发送 VRRP广播包,告诉备它还活着，此时备不会枪占主，当主不可用时，即备监听不到主发送的广播包时，就会启动相关服务接管资源，保证业务的连续性.接管速度最快可以小于1秒。</p><h3 id="4-2-keepalived的安装"><a href="#4-2-keepalived的安装" class="headerlink" title="4.2 keepalived的安装"></a>4.2 keepalived的安装</h3><p>安装方式分为两种：apt直接安装和手动编译安装</p><p>1)手动编译安装</p><p>手动编译的好处是可以使用较新的源码，首先下载源码</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://www.keepalived.org/software/keepalived-1.4.2.tar.gz</span><br></pre></td></tr></table></figure><p>安装必要的编译包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y gcc build-essential make curl libssl-dev libnl-3-dev libnl-genl-3-dev libsnmp-dev</span><br></pre></td></tr></table></figure><p>配置编译，prefix指明需要安装在哪里，也可以不配置使用默认路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar xf keepalived-1.4.2.tar.gz</span><br><span class="line">cd keepalived-1.4.2</span><br><span class="line">./configure --prefix=/usr/local/keepalived</span><br></pre></td></tr></table></figure><p>配置完成后直接编译二连<code>make</code>和<code>make install</code>即可</p><p>2) apt安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt install keepalived</span><br></pre></td></tr></table></figure><h3 id="4-3-keepapiled配置文件"><a href="#4-3-keepapiled配置文件" class="headerlink" title="4.3 keepapiled配置文件"></a>4.3 keepapiled配置文件</h3><p>keepalived服务安装完成之后，后面的主要工作就是在keepalived.conf文件中配置HA和负载均衡。一个功能比较完整的常用的keepalived配置文件，主要包含三块：<em>全局定义块</em>、<em>VRRP实例定义块</em>和<em>虚拟服务器定义块</em>。全局定义块是必须的，如果keepalived只用来做ha，虚拟服务器是可选的。下面数据库HA的配置文件模板：</p><h4 id="4-3-1-keepalived-conf配置"><a href="#4-3-1-keepalived-conf配置" class="headerlink" title="4.3.1 keepalived.conf配置"></a>4.3.1 keepalived.conf配置</h4><p><strong>HA-1主机</strong>上的keepalived.conf文件的修改：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~# cat /etc/keepalived/keepalived.conf</span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">    router_id HA-1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script chk_mysql &#123;</span><br><span class="line">    script /etc/keepalived/bin/chk_mysql.sh    #健康监测脚本路径</span><br><span class="line">    interval 2</span><br><span class="line">    fall 3</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_MYSQL &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface enp0s9       # 监听网卡</span><br><span class="line">    virtual_router_id 100  # 虚拟路由编号，同一实例可以一致，但是其权重一定不能一致</span><br><span class="line">    nopreempt</span><br><span class="line">    priority 100           # 权重，两个节点不能一样</span><br><span class="line">    advert_int 1</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.10.103      # 虚拟IP地址</span><br><span class="line">        240C::1234/64       # 支持IPv6</span><br><span class="line">    &#125;</span><br><span class="line">    notify /etc/keepalived/bin/kpad_notify.sh     # keep状态传入脚本，通过该脚本可得知当前keep运行状态</span><br><span class="line">    track_script &#123;</span><br><span class="line">        chk_mysql            # 健康检查配置</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="4-3-1-2-健康监测脚本"><a href="#4-3-1-2-健康监测脚本" class="headerlink" title="4.3.1.2 健康监测脚本"></a>4.3.1.2 健康监测脚本</h5><p>创建脚本存放目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~# mkdir -p /etc/keepalived/bin</span><br></pre></td></tr></table></figure><p>1)keepalived状态脚本</p><p>创建脚本<code>/etc/keepalived/bin/kpad_notify.sh</code>内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~<span class="comment"># cat /etc/keepalived/bin/kpad_notify.sh</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">TYPE=<span class="variable">$1</span></span><br><span class="line">NAME=<span class="variable">$2</span></span><br><span class="line">STATE=<span class="variable">$3</span></span><br><span class="line"></span><br><span class="line">log_file=<span class="string">"/var/log/test/keepalived/keepalived.log"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">log</span></span>() &#123;</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$(date +"%Y-%m-%d %H:%M:%S.%4N")</span> [<span class="variable">$STATE</span>] <span class="variable">$1</span>"</span> &gt;&gt; <span class="variable">$log_file</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$STATE</span> <span class="keyword">in</span></span><br><span class="line">    <span class="string">"MASTER"</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">'MASTER'</span> &gt; /tmp/keepalived-state</span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line">        ;;</span><br><span class="line">    <span class="string">"BACKUP"</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">'BACKUP'</span> &gt; /tmp/keepalived-state</span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line">        ;;</span><br><span class="line">    <span class="string">"FAULT"</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">'FAULT'</span> &gt; /tmp/keepalived-state</span><br><span class="line">        <span class="built_in">log</span> <span class="string">"keepalived status is fault."</span></span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line">        ;;</span><br><span class="line">    *)</span><br><span class="line">        <span class="built_in">log</span> <span class="string">"unknown keepalived status."</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">        ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>设置脚本运行权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~# chmod +x /etc/keepalived/bin/kpad_notify.sh</span><br></pre></td></tr></table></figure><p>2）配置mysql健康检查脚本</p><p>编辑<code>/etc/keepalived/bin/chk_mysql.sh</code>脚本内容如下，脚本的大致思路是如果在master和backup状态下mysqld进程不存在则尝试重启mysql，若重启失败则任务该节点的mysql彻底故障，进行故障转移。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line"><span class="comment"># File Name: chk_mysql.sh</span></span><br><span class="line"><span class="comment"># Author: louie.long</span></span><br><span class="line"><span class="comment"># Mail: ylong@biigroup.cn</span></span><br><span class="line"><span class="comment"># Created Time: Wed 04 Apr 2018 10:44:20 AM CST</span></span><br><span class="line"><span class="comment"># Description: check mysql service</span></span><br><span class="line"><span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">STATE=`cat /tmp/keepalived-state`</span><br><span class="line">log_file=<span class="string">"/var/log/test/keepalived/keepalived.log"</span></span><br><span class="line">service_name=<span class="string">"mysqld"</span></span><br><span class="line">service_cmd=<span class="string">"/etc/init.d/mysql"</span></span><br><span class="line">get_pid=`pidof <span class="variable">$service_name</span>`</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">log</span></span>() &#123;</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$(date +"%Y-%m-%d %H:%M:%S.%4N")</span> [<span class="variable">$STATE</span>] <span class="variable">$1</span>"</span> &gt;&gt; <span class="variable">$log_file</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$STATE</span> <span class="keyword">in</span></span><br><span class="line">    <span class="string">"MASTER"</span>)</span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"<span class="variable">$&#123;get_pid&#125;</span>"</span> == <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">log</span> <span class="string">"<span class="variable">$service_name</span> service isn't exist."</span></span><br><span class="line">            <span class="built_in">log</span> <span class="string">"Try to restart <span class="variable">$service_name</span> service."</span></span><br><span class="line">            <span class="variable">$service_cmd</span> start</span><br><span class="line">            <span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">                <span class="built_in">log</span> <span class="string">"restart <span class="variable">$service_name</span> service successfully."</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="built_in">log</span> <span class="string">"restart <span class="variable">$service_name</span> service failed."</span></span><br><span class="line">                <span class="built_in">exit</span> 1</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line">        ;;</span><br><span class="line">    <span class="string">"BACKUP"</span>)</span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"<span class="variable">$&#123;get_pid&#125;</span>"</span> == <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">            <span class="built_in">log</span> <span class="string">"<span class="variable">$service_name</span> service isn't exist."</span></span><br><span class="line">            <span class="built_in">log</span> <span class="string">"Try to restart <span class="variable">$service_name</span> service."</span></span><br><span class="line">            <span class="variable">$service_cmd</span> start</span><br><span class="line">            <span class="keyword">if</span> [ $? -eq 0 ]; <span class="keyword">then</span></span><br><span class="line">                <span class="built_in">log</span> <span class="string">"restart <span class="variable">$service_name</span> service successfully."</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="built_in">log</span> <span class="string">"restart <span class="variable">$service_name</span> service failed."</span></span><br><span class="line">                <span class="built_in">exit</span> 1</span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line">        ;;</span><br><span class="line">    <span class="string">"FAULT"</span>)</span><br><span class="line">        <span class="built_in">exit</span> 0</span><br><span class="line">        ;;</span><br><span class="line">       *)</span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">        ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure><p>然后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~# chmod +x /etc/keepalived/bin/chk_mysql.sh</span><br></pre></td></tr></table></figure><p>随后重启keepalived服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~# service keepalived restart</span><br></pre></td></tr></table></figure><h4 id="4-3-2-在HA-2节点上配置"><a href="#4-3-2-在HA-2节点上配置" class="headerlink" title="4.3.2 在HA-2节点上配置"></a>4.3.2 在HA-2节点上配置</h4><p>基本配置个HA-1节点上一样，两个健康监测脚本完全一致，不同的是keepalived.conf脚本中权重值和节点初始属性</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">root@HA-2:~# cat /etc/keepalived/keepalived.conf</span><br><span class="line"></span><br><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">    router_id HA-1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script chk_mysql &#123;</span><br><span class="line">    script /etc/keepalived/bin/chk_mysql.sh    #健康监测脚本路径</span><br><span class="line">    interval 2</span><br><span class="line">    fall 3</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_MYSQL &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface enp0s9       # 监听网卡</span><br><span class="line">    virtual_router_id 100  # 虚拟路由编号，同一实例可以一致，但是其权重一定不能一致</span><br><span class="line">    nopreempt</span><br><span class="line">    priority 90           # 权重，两个节点不能一样</span><br><span class="line">    advert_int 1</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.10.103      # 虚拟IP地址</span><br><span class="line">        240C::1234</span><br><span class="line">    &#125;</span><br><span class="line">    notify /etc/keepalived/bin/kpad_notify.sh     # keep状态传入脚本，通过该脚本可得知当前keep运行状态</span><br><span class="line">    track_script &#123;</span><br><span class="line">        chk_mysql            # 健康检查配置</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在拷贝<code>/etc/keepalived/bin/kpad_notify.sh</code>和<code>/etc/keepalived/bin/chk_mysql.sh</code>两个脚本后重启keepalived服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@HA-2:~# service keepalived restart</span><br></pre></td></tr></table></figure><h4 id="4-3-3-测试"><a href="#4-3-3-测试" class="headerlink" title="4.3.3 测试"></a>4.3.3 测试</h4><p>在HA-1和HA-2分别执行ip addr show dev enp0s9命令查看HA-1和HA-2对VIP（群集虚拟IP）的控制权。HA-1主的查看结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@HA-1:~# ip addr show dev enp0s9</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:fd:98:1b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.10.101/20 brd 192.168.15.255 scope global enp0s9</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.10.103/32 scope global enp0s9</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 240c:f:1:4000:20c:29ff:fefd:981b/64 scope global mngtmpaddr dynamic</span><br><span class="line">       valid_lft 2591546sec preferred_lft 604346sec</span><br><span class="line">    inet6 fe80::20c:29ff:fefd:981b/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>可以看到生成了192.168.10.101这个虚拟IP。</p><p>停止HA-1的keepalived服务，HA-2将会成为新的主节点，HA-2主的查看结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">root@HA-2:~# ip addr show dev enp0s9</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether 00:0c:29:fd:98:1b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.10.102/20 brd 192.168.15.255 scope global enp0s9</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.10.103/32 scope global enp0s9</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 240c:f:1:4000:20c:29ff:fefd:981b/64 scope global mngtmpaddr dynamic</span><br><span class="line">       valid_lft 2591855sec preferred_lft 604655sec</span><br><span class="line">    inet6 fe80::20c:29ff:fefd:981b/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>可以看到生成了192.168.10.103这个虚IP。</p><p>MySQL远程登录测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@HA-2:~#  mysql -h192.168.10.103 -uroot -p</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2119</span><br><span class="line">Server version: 5.7.21-0ubuntu0.16.04.1-log (Ubuntu)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.</span><br><span class="line"></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> show variables like <span class="string">'server_id'</span>;</span></span><br><span class="line">+---------------+-------+</span><br><span class="line">| Variable_name | Value |</span><br><span class="line">+---------------+-------+</span><br><span class="line">| server_id     | 2     |</span><br><span class="line">+---------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure><p>说明在客户端访问VIP地址，由HA-2主机提供响应的，当前状态下HA-2充当主服务器。</p><p>【Note】</p><p>经过测试在纯IPv6的环境下上述HA依然可以正常运行。</p><p>【参考链接】</p><p>1)<a href="https://baike.baidu.com/item/mySQL/471251?fr=aladdin" target="_blank" rel="noopener">mysql介绍</a></p><p>2)<a href="https://stackoverflow.com/questions/7739645/install-mysql-on-ubuntu-without-a-password-prompt" target="_blank" rel="noopener">mysql安装跳过密码设置</a></p><p>3)<a href="http://www.keepalived.org" target="_blank" rel="noopener">keepalived官网</a></p><p>4)<a href="https://www.cnblogs.com/clsn/p/8052649.html" target="_blank" rel="noopener">keepaliced介绍</a></p>]]></content>
    
    <summary type="html">
    
      MySQL主主复制配置
    
    </summary>
    
      <category term="ubuntu" scheme="https://louielong.github.io/categories/ubuntu/"/>
    
    
      <category term="ubuntu" scheme="https://louielong.github.io/source/tags/ubuntu/"/>
    
      <category term="mysql" scheme="https://louielong.github.io/source/tags/mysql/"/>
    
      <category term="keepalived" scheme="https://louielong.github.io/source/tags/keepalived/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下安装配置NTP服务器</title>
    <link href="https://louielong.github.io/linux_ntp_install.html"/>
    <id>https://louielong.github.io/linux_ntp_install.html</id>
    <published>2018-05-10T06:35:22.000Z</published>
    <updated>2018-09-12T09:51:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><p>在建立Linux主机集群时，为了避免集群主机时间不同以及长时间运行下所导致的时间偏差，需要进行时间同步(synchronize)。Linux系统下，一般使用ntp服务来同步不同机器的时间。NTP（Network Time Protocol）即网络时间协议，就是通过网络协议使计算机之间的时间同步化。</p><p><strong>时间同步方式</strong></p><p>NTP在linux下有两种时钟同步方式，分别为直接同步和平滑同步[1]：</p><ul><li>直接同步</li></ul><p>使用ntpdate命令进行同步，直接进行时间变更。如果服务器上存在一个12点运行的任务，当前服务器时间是13点，但标准时间时11点，使用此命令可能会造成任务重复执行。因此使用ntpdate同步可能会引发风险，因此该命令也多用于配置时钟同步服务时第一次同步时间时使用，如：系统重启时。</p><ul><li>平滑同步</li></ul><p>使用ntpd进行时钟同步，可以保证一个时间不经历两次，它每次同步时间的偏移量不会太陡，是慢慢来的，这正因为这样，ntpd平滑同步可能耗费的时间比较长。对ntp时间同步修正原理感兴趣的可以查看官方文档：<a href="https://www.eecis.udel.edu/~mills/ntp/html/warp.html" target="_blank" rel="noopener">How NTP Works</a></p><p>时钟的跃变，对于某些程序会导致很严重的问题。许多应用程序依赖连续的时钟，取得的时间是线性的，一些操作，例如数据库事务，通常会地依赖这样的事实：时间不会往回跳跃。不幸的是，ntpdate调整时间的方式就是我们所说的”跃变“：在获得一个时间之后，ntpdate使用settimeofday(2)设置系统时间，这有几个非常明显的问题[2]：</p><p>第一，这样做不安全。ntpdate的设置依赖于ntp服务器的安全性，攻击者可以利用一些软件设计上的缺陷，拿下ntp服务器并令与其同步的服务器执行某些消耗性的任务。由于ntpdate采用的方式是跳变，跟随它的服务器无法知道是否发生了异常（时间不一样的时候，唯一的办法是以服务器为准）。</p><p>第二，这样做不精确。一旦ntp服务器宕机，跟随它的服务器也就会无法同步时间。与此不同，ntpd不仅能够校准计算机的时间，而且能够校准计算机的时钟。</p><p>第三，这样做不够优雅。由于是跳变，而不是使时间变快或变慢，依赖时序的程序会出错（例如，如果ntpdate发现你的时间快了，则可能会经历两个相同的时刻，对某些应用而言，这是致命的）。因而，唯一一个可以令时间发生跳变的点，是计算机刚刚启动，但还没有启动很多服务的那个时候。其余的时候，理想的做法是使用ntpd来校准时钟，而不是调整计算机时钟上的时间。</p><p>NTPD 在和时间服务器的同步过程中，会把 BIOS 计时器的振荡频率偏差——或者说 Local Clock 的自然漂移(drift)——记录下来。这样即使网络有问题，本机仍然能维持一个相当精确的走时。</p><h2 id="2-安装ntp"><a href="#2-安装ntp" class="headerlink" title="2 安装ntp"></a>2 安装ntp</h2><p>ntp的安装是服务器和客户端集成在一起的，在ubuntu或centos下安装ntp服务器都非常简单，这里以ubuntu为例，使用NTP命令即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y ntp</span><br></pre></td></tr></table></figure><p>如果需要ntpdate工具则需要额外安装ntpdate</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y ntpdate</span><br></pre></td></tr></table></figure><h2 id="3-配置ntp服务器或server"><a href="#3-配置ntp服务器或server" class="headerlink" title="3 配置ntp服务器或server"></a>3 配置ntp服务器或server</h2><p>ntp的配置文件存放在<code>/etc/ntp.conf</code>，打开文件并进行修改，部分参数说明如下</p><h3 id="3-1-使用server命令设定上层NTP服务器"><a href="#3-1-使用server命令设定上层NTP服务器" class="headerlink" title="3.1 使用server命令设定上层NTP服务器"></a>3.1 使用server命令设定上层NTP服务器</h3><p>设定方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server [address] [options...]</span><br></pre></td></tr></table></figure><p>在server后面填写服务器地址（可以使IP或主机名），之后是命令参数主要包括autokey，brust，ibrust，key，minpoll ，maxpoll。这里最长使用的是ibrust和prefer。</p><table><thead><tr><th>参数</th><th>含义</th></tr></thead><tbody><tr><td>brust</td><td>当时间服务器不可达时，将发送间隔为2秒的连续八个包</td></tr><tr><td>ibrust</td><td>当时间服务器不可达时，将发送间隔为2秒的连续八个包</td></tr><tr><td>prefer</td><td>当其他数据相同时该节点将作为首选时间同步</td></tr></tbody></table><p>其它参数的详细说明可参考NTP的帮助文档（<code>man 5 ntp.conf</code>）。</p><h3 id="3-2-使用restrict命令管理权限控制"><a href="#3-2-使用restrict命令管理权限控制" class="headerlink" title="3.2 使用restrict命令管理权限控制"></a>3.2 使用restrict命令管理权限控制</h3><p>设定方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">restrict [address] mask [mask] [parameter]</span><br></pre></td></tr></table></figure><p>其中parameter的参数主要有以下这些：</p><ul><li>ignore: 拒绝所有类型的NTP联机；</li><li>nomodify: 客户端不能使用ntpc与ntpq这两个程序来修改服务器的时间参数，但客户端仍可透过这个主机来进行网络校时；</li><li>noquery: 客户端不能使用ntpq，ntpc等指令来查询时间服务器，等于不提供NTP的网络校时；</li><li>notrap: 不提供trap这个远程事件登录(remote event logging)的功能；</li><li>notrust: 拒绝没有认证的客户端；</li></ul><p>如果你没有在 parameter 的地方加上任何参数的话，这表示该 IP 或网段不受任何限制。</p><h3 id="3-3-使用driftfile记录时间差异"><a href="#3-3-使用driftfile记录时间差异" class="headerlink" title="3.3 使用driftfile记录时间差异"></a>3.3 使用driftfile记录时间差异</h3><p>设定方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driftfile [可以被ntpd写入的目录与档案]</span><br></pre></td></tr></table></figure><p>因为预设的 NTP Server 本身的时间计算是依据 BIOS 的芯片震荡周期频率来计算的，但是这个数值与上层 Time Server 不见得会一致啊！所以 NTP 这个 daemon (ntpd) 会自动的去计算我们自己主机的频率与上层 Time server 的频率，并且将两个频率的误差记录下来，记录下来的档案就是在 driftfile 后面接的完整档名当中了！关于档名你必须要知道：</p><ul><li>driftfile 后面接的档案需要使用完整路径文件名；</li><li>该档案不能是连结档；</li><li>该档案需要设定成 ntpd 这个 daemon 可以写入的权限；</li><li>该档案所记录的数值单位为：百万分之一秒 (ppm)；</li></ul><p>driftfile 后面接的档案会被 ntpd 自动更新，所以他的权限一定要能够让 ntpd 写入才行。</p><h3 id="3-4-使用statsdir和filegen开启统计分析"><a href="#3-4-使用statsdir和filegen开启统计分析" class="headerlink" title="3.4 使用statsdir和filegen开启统计分析"></a>3.4 使用statsdir和filegen开启统计分析</h3><p>设定方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">statsdir directory_path</span><br><span class="line">filegen name file filename [type type] [link | nolink] [enable | disable]</span><br></pre></td></tr></table></figure><p>当打开统计分析是会在directory_path目录下产生filegen中所设定的统计文件。</p><h3 id="3-5-指定接口"><a href="#3-5-指定接口" class="headerlink" title="3.5 指定接口"></a>3.5 指定接口</h3><p>ntp服务开启时默认监听所有的接口，如果想指定ip段或者接口则按照一下方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interface [listen | ignore | drop] [all | ipv4 | ipv6 | wildcard | name | address[/prefixlen]]</span><br></pre></td></tr></table></figure><p>listen指定监听接口，ignore忽略该接口，drop丢弃接口的请求数据包，接口类型可以指定为v4、v6或接口名。</p><h3 id="3-6-配置文件示例"><a href="#3-6-配置文件示例" class="headerlink" title="3.6 配置文件示例"></a>3.6 配置文件示例</h3><p>为了支持IPv6这里添加清华源的NTP服务器，清华源官方链接：<a href="https://tuna.moe/help/ntp/" target="_blank" rel="noopener">传送门</a></p><p>配置文件完整示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"># /etc/ntp.conf, configuration for ntpd; see ntp.conf(5) for help</span><br><span class="line"># 时间差异文件</span><br><span class="line">driftfile /var/lib/ntp/ntp.drift</span><br><span class="line"></span><br><span class="line"># 接口设置</span><br><span class="line">interface listen eth0</span><br><span class="line">interface ignore ipv4</span><br><span class="line"></span><br><span class="line"># 分析统计信息</span><br><span class="line">#statsdir /var/log/ntpstats/</span><br><span class="line"></span><br><span class="line">statistics loopstats peerstats clockstats</span><br><span class="line">filegen loopstats file loopstats type day enable</span><br><span class="line">filegen peerstats file peerstats type day enable</span><br><span class="line">filegen clockstats file clockstats type day enable</span><br><span class="line"></span><br><span class="line"># 上层ntp server.</span><br><span class="line">pool ntp1.aliyun.com iburst</span><br><span class="line">pool ntp2.aliyun.com iburst</span><br><span class="line">pool ntp3.aliyun.com iburst</span><br><span class="line">pool ntp4.aliyun.com iburst</span><br><span class="line"># 清华源提供IPv4和IPv6双栈</span><br><span class="line">pool ntp.tuna.tsinghua.edu.cn iburst perfer</span><br><span class="line"></span><br><span class="line"># Use Ubuntu&apos;s ntp server as a fallback.</span><br><span class="line">pool ntp.ubuntu.com</span><br><span class="line"></span><br><span class="line"># 不允许来自公网上ipv4和ipv6客户端的访问</span><br><span class="line">restrict -4 default kod notrap nomodify nopeer noquery limited</span><br><span class="line">restrict -6 default kod notrap nomodify nopeer noquery limited</span><br><span class="line"># 准许以下网络的ntp请求</span><br><span class="line">restrict -6 240c:6100:ffff:: netmask 64 nomodify</span><br><span class="line">restrict 172.16.0.1 netmask 20 nomodify</span><br><span class="line"></span><br><span class="line"># 让NTP Server和其自身保持同步，如果在/etc/ntp.conf中定义的server都不可用时，将使用local时间作为ntp服务提供给ntp客户端.</span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line"></span><br><span class="line"># Needed for adding pool entries</span><br><span class="line">restrict source notrap nomodify noquery</span><br><span class="line"></span><br><span class="line"># If you want to provide time to your local subnet, change the next line.</span><br><span class="line"># (Again, the address is an example only.)</span><br><span class="line">#broadcast 192.168.123.255</span><br><span class="line"></span><br><span class="line"># If you want to listen to time broadcasts on your local subnet, de-comment the</span><br><span class="line"># next lines.  Please do this only if you trust everybody on the network!</span><br><span class="line">#disable auth</span><br><span class="line">#broadcastclient</span><br><span class="line"></span><br><span class="line">#Changes recquired to use pps synchonisation as explained in documentation:</span><br><span class="line">#http://www.ntp.org/ntpfaq/NTP-s-config-adv.htm#AEN3918</span><br><span class="line"></span><br><span class="line">#server 127.127.8.1 mode 135 prefer    # Meinberg GPS167 with PPS</span><br><span class="line">#fudge 127.127.8.1 time1 0.0042        # relative to PPS for my hardware</span><br><span class="line"></span><br><span class="line">#server 127.127.22.1                   # ATOM(PPS)</span><br><span class="line">#fudge 127.127.22.1 flag3 1            # enable PPS API</span><br></pre></td></tr></table></figure><h2 id="4-启动ntp服务"><a href="#4-启动ntp服务" class="headerlink" title="4 启动ntp服务"></a>4 启动ntp服务</h2><p>启动ntp服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service ntp restart</span><br></pre></td></tr></table></figure><p>通过ntpq命令查看ntp同步状态</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">watch ntpq -p</span><br><span class="line">Every 2.0s: ntpq -p                                                                                                                                 </span><br><span class="line"></span><br><span class="line">     remote           refid      st t when poll reach   delay   offset  jitter</span><br><span class="line">==============================================================================</span><br><span class="line"> ntp1.aliyun.com .POOL.          16 p    -   64    0    0.000    0.000   0.000</span><br><span class="line"> ntp2.aliyun.com .POOL.          16 p    -   64    0    0.000    0.000   0.000</span><br><span class="line"> ntp3.aliyun.com .POOL.          16 p    -   64    0    0.000    0.000   0.000</span><br><span class="line"> ntp4.aliyun.com .POOL.          16 p    -   64    0    0.000    0.000   0.000</span><br><span class="line"> ntp.ubuntu.com  .POOL.          16 p    -   64    0    0.000    0.000   0.000</span><br><span class="line">+2001:67c:1560:8 192.53.103.108   2 u  411  512  377  401.548  -19.089   4.396</span><br><span class="line">*2001:67c:1560:8 17.253.34.125    2 u  512  512  377  379.682    0.776  18.600</span><br></pre></td></tr></table></figure><p>上述字段的含义如下：</p><ul><li>remote: 指的就是本地机器所连接的远程NTP服务器；</li><li>refid: 指的是给远程服务器提供时间同步的服务器；</li><li>st: 远程服务器的层级别（stratum）. 由于NTP是层型结构,有顶端的服务器,多层的Relay Server再到客户端。所以服务器从高到低级别可以设定为1-16. 为了减缓负荷和网络堵塞,原则上应该避免直接连接到级别为1的服务器的；</li><li>when: 几秒钟前曾经做过时间同步化更新的动作；</li><li>poll: 本地机和远程服务器多少时间进行一次同步(单位为秒).<br>在一开始运行NTP的时候这个poll值会比较小,那样和服务器同步的频率也就增加了,可以尽快调整到正确的时间范围.之后poll值会逐渐增大,同步的频率也就会相应减小；</li><li>reach: 已经向上层 NTP 服务器要求更新的次数；</li><li>delay: 网络传输过程当中延迟的时间，单位为 10^(-6) 秒；</li><li>offset: 时间补偿的结果，单位与 10^(-3) 秒；</li><li>jitter: Linux 系统时间与 BIOS 硬件时间的差异时间， 单位为 10^(-6) 秒。简单地说这个数值的绝对值越小我们和服务器的时间就越精确；</li><li>*: 它告诉我们远端的服务器已经被确认为我们的主NTP Server,我们系统的时间将由这台机器所提供；</li><li>+: 它将作为辅助的NTP Server和带有<em>号的服务器一起为我们提供同步服务. 当</em>号服务器不可用时它就可以接管；</li><li>-: 远程服务器被clustering algorithm认为是不合格的NTP Server；</li><li>x: 远程服务器不可用；</li></ul><p><strong>ntpdate 强制同步时间</strong></p><p>如果本机与时间同步服务器时间间隔太大可以通过ntpdate命令来进行同步，需要指出的是ntpdate使用前需要停止ntpd的进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ntpdate ntp1.ailiyun.com</span><br></pre></td></tr></table></figure><p>【Note】</p><p>ntp监听的是123端口，部分操作系统或防火墙等可能限制123端口因此需要放开端口限制。</p><p>【参考链接】</p><p>1)<a href="https://www.linuxidc.com/Linux/2015-11/124911.htm" target="_blank" rel="noopener">centos 7 ntp 同步说明</a></p><p>2)<a href="https://www.cnblogs.com/kerrycode/archive/2015/08/20/4744804.html" target="_blank" rel="noopener">NTP 安装与配置</a></p>]]></content>
    
    <summary type="html">
    
      NTP 服务器安装
    
    </summary>
    
      <category term="Ubuntu" scheme="https://louielong.github.io/categories/Ubuntu/"/>
    
    
      <category term="Ubuntu" scheme="https://louielong.github.io/source/tags/Ubuntu/"/>
    
      <category term="NTP" scheme="https://louielong.github.io/source/tags/NTP/"/>
    
  </entry>
  
</feed>
